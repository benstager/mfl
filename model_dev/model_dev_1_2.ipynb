{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mfl as mfl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mfl.api.data_loaders as mfldata\n",
    "import nfl_data_py as nfl\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\n",
    "from keras.models import Sequential\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FranchiseQB:\n",
    "    def __init__(self, \n",
    "                 feature_set='numeric', \n",
    "                 model='catboost', \n",
    "                 dataset='../data/for_modeling.csv', \n",
    "                 **kwargs):\n",
    "        \n",
    "        self.feature_set = feature_set\n",
    "        self.model = model \n",
    "        self.dataset = dataset\n",
    "        self.kwargs = kwargs\n",
    "        self.model_map = {\n",
    "            'catboost' : CatBoostClassifier(),\n",
    "            'xgb' : XGBClassifier(), \n",
    "            'rf' : RandomForestClassifier(),\n",
    "            'lr' : LogisticRegression(),\n",
    "            'hgb' : HistGradientBoostingClassifier(),\n",
    "            'svm' : SVC(),\n",
    "            'nn_basic': ...\n",
    "        }\n",
    "        self.available_models = list(self.model_map.keys())\n",
    "        self.model_func = self.model_map[model]\n",
    "        self.full_dataset = pd.read_csv(dataset)\n",
    "\n",
    "    def create_training_data(self, n_splits=4, stratify=True):\n",
    "        pass\n",
    "\n",
    "    def map_response(self, x):\n",
    "        if x >= 4:\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "        \n",
    "    def score(self, y_test, y_probs, y_preds):\n",
    "        accuracy = accuracy_score(y_test, y_preds)\n",
    "        f1 = f1_score(y_test, y_preds)\n",
    "        roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "        metric_dict = {\n",
    "            'accuracy' : accuracy,\n",
    "            'f1' : f1,\n",
    "            'roc_auc': roc_auc\n",
    "        }\n",
    "\n",
    "        return metric_dict\n",
    "\n",
    "    def catboost(self, feature_set=None, kfold=False, folds=2):\n",
    "        self.feature_set = feature_set\n",
    "        self.kfold = kfold\n",
    "        self.folds = folds\n",
    "\n",
    "        df = self.full_dataset.dropna()\n",
    "        df = df[df['season'] <= 2019]\n",
    "\n",
    "        if feature_set is None:\n",
    "            X = df.drop(['pfr_player_name', 'seasons_with_draft_team', 'name'],axis=1)\n",
    "            y = df['seasons_with_draft_team']\n",
    "        elif feature_set is not None:\n",
    "            pass\n",
    "        \n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.y_mapped = y.apply(self.map_response)\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, \n",
    "                                                                                self.y_mapped, \n",
    "                                                                                test_size=.25, \n",
    "                                                                                stratify=self.y_mapped, \n",
    "                                                                                shuffle=True)\n",
    "\n",
    "        model = CatBoostClassifier(one_hot_max_size=5, \n",
    "                                   iterations=300, \n",
    "                                   cat_features=X.select_dtypes(include='object').columns.tolist())\n",
    "        \n",
    "        model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        self.y_preds = model.predict(self.X_test)\n",
    "        self.y_probs = model.predict_proba(self.X_test)[:,1]\n",
    "\n",
    "        metrics = self.score(self.y_test, self.y_probs, self.y_preds)\n",
    "        \n",
    "        model.fit(X, self.y_mapped)\n",
    "        self.fit_model = model\n",
    "        self.model_results = pd.DataFrame(metrics, index=[0])\n",
    "        \n",
    "\n",
    "    def predict_2025_qb(self, player_name, round, pick, recent_team, season=2020):\n",
    "        \n",
    "        name = player_name\n",
    "        season = season\n",
    "        \n",
    "        variant_features = ['round', 'pick', 'season']\n",
    "        available_features = np.setdiff1d(self.fit_model.feature_names_[:-1], variant_features).tolist()\n",
    "\n",
    "        initial_features = pd.DataFrame({\n",
    "            'round' : round,\n",
    "            'pick' : pick,\n",
    "            'season' : season\n",
    "        }, index=[0])\n",
    "\n",
    "        predictors = mfldata.scrape_NFL_REF_QB(player_name=player_name)[available_features]\n",
    "        \n",
    "        processing = pd.concat([initial_features, predictors], axis=1)\n",
    "        processing['recent_team'] = recent_team\n",
    "\n",
    "        return self.fit_model.predict_proba(processing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>Cmp</th>\n",
       "      <th>Att</th>\n",
       "      <th>Cmp%</th>\n",
       "      <th>Yds</th>\n",
       "      <th>TD</th>\n",
       "      <th>TD%</th>\n",
       "      <th>Int</th>\n",
       "      <th>Int%</th>\n",
       "      <th>Y/A</th>\n",
       "      <th>AY/A</th>\n",
       "      <th>Y/C</th>\n",
       "      <th>Y/G</th>\n",
       "      <th>Rate</th>\n",
       "      <th>seasons</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>39</td>\n",
       "      <td>74.4</td>\n",
       "      <td>287</td>\n",
       "      <td>2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>8.38</td>\n",
       "      <td>9.9</td>\n",
       "      <td>26.1</td>\n",
       "      <td>153.1</td>\n",
       "      <td>7</td>\n",
       "      <td>Joe Burrow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    G Cmp Att  Cmp%  Yds TD  TD% Int Int%  Y/A  AY/A  Y/C   Y/G   Rate  \\\n",
       "0  11  29  39  74.4  287  2  5.1   0  0.0  7.4  8.38  9.9  26.1  153.1   \n",
       "\n",
       "   seasons        name  \n",
       "0        7  Joe Burrow  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfldata.scrape_NFL_REF_QB(player_name='Joe Burrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pfr_player_name</th>\n",
       "      <th>round</th>\n",
       "      <th>pick</th>\n",
       "      <th>season</th>\n",
       "      <th>G</th>\n",
       "      <th>Cmp</th>\n",
       "      <th>Att</th>\n",
       "      <th>Cmp%</th>\n",
       "      <th>Yds</th>\n",
       "      <th>TD</th>\n",
       "      <th>...</th>\n",
       "      <th>Int%</th>\n",
       "      <th>Y/A</th>\n",
       "      <th>AY/A</th>\n",
       "      <th>Y/C</th>\n",
       "      <th>Y/G</th>\n",
       "      <th>Rate</th>\n",
       "      <th>seasons</th>\n",
       "      <th>name</th>\n",
       "      <th>recent_team</th>\n",
       "      <th>seasons_with_draft_team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sam Bradford</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>31.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>67.6</td>\n",
       "      <td>8403.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>10.57</td>\n",
       "      <td>13.9</td>\n",
       "      <td>271.1</td>\n",
       "      <td>175.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sam Bradford</td>\n",
       "      <td>LA</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Cam Newton</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.50</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>87.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Cam Newton</td>\n",
       "      <td>CAR</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Andrew Luck</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>38.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>1064.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>9430.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>8.9</td>\n",
       "      <td>9.47</td>\n",
       "      <td>13.2</td>\n",
       "      <td>248.2</td>\n",
       "      <td>162.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Andrew Luck</td>\n",
       "      <td>IND</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Jameis Winston</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>27.0</td>\n",
       "      <td>562.0</td>\n",
       "      <td>851.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>7964.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>9.4</td>\n",
       "      <td>9.41</td>\n",
       "      <td>14.2</td>\n",
       "      <td>295.0</td>\n",
       "      <td>163.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jameis Winston</td>\n",
       "      <td>TB</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Jared Goff</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>37.0</td>\n",
       "      <td>977.0</td>\n",
       "      <td>1569.0</td>\n",
       "      <td>62.3</td>\n",
       "      <td>12200.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>7.8</td>\n",
       "      <td>8.14</td>\n",
       "      <td>12.5</td>\n",
       "      <td>329.7</td>\n",
       "      <td>144.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Jared Goff</td>\n",
       "      <td>LA</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Trevor Lawrence</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2021</td>\n",
       "      <td>40.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>66.6</td>\n",
       "      <td>10098.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>8.9</td>\n",
       "      <td>9.78</td>\n",
       "      <td>13.3</td>\n",
       "      <td>252.5</td>\n",
       "      <td>164.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Trevor Lawrence</td>\n",
       "      <td>JAX</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Joe Burrow</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>74.4</td>\n",
       "      <td>287.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>8.38</td>\n",
       "      <td>9.9</td>\n",
       "      <td>26.1</td>\n",
       "      <td>153.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Joe Burrow</td>\n",
       "      <td>CIN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Kyler Murray</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>8.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>59.5</td>\n",
       "      <td>686.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.7</td>\n",
       "      <td>3.89</td>\n",
       "      <td>9.5</td>\n",
       "      <td>85.8</td>\n",
       "      <td>109.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Kyler Murray</td>\n",
       "      <td>ARI</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Baker Mayfield</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>8.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>64.1</td>\n",
       "      <td>2315.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.8</td>\n",
       "      <td>6.32</td>\n",
       "      <td>10.6</td>\n",
       "      <td>289.4</td>\n",
       "      <td>127.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Baker Mayfield</td>\n",
       "      <td>CLE</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Robert Griffin III</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2012</td>\n",
       "      <td>41.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>1192.0</td>\n",
       "      <td>67.1</td>\n",
       "      <td>10366.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>8.7</td>\n",
       "      <td>9.36</td>\n",
       "      <td>13.0</td>\n",
       "      <td>252.8</td>\n",
       "      <td>158.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Robert Griffin III</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pfr_player_name  round  pick  season     G    Cmp     Att  Cmp%  \\\n",
       "0          Sam Bradford      1     1    2010  31.0  604.0   893.0  67.6   \n",
       "13           Cam Newton      1     1    2011   6.0    6.0    12.0  50.0   \n",
       "25          Andrew Luck      1     1    2012  38.0  713.0  1064.0  67.0   \n",
       "61       Jameis Winston      1     1    2015  27.0  562.0   851.0  66.0   \n",
       "68           Jared Goff      1     1    2016  37.0  977.0  1569.0  62.3   \n",
       "92      Trevor Lawrence      1     1    2021  40.0  758.0  1138.0  66.6   \n",
       "102          Joe Burrow      1     1    2020  11.0   29.0    39.0  74.4   \n",
       "115        Kyler Murray      1     1    2019   8.0   72.0   121.0  59.5   \n",
       "126      Baker Mayfield      1     1    2018   8.0  218.0   340.0  64.1   \n",
       "26   Robert Griffin III      1     2    2012  41.0  800.0  1192.0  67.1   \n",
       "\n",
       "         Yds    TD  ...  Int%  Y/A   AY/A   Y/C    Y/G   Rate  seasons  \\\n",
       "0     8403.0  88.0  ...   1.8  9.4  10.57  13.9  271.1  175.6      3.0   \n",
       "13      54.0   0.0  ...   0.0  4.5   4.50   9.0    9.0   87.8      7.0   \n",
       "25    9430.0  82.0  ...   2.1  8.9   9.47  13.2  248.2  162.8      3.0   \n",
       "61    7964.0  65.0  ...   3.3  9.4   9.41  14.2  295.0  163.3      2.0   \n",
       "68   12200.0  96.0  ...   1.9  7.8   8.14  12.5  329.7  144.0      3.0   \n",
       "92   10098.0  90.0  ...   1.5  8.9   9.78  13.3  252.5  164.3      3.0   \n",
       "102    287.0   2.0  ...   0.0  7.4   8.38   9.9   26.1  153.1      7.0   \n",
       "115    686.0   5.0  ...   5.8  5.7   3.89   9.5   85.8  109.2      7.0   \n",
       "126   2315.0  12.0  ...   2.6  6.8   6.32  10.6  289.4  127.7      8.0   \n",
       "26   10366.0  78.0  ...   1.4  8.7   9.36  13.0  252.8  158.9      4.0   \n",
       "\n",
       "                   name  recent_team seasons_with_draft_team  \n",
       "0          Sam Bradford           LA                     4.0  \n",
       "13           Cam Newton          CAR                    10.0  \n",
       "25          Andrew Luck          IND                     6.0  \n",
       "61       Jameis Winston           TB                     5.0  \n",
       "68           Jared Goff           LA                     5.0  \n",
       "92      Trevor Lawrence          JAX                     2.0  \n",
       "102          Joe Burrow          CIN                     3.0  \n",
       "115        Kyler Murray          ARI                     4.0  \n",
       "126      Baker Mayfield          CLE                     4.0  \n",
       "26   Robert Griffin III          NaN                     NaN  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.full_dataset.sort_values(['round', 'pick']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.009413\n",
      "0:\tlearn: 0.6882563\ttotal: 1.75ms\tremaining: 524ms\n",
      "1:\tlearn: 0.6830361\ttotal: 3.6ms\tremaining: 536ms\n",
      "2:\tlearn: 0.6765017\ttotal: 4.36ms\tremaining: 431ms\n",
      "3:\tlearn: 0.6718960\ttotal: 5.12ms\tremaining: 379ms\n",
      "4:\tlearn: 0.6677894\ttotal: 6.13ms\tremaining: 362ms\n",
      "5:\tlearn: 0.6639782\ttotal: 7ms\tremaining: 343ms\n",
      "6:\tlearn: 0.6587472\ttotal: 7.77ms\tremaining: 325ms\n",
      "7:\tlearn: 0.6542079\ttotal: 8.23ms\tremaining: 300ms\n",
      "8:\tlearn: 0.6481893\ttotal: 8.92ms\tremaining: 288ms\n",
      "9:\tlearn: 0.6447308\ttotal: 9.44ms\tremaining: 274ms\n",
      "10:\tlearn: 0.6401899\ttotal: 10.2ms\tremaining: 268ms\n",
      "11:\tlearn: 0.6361296\ttotal: 10.9ms\tremaining: 261ms\n",
      "12:\tlearn: 0.6323282\ttotal: 11.5ms\tremaining: 253ms\n",
      "13:\tlearn: 0.6289539\ttotal: 12.1ms\tremaining: 246ms\n",
      "14:\tlearn: 0.6244628\ttotal: 12.7ms\tremaining: 241ms\n",
      "15:\tlearn: 0.6200875\ttotal: 13.4ms\tremaining: 237ms\n",
      "16:\tlearn: 0.6158711\ttotal: 13.9ms\tremaining: 232ms\n",
      "17:\tlearn: 0.6101955\ttotal: 14.5ms\tremaining: 228ms\n",
      "18:\tlearn: 0.6078847\ttotal: 15.1ms\tremaining: 223ms\n",
      "19:\tlearn: 0.6033619\ttotal: 15.8ms\tremaining: 221ms\n",
      "20:\tlearn: 0.5988348\ttotal: 16.3ms\tremaining: 217ms\n",
      "21:\tlearn: 0.5935435\ttotal: 17.1ms\tremaining: 216ms\n",
      "22:\tlearn: 0.5898330\ttotal: 17.7ms\tremaining: 213ms\n",
      "23:\tlearn: 0.5843380\ttotal: 18.4ms\tremaining: 212ms\n",
      "24:\tlearn: 0.5801816\ttotal: 19.2ms\tremaining: 211ms\n",
      "25:\tlearn: 0.5762134\ttotal: 19.8ms\tremaining: 209ms\n",
      "26:\tlearn: 0.5716651\ttotal: 20.3ms\tremaining: 205ms\n",
      "27:\tlearn: 0.5690704\ttotal: 20.9ms\tremaining: 203ms\n",
      "28:\tlearn: 0.5656669\ttotal: 21.3ms\tremaining: 199ms\n",
      "29:\tlearn: 0.5632917\ttotal: 22.7ms\tremaining: 204ms\n",
      "30:\tlearn: 0.5592972\ttotal: 23.1ms\tremaining: 201ms\n",
      "31:\tlearn: 0.5558963\ttotal: 23.7ms\tremaining: 199ms\n",
      "32:\tlearn: 0.5539804\ttotal: 24.3ms\tremaining: 196ms\n",
      "33:\tlearn: 0.5494279\ttotal: 24.8ms\tremaining: 194ms\n",
      "34:\tlearn: 0.5464365\ttotal: 25.5ms\tremaining: 193ms\n",
      "35:\tlearn: 0.5436249\ttotal: 26ms\tremaining: 191ms\n",
      "36:\tlearn: 0.5399831\ttotal: 26.5ms\tremaining: 188ms\n",
      "37:\tlearn: 0.5388952\ttotal: 26.8ms\tremaining: 184ms\n",
      "38:\tlearn: 0.5353786\ttotal: 27.3ms\tremaining: 183ms\n",
      "39:\tlearn: 0.5313650\ttotal: 27.9ms\tremaining: 182ms\n",
      "40:\tlearn: 0.5291646\ttotal: 28.2ms\tremaining: 178ms\n",
      "41:\tlearn: 0.5253914\ttotal: 28.8ms\tremaining: 177ms\n",
      "42:\tlearn: 0.5228488\ttotal: 29.2ms\tremaining: 175ms\n",
      "43:\tlearn: 0.5201393\ttotal: 29.9ms\tremaining: 174ms\n",
      "44:\tlearn: 0.5177619\ttotal: 30.5ms\tremaining: 173ms\n",
      "45:\tlearn: 0.5145914\ttotal: 31.2ms\tremaining: 172ms\n",
      "46:\tlearn: 0.5099129\ttotal: 31.7ms\tremaining: 171ms\n",
      "47:\tlearn: 0.5062331\ttotal: 32.2ms\tremaining: 169ms\n",
      "48:\tlearn: 0.5034463\ttotal: 32.7ms\tremaining: 167ms\n",
      "49:\tlearn: 0.5009358\ttotal: 33.1ms\tremaining: 165ms\n",
      "50:\tlearn: 0.4984009\ttotal: 33.6ms\tremaining: 164ms\n",
      "51:\tlearn: 0.4964285\ttotal: 33.9ms\tremaining: 162ms\n",
      "52:\tlearn: 0.4932930\ttotal: 34.5ms\tremaining: 161ms\n",
      "53:\tlearn: 0.4902762\ttotal: 35ms\tremaining: 160ms\n",
      "54:\tlearn: 0.4879028\ttotal: 35.5ms\tremaining: 158ms\n",
      "55:\tlearn: 0.4847685\ttotal: 36ms\tremaining: 157ms\n",
      "56:\tlearn: 0.4822956\ttotal: 36.5ms\tremaining: 156ms\n",
      "57:\tlearn: 0.4796285\ttotal: 37.1ms\tremaining: 155ms\n",
      "58:\tlearn: 0.4765588\ttotal: 37.6ms\tremaining: 153ms\n",
      "59:\tlearn: 0.4725296\ttotal: 38.1ms\tremaining: 152ms\n",
      "60:\tlearn: 0.4707368\ttotal: 38.5ms\tremaining: 151ms\n",
      "61:\tlearn: 0.4678042\ttotal: 39ms\tremaining: 150ms\n",
      "62:\tlearn: 0.4655165\ttotal: 39.4ms\tremaining: 148ms\n",
      "63:\tlearn: 0.4626493\ttotal: 39.9ms\tremaining: 147ms\n",
      "64:\tlearn: 0.4594841\ttotal: 40.3ms\tremaining: 146ms\n",
      "65:\tlearn: 0.4565420\ttotal: 40.9ms\tremaining: 145ms\n",
      "66:\tlearn: 0.4540551\ttotal: 41.5ms\tremaining: 144ms\n",
      "67:\tlearn: 0.4514376\ttotal: 42.7ms\tremaining: 146ms\n",
      "68:\tlearn: 0.4488221\ttotal: 43.1ms\tremaining: 144ms\n",
      "69:\tlearn: 0.4455066\ttotal: 43.6ms\tremaining: 143ms\n",
      "70:\tlearn: 0.4419033\ttotal: 44.8ms\tremaining: 145ms\n",
      "71:\tlearn: 0.4402352\ttotal: 45.3ms\tremaining: 144ms\n",
      "72:\tlearn: 0.4367884\ttotal: 45.9ms\tremaining: 143ms\n",
      "73:\tlearn: 0.4337078\ttotal: 46.4ms\tremaining: 142ms\n",
      "74:\tlearn: 0.4310547\ttotal: 46.9ms\tremaining: 141ms\n",
      "75:\tlearn: 0.4288406\ttotal: 47.3ms\tremaining: 139ms\n",
      "76:\tlearn: 0.4263554\ttotal: 47.9ms\tremaining: 139ms\n",
      "77:\tlearn: 0.4238787\ttotal: 48.3ms\tremaining: 138ms\n",
      "78:\tlearn: 0.4216069\ttotal: 48.9ms\tremaining: 137ms\n",
      "79:\tlearn: 0.4195076\ttotal: 49.5ms\tremaining: 136ms\n",
      "80:\tlearn: 0.4171192\ttotal: 49.9ms\tremaining: 135ms\n",
      "81:\tlearn: 0.4139603\ttotal: 50.5ms\tremaining: 134ms\n",
      "82:\tlearn: 0.4122462\ttotal: 50.9ms\tremaining: 133ms\n",
      "83:\tlearn: 0.4093826\ttotal: 51.4ms\tremaining: 132ms\n",
      "84:\tlearn: 0.4083519\ttotal: 52.1ms\tremaining: 132ms\n",
      "85:\tlearn: 0.4057217\ttotal: 52.8ms\tremaining: 131ms\n",
      "86:\tlearn: 0.4027051\ttotal: 53.3ms\tremaining: 131ms\n",
      "87:\tlearn: 0.4007109\ttotal: 53.9ms\tremaining: 130ms\n",
      "88:\tlearn: 0.3977700\ttotal: 54.5ms\tremaining: 129ms\n",
      "89:\tlearn: 0.3959385\ttotal: 55.2ms\tremaining: 129ms\n",
      "90:\tlearn: 0.3933252\ttotal: 55.7ms\tremaining: 128ms\n",
      "91:\tlearn: 0.3911820\ttotal: 56.1ms\tremaining: 127ms\n",
      "92:\tlearn: 0.3882776\ttotal: 56.8ms\tremaining: 126ms\n",
      "93:\tlearn: 0.3864254\ttotal: 57.2ms\tremaining: 125ms\n",
      "94:\tlearn: 0.3863326\ttotal: 57.4ms\tremaining: 124ms\n",
      "95:\tlearn: 0.3847415\ttotal: 58ms\tremaining: 123ms\n",
      "96:\tlearn: 0.3817521\ttotal: 58.5ms\tremaining: 122ms\n",
      "97:\tlearn: 0.3800099\ttotal: 58.8ms\tremaining: 121ms\n",
      "98:\tlearn: 0.3782904\ttotal: 59.3ms\tremaining: 120ms\n",
      "99:\tlearn: 0.3757462\ttotal: 59.8ms\tremaining: 120ms\n",
      "100:\tlearn: 0.3737833\ttotal: 60.3ms\tremaining: 119ms\n",
      "101:\tlearn: 0.3712739\ttotal: 60.8ms\tremaining: 118ms\n",
      "102:\tlearn: 0.3695578\ttotal: 61.3ms\tremaining: 117ms\n",
      "103:\tlearn: 0.3667117\ttotal: 61.8ms\tremaining: 117ms\n",
      "104:\tlearn: 0.3644179\ttotal: 62.3ms\tremaining: 116ms\n",
      "105:\tlearn: 0.3616235\ttotal: 62.8ms\tremaining: 115ms\n",
      "106:\tlearn: 0.3596343\ttotal: 63.3ms\tremaining: 114ms\n",
      "107:\tlearn: 0.3577898\ttotal: 63.8ms\tremaining: 113ms\n",
      "108:\tlearn: 0.3557832\ttotal: 64.3ms\tremaining: 113ms\n",
      "109:\tlearn: 0.3541993\ttotal: 64.8ms\tremaining: 112ms\n",
      "110:\tlearn: 0.3524268\ttotal: 65.4ms\tremaining: 111ms\n",
      "111:\tlearn: 0.3497370\ttotal: 65.9ms\tremaining: 111ms\n",
      "112:\tlearn: 0.3485286\ttotal: 66.3ms\tremaining: 110ms\n",
      "113:\tlearn: 0.3464373\ttotal: 66.8ms\tremaining: 109ms\n",
      "114:\tlearn: 0.3441926\ttotal: 67.3ms\tremaining: 108ms\n",
      "115:\tlearn: 0.3427804\ttotal: 67.8ms\tremaining: 108ms\n",
      "116:\tlearn: 0.3416401\ttotal: 68.2ms\tremaining: 107ms\n",
      "117:\tlearn: 0.3397743\ttotal: 68.8ms\tremaining: 106ms\n",
      "118:\tlearn: 0.3379863\ttotal: 69.2ms\tremaining: 105ms\n",
      "119:\tlearn: 0.3355029\ttotal: 69.7ms\tremaining: 105ms\n",
      "120:\tlearn: 0.3343596\ttotal: 70.2ms\tremaining: 104ms\n",
      "121:\tlearn: 0.3321596\ttotal: 70.7ms\tremaining: 103ms\n",
      "122:\tlearn: 0.3302397\ttotal: 71.1ms\tremaining: 102ms\n",
      "123:\tlearn: 0.3278317\ttotal: 71.6ms\tremaining: 102ms\n",
      "124:\tlearn: 0.3251400\ttotal: 72.1ms\tremaining: 101ms\n",
      "125:\tlearn: 0.3244062\ttotal: 72.6ms\tremaining: 100ms\n",
      "126:\tlearn: 0.3222601\ttotal: 73.1ms\tremaining: 99.6ms\n",
      "127:\tlearn: 0.3203208\ttotal: 73.6ms\tremaining: 98.9ms\n",
      "128:\tlearn: 0.3185153\ttotal: 74.1ms\tremaining: 98.3ms\n",
      "129:\tlearn: 0.3168537\ttotal: 74.6ms\tremaining: 97.6ms\n",
      "130:\tlearn: 0.3148636\ttotal: 75ms\tremaining: 96.8ms\n",
      "131:\tlearn: 0.3126371\ttotal: 75.8ms\tremaining: 96.5ms\n",
      "132:\tlearn: 0.3106060\ttotal: 76.3ms\tremaining: 95.8ms\n",
      "133:\tlearn: 0.3092830\ttotal: 76.7ms\tremaining: 95ms\n",
      "134:\tlearn: 0.3077846\ttotal: 77.2ms\tremaining: 94.4ms\n",
      "135:\tlearn: 0.3054715\ttotal: 77.7ms\tremaining: 93.7ms\n",
      "136:\tlearn: 0.3037657\ttotal: 78.2ms\tremaining: 93.1ms\n",
      "137:\tlearn: 0.3018321\ttotal: 79.3ms\tremaining: 93.1ms\n",
      "138:\tlearn: 0.3004174\ttotal: 79.8ms\tremaining: 92.4ms\n",
      "139:\tlearn: 0.2987265\ttotal: 80.4ms\tremaining: 91.9ms\n",
      "140:\tlearn: 0.2976412\ttotal: 81ms\tremaining: 91.3ms\n",
      "141:\tlearn: 0.2963838\ttotal: 81.5ms\tremaining: 90.7ms\n",
      "142:\tlearn: 0.2949214\ttotal: 82ms\tremaining: 90ms\n",
      "143:\tlearn: 0.2929821\ttotal: 82.6ms\tremaining: 89.5ms\n",
      "144:\tlearn: 0.2916692\ttotal: 83.2ms\tremaining: 89ms\n",
      "145:\tlearn: 0.2897614\ttotal: 83.8ms\tremaining: 88.4ms\n",
      "146:\tlearn: 0.2880233\ttotal: 84.3ms\tremaining: 87.8ms\n",
      "147:\tlearn: 0.2865290\ttotal: 84.8ms\tremaining: 87.1ms\n",
      "148:\tlearn: 0.2845968\ttotal: 85.3ms\tremaining: 86.5ms\n",
      "149:\tlearn: 0.2835260\ttotal: 85.8ms\tremaining: 85.8ms\n",
      "150:\tlearn: 0.2811976\ttotal: 86.3ms\tremaining: 85.2ms\n",
      "151:\tlearn: 0.2791398\ttotal: 86.8ms\tremaining: 84.5ms\n",
      "152:\tlearn: 0.2776856\ttotal: 87.4ms\tremaining: 84ms\n",
      "153:\tlearn: 0.2759653\ttotal: 88ms\tremaining: 83.5ms\n",
      "154:\tlearn: 0.2751125\ttotal: 88.5ms\tremaining: 82.8ms\n",
      "155:\tlearn: 0.2736676\ttotal: 89ms\tremaining: 82.1ms\n",
      "156:\tlearn: 0.2721458\ttotal: 89.5ms\tremaining: 81.5ms\n",
      "157:\tlearn: 0.2714606\ttotal: 90.1ms\tremaining: 81ms\n",
      "158:\tlearn: 0.2702335\ttotal: 90.4ms\tremaining: 80.2ms\n",
      "159:\tlearn: 0.2689191\ttotal: 91ms\tremaining: 79.6ms\n",
      "160:\tlearn: 0.2678861\ttotal: 91.5ms\tremaining: 79ms\n",
      "161:\tlearn: 0.2663849\ttotal: 92.1ms\tremaining: 78.4ms\n",
      "162:\tlearn: 0.2656571\ttotal: 92.6ms\tremaining: 77.8ms\n",
      "163:\tlearn: 0.2640245\ttotal: 93ms\tremaining: 77.1ms\n",
      "164:\tlearn: 0.2631471\ttotal: 93.5ms\tremaining: 76.5ms\n",
      "165:\tlearn: 0.2622520\ttotal: 93.9ms\tremaining: 75.8ms\n",
      "166:\tlearn: 0.2608134\ttotal: 94.5ms\tremaining: 75.2ms\n",
      "167:\tlearn: 0.2604438\ttotal: 94.9ms\tremaining: 74.5ms\n",
      "168:\tlearn: 0.2591961\ttotal: 95.7ms\tremaining: 74.1ms\n",
      "169:\tlearn: 0.2584689\ttotal: 96.2ms\tremaining: 73.6ms\n",
      "170:\tlearn: 0.2572416\ttotal: 96.8ms\tremaining: 73ms\n",
      "171:\tlearn: 0.2559259\ttotal: 97.2ms\tremaining: 72.4ms\n",
      "172:\tlearn: 0.2547618\ttotal: 97.7ms\tremaining: 71.7ms\n",
      "173:\tlearn: 0.2535886\ttotal: 98.2ms\tremaining: 71.1ms\n",
      "174:\tlearn: 0.2524440\ttotal: 98.7ms\tremaining: 70.5ms\n",
      "175:\tlearn: 0.2509488\ttotal: 99.2ms\tremaining: 69.9ms\n",
      "176:\tlearn: 0.2501331\ttotal: 99.8ms\tremaining: 69.3ms\n",
      "177:\tlearn: 0.2486229\ttotal: 100ms\tremaining: 68.8ms\n",
      "178:\tlearn: 0.2477215\ttotal: 101ms\tremaining: 68.2ms\n",
      "179:\tlearn: 0.2467983\ttotal: 102ms\tremaining: 67.7ms\n",
      "180:\tlearn: 0.2461511\ttotal: 102ms\tremaining: 67.1ms\n",
      "181:\tlearn: 0.2453675\ttotal: 103ms\tremaining: 66.6ms\n",
      "182:\tlearn: 0.2446147\ttotal: 103ms\tremaining: 66.1ms\n",
      "183:\tlearn: 0.2432481\ttotal: 104ms\tremaining: 65.5ms\n",
      "184:\tlearn: 0.2425924\ttotal: 104ms\tremaining: 64.9ms\n",
      "185:\tlearn: 0.2415633\ttotal: 105ms\tremaining: 64.3ms\n",
      "186:\tlearn: 0.2405457\ttotal: 105ms\tremaining: 63.7ms\n",
      "187:\tlearn: 0.2395072\ttotal: 106ms\tremaining: 63ms\n",
      "188:\tlearn: 0.2381321\ttotal: 106ms\tremaining: 62.5ms\n",
      "189:\tlearn: 0.2366510\ttotal: 107ms\tremaining: 61.9ms\n",
      "190:\tlearn: 0.2358191\ttotal: 107ms\tremaining: 61.3ms\n",
      "191:\tlearn: 0.2348666\ttotal: 108ms\tremaining: 60.7ms\n",
      "192:\tlearn: 0.2333618\ttotal: 108ms\tremaining: 60ms\n",
      "193:\tlearn: 0.2320001\ttotal: 109ms\tremaining: 59.5ms\n",
      "194:\tlearn: 0.2308065\ttotal: 109ms\tremaining: 58.9ms\n",
      "195:\tlearn: 0.2296524\ttotal: 110ms\tremaining: 58.3ms\n",
      "196:\tlearn: 0.2287694\ttotal: 110ms\tremaining: 57.7ms\n",
      "197:\tlearn: 0.2282179\ttotal: 111ms\tremaining: 57.1ms\n",
      "198:\tlearn: 0.2274835\ttotal: 111ms\tremaining: 56.6ms\n",
      "199:\tlearn: 0.2264860\ttotal: 112ms\tremaining: 56ms\n",
      "200:\tlearn: 0.2255494\ttotal: 113ms\tremaining: 55.4ms\n",
      "201:\tlearn: 0.2249256\ttotal: 113ms\tremaining: 54.9ms\n",
      "202:\tlearn: 0.2239754\ttotal: 114ms\tremaining: 54.3ms\n",
      "203:\tlearn: 0.2235252\ttotal: 114ms\tremaining: 53.7ms\n",
      "204:\tlearn: 0.2227815\ttotal: 115ms\tremaining: 53.1ms\n",
      "205:\tlearn: 0.2218732\ttotal: 115ms\tremaining: 52.5ms\n",
      "206:\tlearn: 0.2208575\ttotal: 115ms\tremaining: 51.9ms\n",
      "207:\tlearn: 0.2195307\ttotal: 116ms\tremaining: 51.4ms\n",
      "208:\tlearn: 0.2189732\ttotal: 117ms\tremaining: 50.8ms\n",
      "209:\tlearn: 0.2181452\ttotal: 117ms\tremaining: 50.2ms\n",
      "210:\tlearn: 0.2173058\ttotal: 118ms\tremaining: 49.7ms\n",
      "211:\tlearn: 0.2164619\ttotal: 118ms\tremaining: 49.1ms\n",
      "212:\tlearn: 0.2155487\ttotal: 119ms\tremaining: 48.5ms\n",
      "213:\tlearn: 0.2142945\ttotal: 119ms\tremaining: 48ms\n",
      "214:\tlearn: 0.2131291\ttotal: 120ms\tremaining: 47.4ms\n",
      "215:\tlearn: 0.2125427\ttotal: 120ms\tremaining: 46.8ms\n",
      "216:\tlearn: 0.2115901\ttotal: 121ms\tremaining: 46.2ms\n",
      "217:\tlearn: 0.2112514\ttotal: 121ms\tremaining: 45.7ms\n",
      "218:\tlearn: 0.2102681\ttotal: 122ms\tremaining: 45.1ms\n",
      "219:\tlearn: 0.2096011\ttotal: 122ms\tremaining: 44.5ms\n",
      "220:\tlearn: 0.2085349\ttotal: 123ms\tremaining: 44ms\n",
      "221:\tlearn: 0.2078180\ttotal: 123ms\tremaining: 43.4ms\n",
      "222:\tlearn: 0.2069917\ttotal: 124ms\tremaining: 42.8ms\n",
      "223:\tlearn: 0.2063763\ttotal: 124ms\tremaining: 42.2ms\n",
      "224:\tlearn: 0.2060759\ttotal: 125ms\tremaining: 41.6ms\n",
      "225:\tlearn: 0.2056127\ttotal: 125ms\tremaining: 41ms\n",
      "226:\tlearn: 0.2048074\ttotal: 126ms\tremaining: 40.4ms\n",
      "227:\tlearn: 0.2037649\ttotal: 126ms\tremaining: 39.9ms\n",
      "228:\tlearn: 0.2029052\ttotal: 127ms\tremaining: 39.3ms\n",
      "229:\tlearn: 0.2020861\ttotal: 127ms\tremaining: 38.7ms\n",
      "230:\tlearn: 0.2017037\ttotal: 128ms\tremaining: 38.1ms\n",
      "231:\tlearn: 0.2009964\ttotal: 128ms\tremaining: 37.6ms\n",
      "232:\tlearn: 0.2003281\ttotal: 129ms\tremaining: 37ms\n",
      "233:\tlearn: 0.1995051\ttotal: 129ms\tremaining: 36.4ms\n",
      "234:\tlearn: 0.1986462\ttotal: 130ms\tremaining: 35.8ms\n",
      "235:\tlearn: 0.1976690\ttotal: 130ms\tremaining: 35.3ms\n",
      "236:\tlearn: 0.1969988\ttotal: 131ms\tremaining: 34.7ms\n",
      "237:\tlearn: 0.1958999\ttotal: 131ms\tremaining: 34.1ms\n",
      "238:\tlearn: 0.1949330\ttotal: 131ms\tremaining: 33.6ms\n",
      "239:\tlearn: 0.1939425\ttotal: 132ms\tremaining: 33ms\n",
      "240:\tlearn: 0.1933986\ttotal: 133ms\tremaining: 32.5ms\n",
      "241:\tlearn: 0.1925621\ttotal: 133ms\tremaining: 31.9ms\n",
      "242:\tlearn: 0.1921138\ttotal: 134ms\tremaining: 31.4ms\n",
      "243:\tlearn: 0.1912178\ttotal: 134ms\tremaining: 30.8ms\n",
      "244:\tlearn: 0.1906549\ttotal: 135ms\tremaining: 30.2ms\n",
      "245:\tlearn: 0.1900347\ttotal: 135ms\tremaining: 29.7ms\n",
      "246:\tlearn: 0.1893885\ttotal: 136ms\tremaining: 29.1ms\n",
      "247:\tlearn: 0.1886057\ttotal: 136ms\tremaining: 28.5ms\n",
      "248:\tlearn: 0.1873903\ttotal: 137ms\tremaining: 28ms\n",
      "249:\tlearn: 0.1863657\ttotal: 138ms\tremaining: 27.5ms\n",
      "250:\tlearn: 0.1858113\ttotal: 138ms\tremaining: 27ms\n",
      "251:\tlearn: 0.1856958\ttotal: 138ms\tremaining: 26.4ms\n",
      "252:\tlearn: 0.1849302\ttotal: 139ms\tremaining: 25.8ms\n",
      "253:\tlearn: 0.1841337\ttotal: 140ms\tremaining: 25.4ms\n",
      "254:\tlearn: 0.1836236\ttotal: 141ms\tremaining: 24.8ms\n",
      "255:\tlearn: 0.1827005\ttotal: 141ms\tremaining: 24.3ms\n",
      "256:\tlearn: 0.1818782\ttotal: 142ms\tremaining: 23.7ms\n",
      "257:\tlearn: 0.1809510\ttotal: 142ms\tremaining: 23.2ms\n",
      "258:\tlearn: 0.1806378\ttotal: 143ms\tremaining: 22.6ms\n",
      "259:\tlearn: 0.1797833\ttotal: 143ms\tremaining: 22ms\n",
      "260:\tlearn: 0.1789773\ttotal: 144ms\tremaining: 21.5ms\n",
      "261:\tlearn: 0.1782258\ttotal: 144ms\tremaining: 20.9ms\n",
      "262:\tlearn: 0.1771730\ttotal: 145ms\tremaining: 20.3ms\n",
      "263:\tlearn: 0.1768352\ttotal: 145ms\tremaining: 19.8ms\n",
      "264:\tlearn: 0.1762476\ttotal: 146ms\tremaining: 19.2ms\n",
      "265:\tlearn: 0.1755076\ttotal: 146ms\tremaining: 18.7ms\n",
      "266:\tlearn: 0.1747927\ttotal: 147ms\tremaining: 18.1ms\n",
      "267:\tlearn: 0.1743971\ttotal: 147ms\tremaining: 17.5ms\n",
      "268:\tlearn: 0.1737517\ttotal: 148ms\tremaining: 17ms\n",
      "269:\tlearn: 0.1730057\ttotal: 148ms\tremaining: 16.4ms\n",
      "270:\tlearn: 0.1724797\ttotal: 149ms\tremaining: 15.9ms\n",
      "271:\tlearn: 0.1717489\ttotal: 149ms\tremaining: 15.3ms\n",
      "272:\tlearn: 0.1710446\ttotal: 150ms\tremaining: 14.8ms\n",
      "273:\tlearn: 0.1704127\ttotal: 150ms\tremaining: 14.2ms\n",
      "274:\tlearn: 0.1693183\ttotal: 150ms\tremaining: 13.7ms\n",
      "275:\tlearn: 0.1688828\ttotal: 151ms\tremaining: 13.1ms\n",
      "276:\tlearn: 0.1683100\ttotal: 151ms\tremaining: 12.6ms\n",
      "277:\tlearn: 0.1677198\ttotal: 152ms\tremaining: 12ms\n",
      "278:\tlearn: 0.1674372\ttotal: 153ms\tremaining: 11.5ms\n",
      "279:\tlearn: 0.1668621\ttotal: 153ms\tremaining: 10.9ms\n",
      "280:\tlearn: 0.1659580\ttotal: 154ms\tremaining: 10.4ms\n",
      "281:\tlearn: 0.1652418\ttotal: 154ms\tremaining: 9.83ms\n",
      "282:\tlearn: 0.1648187\ttotal: 154ms\tremaining: 9.28ms\n",
      "283:\tlearn: 0.1642836\ttotal: 155ms\tremaining: 8.73ms\n",
      "284:\tlearn: 0.1637236\ttotal: 156ms\tremaining: 8.19ms\n",
      "285:\tlearn: 0.1630730\ttotal: 156ms\tremaining: 7.64ms\n",
      "286:\tlearn: 0.1622806\ttotal: 157ms\tremaining: 7.09ms\n",
      "287:\tlearn: 0.1618007\ttotal: 157ms\tremaining: 6.55ms\n",
      "288:\tlearn: 0.1610490\ttotal: 158ms\tremaining: 6ms\n",
      "289:\tlearn: 0.1603938\ttotal: 158ms\tremaining: 5.46ms\n",
      "290:\tlearn: 0.1598015\ttotal: 159ms\tremaining: 4.91ms\n",
      "291:\tlearn: 0.1592121\ttotal: 159ms\tremaining: 4.36ms\n",
      "292:\tlearn: 0.1586690\ttotal: 160ms\tremaining: 3.81ms\n",
      "293:\tlearn: 0.1581811\ttotal: 160ms\tremaining: 3.27ms\n",
      "294:\tlearn: 0.1576333\ttotal: 161ms\tremaining: 2.72ms\n",
      "295:\tlearn: 0.1571723\ttotal: 161ms\tremaining: 2.18ms\n",
      "296:\tlearn: 0.1565640\ttotal: 162ms\tremaining: 1.63ms\n",
      "297:\tlearn: 0.1556009\ttotal: 162ms\tremaining: 1.09ms\n",
      "298:\tlearn: 0.1554170\ttotal: 162ms\tremaining: 543us\n",
      "299:\tlearn: 0.1546861\ttotal: 163ms\tremaining: 0us\n",
      "Learning rate set to 0.010681\n",
      "0:\tlearn: 0.6872454\ttotal: 649us\tremaining: 194ms\n",
      "1:\tlearn: 0.6805426\ttotal: 1.28ms\tremaining: 191ms\n",
      "2:\tlearn: 0.6743198\ttotal: 1.79ms\tremaining: 177ms\n",
      "3:\tlearn: 0.6682950\ttotal: 2.3ms\tremaining: 170ms\n",
      "4:\tlearn: 0.6624152\ttotal: 2.86ms\tremaining: 169ms\n",
      "5:\tlearn: 0.6561972\ttotal: 3.38ms\tremaining: 165ms\n",
      "6:\tlearn: 0.6521196\ttotal: 3.71ms\tremaining: 155ms\n",
      "7:\tlearn: 0.6471767\ttotal: 4.39ms\tremaining: 160ms\n",
      "8:\tlearn: 0.6420625\ttotal: 4.92ms\tremaining: 159ms\n",
      "9:\tlearn: 0.6369448\ttotal: 5.53ms\tremaining: 160ms\n",
      "10:\tlearn: 0.6323299\ttotal: 5.99ms\tremaining: 157ms\n",
      "11:\tlearn: 0.6260949\ttotal: 6.56ms\tremaining: 158ms\n",
      "12:\tlearn: 0.6220658\ttotal: 7.11ms\tremaining: 157ms\n",
      "13:\tlearn: 0.6173813\ttotal: 7.66ms\tremaining: 156ms\n",
      "14:\tlearn: 0.6128132\ttotal: 8.1ms\tremaining: 154ms\n",
      "15:\tlearn: 0.6083703\ttotal: 8.68ms\tremaining: 154ms\n",
      "16:\tlearn: 0.6017075\ttotal: 9.14ms\tremaining: 152ms\n",
      "17:\tlearn: 0.5959915\ttotal: 9.61ms\tremaining: 151ms\n",
      "18:\tlearn: 0.5917712\ttotal: 10.1ms\tremaining: 149ms\n",
      "19:\tlearn: 0.5869521\ttotal: 10.5ms\tremaining: 148ms\n",
      "20:\tlearn: 0.5818503\ttotal: 11.1ms\tremaining: 147ms\n",
      "21:\tlearn: 0.5765791\ttotal: 11.7ms\tremaining: 147ms\n",
      "22:\tlearn: 0.5722917\ttotal: 12.3ms\tremaining: 148ms\n",
      "23:\tlearn: 0.5693812\ttotal: 12.9ms\tremaining: 148ms\n",
      "24:\tlearn: 0.5667336\ttotal: 13.5ms\tremaining: 148ms\n",
      "25:\tlearn: 0.5616709\ttotal: 14.1ms\tremaining: 149ms\n",
      "26:\tlearn: 0.5572903\ttotal: 14.7ms\tremaining: 149ms\n",
      "27:\tlearn: 0.5528059\ttotal: 15.3ms\tremaining: 148ms\n",
      "28:\tlearn: 0.5476304\ttotal: 15.7ms\tremaining: 147ms\n",
      "29:\tlearn: 0.5441135\ttotal: 16.2ms\tremaining: 146ms\n",
      "30:\tlearn: 0.5395235\ttotal: 16.7ms\tremaining: 145ms\n",
      "31:\tlearn: 0.5358645\ttotal: 17.3ms\tremaining: 145ms\n",
      "32:\tlearn: 0.5317945\ttotal: 17.8ms\tremaining: 144ms\n",
      "33:\tlearn: 0.5275108\ttotal: 18.3ms\tremaining: 143ms\n",
      "34:\tlearn: 0.5235151\ttotal: 18.9ms\tremaining: 143ms\n",
      "35:\tlearn: 0.5190760\ttotal: 19.4ms\tremaining: 142ms\n",
      "36:\tlearn: 0.5157406\ttotal: 19.9ms\tremaining: 141ms\n",
      "37:\tlearn: 0.5115384\ttotal: 20.4ms\tremaining: 141ms\n",
      "38:\tlearn: 0.5074406\ttotal: 21ms\tremaining: 141ms\n",
      "39:\tlearn: 0.5042524\ttotal: 21.5ms\tremaining: 140ms\n",
      "40:\tlearn: 0.5005815\ttotal: 22ms\tremaining: 139ms\n",
      "41:\tlearn: 0.4965556\ttotal: 22.5ms\tremaining: 138ms\n",
      "42:\tlearn: 0.4919668\ttotal: 23ms\tremaining: 138ms\n",
      "43:\tlearn: 0.4878425\ttotal: 23.5ms\tremaining: 137ms\n",
      "44:\tlearn: 0.4851661\ttotal: 24ms\tremaining: 136ms\n",
      "45:\tlearn: 0.4810374\ttotal: 24.5ms\tremaining: 135ms\n",
      "46:\tlearn: 0.4775259\ttotal: 25.2ms\tremaining: 136ms\n",
      "47:\tlearn: 0.4729830\ttotal: 25.9ms\tremaining: 136ms\n",
      "48:\tlearn: 0.4691547\ttotal: 26.8ms\tremaining: 137ms\n",
      "49:\tlearn: 0.4654167\ttotal: 27.3ms\tremaining: 136ms\n",
      "50:\tlearn: 0.4618487\ttotal: 27.8ms\tremaining: 136ms\n",
      "51:\tlearn: 0.4572625\ttotal: 28.4ms\tremaining: 135ms\n",
      "52:\tlearn: 0.4541687\ttotal: 29.1ms\tremaining: 136ms\n",
      "53:\tlearn: 0.4501678\ttotal: 29.8ms\tremaining: 136ms\n",
      "54:\tlearn: 0.4473128\ttotal: 30.5ms\tremaining: 136ms\n",
      "55:\tlearn: 0.4443551\ttotal: 31.3ms\tremaining: 136ms\n",
      "56:\tlearn: 0.4419522\ttotal: 31.9ms\tremaining: 136ms\n",
      "57:\tlearn: 0.4387321\ttotal: 32.6ms\tremaining: 136ms\n",
      "58:\tlearn: 0.4349810\ttotal: 33.1ms\tremaining: 135ms\n",
      "59:\tlearn: 0.4330567\ttotal: 33.6ms\tremaining: 135ms\n",
      "60:\tlearn: 0.4291252\ttotal: 34.1ms\tremaining: 134ms\n",
      "61:\tlearn: 0.4266977\ttotal: 34.8ms\tremaining: 134ms\n",
      "62:\tlearn: 0.4240112\ttotal: 35.4ms\tremaining: 133ms\n",
      "63:\tlearn: 0.4205845\ttotal: 35.9ms\tremaining: 132ms\n",
      "64:\tlearn: 0.4184778\ttotal: 36.4ms\tremaining: 132ms\n",
      "65:\tlearn: 0.4164209\ttotal: 36.9ms\tremaining: 131ms\n",
      "66:\tlearn: 0.4134483\ttotal: 37.4ms\tremaining: 130ms\n",
      "67:\tlearn: 0.4109955\ttotal: 38ms\tremaining: 129ms\n",
      "68:\tlearn: 0.4091205\ttotal: 38.5ms\tremaining: 129ms\n",
      "69:\tlearn: 0.4066019\ttotal: 39ms\tremaining: 128ms\n",
      "70:\tlearn: 0.4043283\ttotal: 39.6ms\tremaining: 128ms\n",
      "71:\tlearn: 0.4012348\ttotal: 40.7ms\tremaining: 129ms\n",
      "72:\tlearn: 0.3983262\ttotal: 41.6ms\tremaining: 129ms\n",
      "73:\tlearn: 0.3960483\ttotal: 42.3ms\tremaining: 129ms\n",
      "74:\tlearn: 0.3925742\ttotal: 42.8ms\tremaining: 129ms\n",
      "75:\tlearn: 0.3909345\ttotal: 43.2ms\tremaining: 127ms\n",
      "76:\tlearn: 0.3905868\ttotal: 43.5ms\tremaining: 126ms\n",
      "77:\tlearn: 0.3882597\ttotal: 44ms\tremaining: 125ms\n",
      "78:\tlearn: 0.3869061\ttotal: 44.7ms\tremaining: 125ms\n",
      "79:\tlearn: 0.3846767\ttotal: 45.4ms\tremaining: 125ms\n",
      "80:\tlearn: 0.3814775\ttotal: 46.1ms\tremaining: 125ms\n",
      "81:\tlearn: 0.3786844\ttotal: 47.1ms\tremaining: 125ms\n",
      "82:\tlearn: 0.3753540\ttotal: 47.8ms\tremaining: 125ms\n",
      "83:\tlearn: 0.3736461\ttotal: 48.4ms\tremaining: 125ms\n",
      "84:\tlearn: 0.3706753\ttotal: 49.1ms\tremaining: 124ms\n",
      "85:\tlearn: 0.3682189\ttotal: 49.6ms\tremaining: 123ms\n",
      "86:\tlearn: 0.3666828\ttotal: 50.1ms\tremaining: 123ms\n",
      "87:\tlearn: 0.3645072\ttotal: 50.6ms\tremaining: 122ms\n",
      "88:\tlearn: 0.3622101\ttotal: 51.3ms\tremaining: 122ms\n",
      "89:\tlearn: 0.3598475\ttotal: 51.9ms\tremaining: 121ms\n",
      "90:\tlearn: 0.3576147\ttotal: 52.5ms\tremaining: 120ms\n",
      "91:\tlearn: 0.3558890\ttotal: 53ms\tremaining: 120ms\n",
      "92:\tlearn: 0.3530082\ttotal: 53.6ms\tremaining: 119ms\n",
      "93:\tlearn: 0.3509877\ttotal: 54.2ms\tremaining: 119ms\n",
      "94:\tlearn: 0.3494488\ttotal: 54.8ms\tremaining: 118ms\n",
      "95:\tlearn: 0.3470895\ttotal: 55.3ms\tremaining: 118ms\n",
      "96:\tlearn: 0.3460974\ttotal: 55.9ms\tremaining: 117ms\n",
      "97:\tlearn: 0.3439325\ttotal: 56.4ms\tremaining: 116ms\n",
      "98:\tlearn: 0.3427322\ttotal: 56.9ms\tremaining: 116ms\n",
      "99:\tlearn: 0.3402572\ttotal: 57.5ms\tremaining: 115ms\n",
      "100:\tlearn: 0.3382255\ttotal: 58.1ms\tremaining: 114ms\n",
      "101:\tlearn: 0.3364435\ttotal: 58.6ms\tremaining: 114ms\n",
      "102:\tlearn: 0.3349767\ttotal: 59.2ms\tremaining: 113ms\n",
      "103:\tlearn: 0.3334882\ttotal: 59.7ms\tremaining: 113ms\n",
      "104:\tlearn: 0.3318858\ttotal: 60.3ms\tremaining: 112ms\n",
      "105:\tlearn: 0.3300981\ttotal: 60.9ms\tremaining: 111ms\n",
      "106:\tlearn: 0.3283198\ttotal: 61.4ms\tremaining: 111ms\n",
      "107:\tlearn: 0.3263027\ttotal: 61.9ms\tremaining: 110ms\n",
      "108:\tlearn: 0.3243424\ttotal: 62.4ms\tremaining: 109ms\n",
      "109:\tlearn: 0.3224451\ttotal: 62.9ms\tremaining: 109ms\n",
      "110:\tlearn: 0.3211775\ttotal: 63.5ms\tremaining: 108ms\n",
      "111:\tlearn: 0.3192470\ttotal: 64ms\tremaining: 107ms\n",
      "112:\tlearn: 0.3182263\ttotal: 64.4ms\tremaining: 107ms\n",
      "113:\tlearn: 0.3160469\ttotal: 65.1ms\tremaining: 106ms\n",
      "114:\tlearn: 0.3151651\ttotal: 65.5ms\tremaining: 105ms\n",
      "115:\tlearn: 0.3124704\ttotal: 66ms\tremaining: 105ms\n",
      "116:\tlearn: 0.3099835\ttotal: 66.5ms\tremaining: 104ms\n",
      "117:\tlearn: 0.3089963\ttotal: 67ms\tremaining: 103ms\n",
      "118:\tlearn: 0.3066917\ttotal: 67.6ms\tremaining: 103ms\n",
      "119:\tlearn: 0.3049436\ttotal: 68ms\tremaining: 102ms\n",
      "120:\tlearn: 0.3035534\ttotal: 68.5ms\tremaining: 101ms\n",
      "121:\tlearn: 0.3017405\ttotal: 69ms\tremaining: 101ms\n",
      "122:\tlearn: 0.3002893\ttotal: 69.7ms\tremaining: 100ms\n",
      "123:\tlearn: 0.2987424\ttotal: 70.2ms\tremaining: 99.7ms\n",
      "124:\tlearn: 0.2964867\ttotal: 70.7ms\tremaining: 98.9ms\n",
      "125:\tlearn: 0.2946939\ttotal: 71.2ms\tremaining: 98.3ms\n",
      "126:\tlearn: 0.2923401\ttotal: 71.8ms\tremaining: 97.8ms\n",
      "127:\tlearn: 0.2909332\ttotal: 72.3ms\tremaining: 97.2ms\n",
      "128:\tlearn: 0.2894212\ttotal: 73ms\tremaining: 96.7ms\n",
      "129:\tlearn: 0.2881118\ttotal: 73.5ms\tremaining: 96.1ms\n",
      "130:\tlearn: 0.2863474\ttotal: 74.1ms\tremaining: 95.6ms\n",
      "131:\tlearn: 0.2838967\ttotal: 74.6ms\tremaining: 95ms\n",
      "132:\tlearn: 0.2825969\ttotal: 75.4ms\tremaining: 94.7ms\n",
      "133:\tlearn: 0.2815832\ttotal: 75.9ms\tremaining: 94.1ms\n",
      "134:\tlearn: 0.2797758\ttotal: 76.5ms\tremaining: 93.5ms\n",
      "135:\tlearn: 0.2786654\ttotal: 77.1ms\tremaining: 93ms\n",
      "136:\tlearn: 0.2773444\ttotal: 77.7ms\tremaining: 92.5ms\n",
      "137:\tlearn: 0.2755467\ttotal: 78.5ms\tremaining: 92.1ms\n",
      "138:\tlearn: 0.2744206\ttotal: 79.1ms\tremaining: 91.6ms\n",
      "139:\tlearn: 0.2733220\ttotal: 79.8ms\tremaining: 91.2ms\n",
      "140:\tlearn: 0.2719490\ttotal: 80.6ms\tremaining: 90.9ms\n",
      "141:\tlearn: 0.2700074\ttotal: 81.4ms\tremaining: 90.5ms\n",
      "142:\tlearn: 0.2686620\ttotal: 82ms\tremaining: 90ms\n",
      "143:\tlearn: 0.2675745\ttotal: 82.6ms\tremaining: 89.5ms\n",
      "144:\tlearn: 0.2662391\ttotal: 83.1ms\tremaining: 88.8ms\n",
      "145:\tlearn: 0.2648537\ttotal: 83.6ms\tremaining: 88.2ms\n",
      "146:\tlearn: 0.2632365\ttotal: 84.1ms\tremaining: 87.6ms\n",
      "147:\tlearn: 0.2622505\ttotal: 84.7ms\tremaining: 87ms\n",
      "148:\tlearn: 0.2612576\ttotal: 85.2ms\tremaining: 86.3ms\n",
      "149:\tlearn: 0.2605479\ttotal: 85.8ms\tremaining: 85.8ms\n",
      "150:\tlearn: 0.2587207\ttotal: 86.5ms\tremaining: 85.3ms\n",
      "151:\tlearn: 0.2571491\ttotal: 87.1ms\tremaining: 84.8ms\n",
      "152:\tlearn: 0.2558571\ttotal: 87.6ms\tremaining: 84.2ms\n",
      "153:\tlearn: 0.2544563\ttotal: 88.3ms\tremaining: 83.7ms\n",
      "154:\tlearn: 0.2531360\ttotal: 89ms\tremaining: 83.2ms\n",
      "155:\tlearn: 0.2521959\ttotal: 89.5ms\tremaining: 82.6ms\n",
      "156:\tlearn: 0.2511488\ttotal: 90ms\tremaining: 82ms\n",
      "157:\tlearn: 0.2503106\ttotal: 90.5ms\tremaining: 81.3ms\n",
      "158:\tlearn: 0.2493176\ttotal: 91ms\tremaining: 80.7ms\n",
      "159:\tlearn: 0.2473736\ttotal: 91.5ms\tremaining: 80.1ms\n",
      "160:\tlearn: 0.2458715\ttotal: 92ms\tremaining: 79.4ms\n",
      "161:\tlearn: 0.2443237\ttotal: 92.5ms\tremaining: 78.8ms\n",
      "162:\tlearn: 0.2432191\ttotal: 93ms\tremaining: 78.2ms\n",
      "163:\tlearn: 0.2416071\ttotal: 93.7ms\tremaining: 77.7ms\n",
      "164:\tlearn: 0.2410332\ttotal: 94.4ms\tremaining: 77.2ms\n",
      "165:\tlearn: 0.2401868\ttotal: 95.1ms\tremaining: 76.7ms\n",
      "166:\tlearn: 0.2388744\ttotal: 95.7ms\tremaining: 76.2ms\n",
      "167:\tlearn: 0.2373248\ttotal: 96.4ms\tremaining: 75.8ms\n",
      "168:\tlearn: 0.2359336\ttotal: 97.2ms\tremaining: 75.3ms\n",
      "169:\tlearn: 0.2349311\ttotal: 97.8ms\tremaining: 74.8ms\n",
      "170:\tlearn: 0.2336849\ttotal: 98.4ms\tremaining: 74.2ms\n",
      "171:\tlearn: 0.2327012\ttotal: 99ms\tremaining: 73.7ms\n",
      "172:\tlearn: 0.2321333\ttotal: 99.4ms\tremaining: 73ms\n",
      "173:\tlearn: 0.2308419\ttotal: 100ms\tremaining: 72.4ms\n",
      "174:\tlearn: 0.2300278\ttotal: 101ms\tremaining: 71.8ms\n",
      "175:\tlearn: 0.2294919\ttotal: 101ms\tremaining: 71.1ms\n",
      "176:\tlearn: 0.2285688\ttotal: 101ms\tremaining: 70.4ms\n",
      "177:\tlearn: 0.2274335\ttotal: 102ms\tremaining: 69.8ms\n",
      "178:\tlearn: 0.2263805\ttotal: 102ms\tremaining: 69.2ms\n",
      "179:\tlearn: 0.2258795\ttotal: 103ms\tremaining: 68.6ms\n",
      "180:\tlearn: 0.2248747\ttotal: 104ms\tremaining: 68.1ms\n",
      "181:\tlearn: 0.2240231\ttotal: 104ms\tremaining: 67.5ms\n",
      "182:\tlearn: 0.2225870\ttotal: 105ms\tremaining: 66.9ms\n",
      "183:\tlearn: 0.2217066\ttotal: 105ms\tremaining: 66.3ms\n",
      "184:\tlearn: 0.2205522\ttotal: 106ms\tremaining: 65.8ms\n",
      "185:\tlearn: 0.2197179\ttotal: 106ms\tremaining: 65.2ms\n",
      "186:\tlearn: 0.2189242\ttotal: 107ms\tremaining: 64.5ms\n",
      "187:\tlearn: 0.2176271\ttotal: 107ms\tremaining: 64ms\n",
      "188:\tlearn: 0.2172491\ttotal: 108ms\tremaining: 63.4ms\n",
      "189:\tlearn: 0.2165904\ttotal: 108ms\tremaining: 62.8ms\n",
      "190:\tlearn: 0.2159039\ttotal: 109ms\tremaining: 62.2ms\n",
      "191:\tlearn: 0.2148440\ttotal: 110ms\tremaining: 61.7ms\n",
      "192:\tlearn: 0.2142617\ttotal: 110ms\tremaining: 61.1ms\n",
      "193:\tlearn: 0.2135995\ttotal: 111ms\tremaining: 60.6ms\n",
      "194:\tlearn: 0.2123959\ttotal: 111ms\tremaining: 60ms\n",
      "195:\tlearn: 0.2110431\ttotal: 112ms\tremaining: 59.4ms\n",
      "196:\tlearn: 0.2092781\ttotal: 113ms\tremaining: 58.8ms\n",
      "197:\tlearn: 0.2086763\ttotal: 113ms\tremaining: 58.3ms\n",
      "198:\tlearn: 0.2079494\ttotal: 114ms\tremaining: 57.7ms\n",
      "199:\tlearn: 0.2073841\ttotal: 114ms\tremaining: 57ms\n",
      "200:\tlearn: 0.2062767\ttotal: 115ms\tremaining: 56.4ms\n",
      "201:\tlearn: 0.2051924\ttotal: 115ms\tremaining: 55.9ms\n",
      "202:\tlearn: 0.2039411\ttotal: 116ms\tremaining: 55.3ms\n",
      "203:\tlearn: 0.2032393\ttotal: 116ms\tremaining: 54.7ms\n",
      "204:\tlearn: 0.2018440\ttotal: 117ms\tremaining: 54.1ms\n",
      "205:\tlearn: 0.2003776\ttotal: 117ms\tremaining: 53.6ms\n",
      "206:\tlearn: 0.1990178\ttotal: 118ms\tremaining: 53ms\n",
      "207:\tlearn: 0.1986107\ttotal: 119ms\tremaining: 52.5ms\n",
      "208:\tlearn: 0.1974747\ttotal: 119ms\tremaining: 51.9ms\n",
      "209:\tlearn: 0.1965415\ttotal: 120ms\tremaining: 51.4ms\n",
      "210:\tlearn: 0.1960221\ttotal: 120ms\tremaining: 50.8ms\n",
      "211:\tlearn: 0.1947842\ttotal: 121ms\tremaining: 50.2ms\n",
      "212:\tlearn: 0.1937421\ttotal: 122ms\tremaining: 49.6ms\n",
      "213:\tlearn: 0.1929931\ttotal: 122ms\tremaining: 49.1ms\n",
      "214:\tlearn: 0.1920467\ttotal: 123ms\tremaining: 48.5ms\n",
      "215:\tlearn: 0.1909528\ttotal: 123ms\tremaining: 47.9ms\n",
      "216:\tlearn: 0.1900685\ttotal: 124ms\tremaining: 47.3ms\n",
      "217:\tlearn: 0.1893359\ttotal: 124ms\tremaining: 46.7ms\n",
      "218:\tlearn: 0.1886195\ttotal: 125ms\tremaining: 46.2ms\n",
      "219:\tlearn: 0.1880082\ttotal: 126ms\tremaining: 45.7ms\n",
      "220:\tlearn: 0.1870243\ttotal: 126ms\tremaining: 45.1ms\n",
      "221:\tlearn: 0.1861785\ttotal: 127ms\tremaining: 44.5ms\n",
      "222:\tlearn: 0.1853836\ttotal: 127ms\tremaining: 43.9ms\n",
      "223:\tlearn: 0.1847582\ttotal: 128ms\tremaining: 43.3ms\n",
      "224:\tlearn: 0.1843145\ttotal: 128ms\tremaining: 42.8ms\n",
      "225:\tlearn: 0.1833650\ttotal: 129ms\tremaining: 42.2ms\n",
      "226:\tlearn: 0.1827720\ttotal: 129ms\tremaining: 41.6ms\n",
      "227:\tlearn: 0.1821863\ttotal: 130ms\tremaining: 41ms\n",
      "228:\tlearn: 0.1815421\ttotal: 131ms\tremaining: 40.5ms\n",
      "229:\tlearn: 0.1805162\ttotal: 131ms\tremaining: 39.9ms\n",
      "230:\tlearn: 0.1797595\ttotal: 132ms\tremaining: 39.3ms\n",
      "231:\tlearn: 0.1793563\ttotal: 132ms\tremaining: 38.7ms\n",
      "232:\tlearn: 0.1785286\ttotal: 133ms\tremaining: 38.1ms\n",
      "233:\tlearn: 0.1778843\ttotal: 133ms\tremaining: 37.5ms\n",
      "234:\tlearn: 0.1769953\ttotal: 134ms\tremaining: 37ms\n",
      "235:\tlearn: 0.1763126\ttotal: 134ms\tremaining: 36.4ms\n",
      "236:\tlearn: 0.1756090\ttotal: 135ms\tremaining: 35.8ms\n",
      "237:\tlearn: 0.1749877\ttotal: 135ms\tremaining: 35.3ms\n",
      "238:\tlearn: 0.1744927\ttotal: 136ms\tremaining: 34.7ms\n",
      "239:\tlearn: 0.1741070\ttotal: 137ms\tremaining: 34.1ms\n",
      "240:\tlearn: 0.1735806\ttotal: 137ms\tremaining: 33.5ms\n",
      "241:\tlearn: 0.1732816\ttotal: 138ms\tremaining: 33ms\n",
      "242:\tlearn: 0.1727771\ttotal: 138ms\tremaining: 32.4ms\n",
      "243:\tlearn: 0.1719460\ttotal: 139ms\tremaining: 31.8ms\n",
      "244:\tlearn: 0.1712476\ttotal: 139ms\tremaining: 31.3ms\n",
      "245:\tlearn: 0.1705549\ttotal: 140ms\tremaining: 30.7ms\n",
      "246:\tlearn: 0.1695459\ttotal: 140ms\tremaining: 30.1ms\n",
      "247:\tlearn: 0.1689533\ttotal: 141ms\tremaining: 29.6ms\n",
      "248:\tlearn: 0.1684701\ttotal: 142ms\tremaining: 29ms\n",
      "249:\tlearn: 0.1677638\ttotal: 142ms\tremaining: 28.5ms\n",
      "250:\tlearn: 0.1669441\ttotal: 143ms\tremaining: 27.9ms\n",
      "251:\tlearn: 0.1664955\ttotal: 144ms\tremaining: 27.4ms\n",
      "252:\tlearn: 0.1657048\ttotal: 144ms\tremaining: 26.8ms\n",
      "253:\tlearn: 0.1656295\ttotal: 145ms\tremaining: 26.2ms\n",
      "254:\tlearn: 0.1648695\ttotal: 145ms\tremaining: 25.6ms\n",
      "255:\tlearn: 0.1639957\ttotal: 146ms\tremaining: 25.1ms\n",
      "256:\tlearn: 0.1635343\ttotal: 146ms\tremaining: 24.5ms\n",
      "257:\tlearn: 0.1627781\ttotal: 147ms\tremaining: 23.9ms\n",
      "258:\tlearn: 0.1619579\ttotal: 148ms\tremaining: 23.4ms\n",
      "259:\tlearn: 0.1613499\ttotal: 148ms\tremaining: 22.8ms\n",
      "260:\tlearn: 0.1606426\ttotal: 148ms\tremaining: 22.2ms\n",
      "261:\tlearn: 0.1601860\ttotal: 149ms\tremaining: 21.6ms\n",
      "262:\tlearn: 0.1596123\ttotal: 150ms\tremaining: 21.1ms\n",
      "263:\tlearn: 0.1592072\ttotal: 150ms\tremaining: 20.5ms\n",
      "264:\tlearn: 0.1588430\ttotal: 151ms\tremaining: 19.9ms\n",
      "265:\tlearn: 0.1581009\ttotal: 151ms\tremaining: 19.4ms\n",
      "266:\tlearn: 0.1573857\ttotal: 152ms\tremaining: 18.8ms\n",
      "267:\tlearn: 0.1568685\ttotal: 153ms\tremaining: 18.2ms\n",
      "268:\tlearn: 0.1564147\ttotal: 153ms\tremaining: 17.6ms\n",
      "269:\tlearn: 0.1556239\ttotal: 154ms\tremaining: 17.1ms\n",
      "270:\tlearn: 0.1547777\ttotal: 154ms\tremaining: 16.5ms\n",
      "271:\tlearn: 0.1536725\ttotal: 155ms\tremaining: 15.9ms\n",
      "272:\tlearn: 0.1533589\ttotal: 155ms\tremaining: 15.4ms\n",
      "273:\tlearn: 0.1527699\ttotal: 156ms\tremaining: 14.8ms\n",
      "274:\tlearn: 0.1521988\ttotal: 156ms\tremaining: 14.2ms\n",
      "275:\tlearn: 0.1514048\ttotal: 157ms\tremaining: 13.6ms\n",
      "276:\tlearn: 0.1506474\ttotal: 158ms\tremaining: 13.1ms\n",
      "277:\tlearn: 0.1498078\ttotal: 158ms\tremaining: 12.5ms\n",
      "278:\tlearn: 0.1496452\ttotal: 158ms\tremaining: 11.9ms\n",
      "279:\tlearn: 0.1491205\ttotal: 159ms\tremaining: 11.4ms\n",
      "280:\tlearn: 0.1485222\ttotal: 159ms\tremaining: 10.8ms\n",
      "281:\tlearn: 0.1478630\ttotal: 160ms\tremaining: 10.2ms\n",
      "282:\tlearn: 0.1472723\ttotal: 161ms\tremaining: 9.64ms\n",
      "283:\tlearn: 0.1468933\ttotal: 161ms\tremaining: 9.07ms\n",
      "284:\tlearn: 0.1465189\ttotal: 161ms\tremaining: 8.5ms\n",
      "285:\tlearn: 0.1459147\ttotal: 162ms\tremaining: 7.93ms\n",
      "286:\tlearn: 0.1456765\ttotal: 162ms\tremaining: 7.35ms\n",
      "287:\tlearn: 0.1451777\ttotal: 163ms\tremaining: 6.78ms\n",
      "288:\tlearn: 0.1448577\ttotal: 163ms\tremaining: 6.21ms\n",
      "289:\tlearn: 0.1443735\ttotal: 164ms\tremaining: 5.65ms\n",
      "290:\tlearn: 0.1439837\ttotal: 164ms\tremaining: 5.08ms\n",
      "291:\tlearn: 0.1434355\ttotal: 165ms\tremaining: 4.51ms\n",
      "292:\tlearn: 0.1425944\ttotal: 165ms\tremaining: 3.95ms\n",
      "293:\tlearn: 0.1419491\ttotal: 166ms\tremaining: 3.38ms\n",
      "294:\tlearn: 0.1414537\ttotal: 166ms\tremaining: 2.82ms\n",
      "295:\tlearn: 0.1407474\ttotal: 167ms\tremaining: 2.25ms\n",
      "296:\tlearn: 0.1399404\ttotal: 168ms\tremaining: 1.69ms\n",
      "297:\tlearn: 0.1396721\ttotal: 168ms\tremaining: 1.13ms\n",
      "298:\tlearn: 0.1393395\ttotal: 169ms\tremaining: 563us\n",
      "299:\tlearn: 0.1389161\ttotal: 169ms\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "sim = FranchiseQB(model='catboost', feature_set='cat')\n",
    "sim.catboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.867347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy        f1   roc_auc\n",
       "0  0.761905  0.615385  0.867347"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40099278, 0.59900722]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.predict_2025_qb(player_name='Kyle McCord', round=1, pick=21, recent_team='PIT', season=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = FranchiseQB(model='catboost', feature_set='cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.010625\n",
      "0:\tlearn: 0.6846945\ttotal: 2.17ms\tremaining: 648ms\n",
      "1:\tlearn: 0.6763246\ttotal: 3.25ms\tremaining: 484ms\n",
      "2:\tlearn: 0.6685398\ttotal: 4.46ms\tremaining: 441ms\n",
      "3:\tlearn: 0.6637113\ttotal: 5.62ms\tremaining: 416ms\n",
      "4:\tlearn: 0.6546585\ttotal: 6.6ms\tremaining: 390ms\n",
      "5:\tlearn: 0.6468109\ttotal: 8.79ms\tremaining: 431ms\n",
      "6:\tlearn: 0.6405927\ttotal: 9.98ms\tremaining: 418ms\n",
      "7:\tlearn: 0.6324809\ttotal: 11.1ms\tremaining: 405ms\n",
      "8:\tlearn: 0.6270464\ttotal: 12ms\tremaining: 388ms\n",
      "9:\tlearn: 0.6219342\ttotal: 13ms\tremaining: 377ms\n",
      "10:\tlearn: 0.6160191\ttotal: 14ms\tremaining: 368ms\n",
      "11:\tlearn: 0.6093047\ttotal: 15ms\tremaining: 360ms\n",
      "12:\tlearn: 0.6030638\ttotal: 16.2ms\tremaining: 358ms\n",
      "13:\tlearn: 0.5970717\ttotal: 17.7ms\tremaining: 361ms\n",
      "14:\tlearn: 0.5902319\ttotal: 19.3ms\tremaining: 367ms\n",
      "15:\tlearn: 0.5858150\ttotal: 20.4ms\tremaining: 362ms\n",
      "16:\tlearn: 0.5779605\ttotal: 21.3ms\tremaining: 355ms\n",
      "17:\tlearn: 0.5764234\ttotal: 21.6ms\tremaining: 338ms\n",
      "18:\tlearn: 0.5705851\ttotal: 22.3ms\tremaining: 330ms\n",
      "19:\tlearn: 0.5662458\ttotal: 22.8ms\tremaining: 319ms\n",
      "20:\tlearn: 0.5604164\ttotal: 23.5ms\tremaining: 312ms\n",
      "21:\tlearn: 0.5567664\ttotal: 24.2ms\tremaining: 306ms\n",
      "22:\tlearn: 0.5509469\ttotal: 24.9ms\tremaining: 300ms\n",
      "23:\tlearn: 0.5458472\ttotal: 25.6ms\tremaining: 295ms\n",
      "24:\tlearn: 0.5402763\ttotal: 26.3ms\tremaining: 289ms\n",
      "25:\tlearn: 0.5352665\ttotal: 26.9ms\tremaining: 284ms\n",
      "26:\tlearn: 0.5300833\ttotal: 27.6ms\tremaining: 279ms\n",
      "27:\tlearn: 0.5253431\ttotal: 28.3ms\tremaining: 275ms\n",
      "28:\tlearn: 0.5216545\ttotal: 28.6ms\tremaining: 268ms\n",
      "29:\tlearn: 0.5159889\ttotal: 30ms\tremaining: 270ms\n",
      "30:\tlearn: 0.5115880\ttotal: 30.6ms\tremaining: 266ms\n",
      "31:\tlearn: 0.5061465\ttotal: 31.4ms\tremaining: 263ms\n",
      "32:\tlearn: 0.5015183\ttotal: 32ms\tremaining: 259ms\n",
      "33:\tlearn: 0.4968939\ttotal: 32.8ms\tremaining: 256ms\n",
      "34:\tlearn: 0.4940738\ttotal: 33.2ms\tremaining: 251ms\n",
      "35:\tlearn: 0.4904749\ttotal: 34.2ms\tremaining: 251ms\n",
      "36:\tlearn: 0.4895221\ttotal: 34.6ms\tremaining: 246ms\n",
      "37:\tlearn: 0.4838336\ttotal: 35.5ms\tremaining: 245ms\n",
      "38:\tlearn: 0.4795596\ttotal: 36.3ms\tremaining: 243ms\n",
      "39:\tlearn: 0.4773583\ttotal: 36.8ms\tremaining: 239ms\n",
      "40:\tlearn: 0.4727565\ttotal: 37.6ms\tremaining: 237ms\n",
      "41:\tlearn: 0.4694179\ttotal: 38.2ms\tremaining: 234ms\n",
      "42:\tlearn: 0.4644545\ttotal: 38.9ms\tremaining: 233ms\n",
      "43:\tlearn: 0.4592766\ttotal: 39.7ms\tremaining: 231ms\n",
      "44:\tlearn: 0.4558640\ttotal: 41.2ms\tremaining: 234ms\n",
      "45:\tlearn: 0.4521516\ttotal: 41.9ms\tremaining: 231ms\n",
      "46:\tlearn: 0.4486709\ttotal: 42.6ms\tremaining: 229ms\n",
      "47:\tlearn: 0.4460357\ttotal: 43.9ms\tremaining: 231ms\n",
      "48:\tlearn: 0.4423131\ttotal: 44.7ms\tremaining: 229ms\n",
      "49:\tlearn: 0.4386082\ttotal: 45.5ms\tremaining: 227ms\n",
      "50:\tlearn: 0.4353902\ttotal: 46.1ms\tremaining: 225ms\n",
      "51:\tlearn: 0.4325876\ttotal: 46.8ms\tremaining: 223ms\n",
      "52:\tlearn: 0.4319460\ttotal: 47ms\tremaining: 219ms\n",
      "53:\tlearn: 0.4285044\ttotal: 47.6ms\tremaining: 217ms\n",
      "54:\tlearn: 0.4255654\ttotal: 48ms\tremaining: 214ms\n",
      "55:\tlearn: 0.4225758\ttotal: 48.6ms\tremaining: 212ms\n",
      "56:\tlearn: 0.4198755\ttotal: 49.3ms\tremaining: 210ms\n",
      "57:\tlearn: 0.4159336\ttotal: 50.4ms\tremaining: 210ms\n",
      "58:\tlearn: 0.4148501\ttotal: 50.9ms\tremaining: 208ms\n",
      "59:\tlearn: 0.4117414\ttotal: 51.8ms\tremaining: 207ms\n",
      "60:\tlearn: 0.4090422\ttotal: 52.6ms\tremaining: 206ms\n",
      "61:\tlearn: 0.4057852\ttotal: 53.2ms\tremaining: 204ms\n",
      "62:\tlearn: 0.4024794\ttotal: 53.8ms\tremaining: 202ms\n",
      "63:\tlearn: 0.3988741\ttotal: 54.4ms\tremaining: 201ms\n",
      "64:\tlearn: 0.3940787\ttotal: 55ms\tremaining: 199ms\n",
      "65:\tlearn: 0.3920472\ttotal: 55.7ms\tremaining: 197ms\n",
      "66:\tlearn: 0.3886338\ttotal: 56.3ms\tremaining: 196ms\n",
      "67:\tlearn: 0.3859887\ttotal: 57ms\tremaining: 194ms\n",
      "68:\tlearn: 0.3831796\ttotal: 57.6ms\tremaining: 193ms\n",
      "69:\tlearn: 0.3808957\ttotal: 58.2ms\tremaining: 191ms\n",
      "70:\tlearn: 0.3783042\ttotal: 58.8ms\tremaining: 190ms\n",
      "71:\tlearn: 0.3765473\ttotal: 59.6ms\tremaining: 189ms\n",
      "72:\tlearn: 0.3738889\ttotal: 60ms\tremaining: 187ms\n",
      "73:\tlearn: 0.3713564\ttotal: 60.6ms\tremaining: 185ms\n",
      "74:\tlearn: 0.3696703\ttotal: 61.2ms\tremaining: 184ms\n",
      "75:\tlearn: 0.3667953\ttotal: 61.8ms\tremaining: 182ms\n",
      "76:\tlearn: 0.3640507\ttotal: 62.4ms\tremaining: 181ms\n",
      "77:\tlearn: 0.3608378\ttotal: 63.1ms\tremaining: 180ms\n",
      "78:\tlearn: 0.3574664\ttotal: 63.8ms\tremaining: 178ms\n",
      "79:\tlearn: 0.3548422\ttotal: 64.4ms\tremaining: 177ms\n",
      "80:\tlearn: 0.3529322\ttotal: 65.1ms\tremaining: 176ms\n",
      "81:\tlearn: 0.3507032\ttotal: 65.8ms\tremaining: 175ms\n",
      "82:\tlearn: 0.3486923\ttotal: 66.5ms\tremaining: 174ms\n",
      "83:\tlearn: 0.3454597\ttotal: 67.1ms\tremaining: 173ms\n",
      "84:\tlearn: 0.3435260\ttotal: 67.8ms\tremaining: 172ms\n",
      "85:\tlearn: 0.3413330\ttotal: 68.5ms\tremaining: 170ms\n",
      "86:\tlearn: 0.3399159\ttotal: 69.1ms\tremaining: 169ms\n",
      "87:\tlearn: 0.3378538\ttotal: 69.7ms\tremaining: 168ms\n",
      "88:\tlearn: 0.3350951\ttotal: 70.3ms\tremaining: 167ms\n",
      "89:\tlearn: 0.3322252\ttotal: 70.9ms\tremaining: 165ms\n",
      "90:\tlearn: 0.3304882\ttotal: 71.5ms\tremaining: 164ms\n",
      "91:\tlearn: 0.3283127\ttotal: 72.3ms\tremaining: 163ms\n",
      "92:\tlearn: 0.3261952\ttotal: 73ms\tremaining: 162ms\n",
      "93:\tlearn: 0.3239972\ttotal: 73.6ms\tremaining: 161ms\n",
      "94:\tlearn: 0.3233098\ttotal: 74.1ms\tremaining: 160ms\n",
      "95:\tlearn: 0.3209443\ttotal: 74.7ms\tremaining: 159ms\n",
      "96:\tlearn: 0.3191772\ttotal: 75.9ms\tremaining: 159ms\n",
      "97:\tlearn: 0.3179821\ttotal: 76.5ms\tremaining: 158ms\n",
      "98:\tlearn: 0.3161749\ttotal: 77.2ms\tremaining: 157ms\n",
      "99:\tlearn: 0.3143391\ttotal: 77.8ms\tremaining: 156ms\n",
      "100:\tlearn: 0.3123709\ttotal: 78.4ms\tremaining: 154ms\n",
      "101:\tlearn: 0.3103316\ttotal: 79ms\tremaining: 153ms\n",
      "102:\tlearn: 0.3082332\ttotal: 80.4ms\tremaining: 154ms\n",
      "103:\tlearn: 0.3066497\ttotal: 81ms\tremaining: 153ms\n",
      "104:\tlearn: 0.3040933\ttotal: 81.8ms\tremaining: 152ms\n",
      "105:\tlearn: 0.3024039\ttotal: 82.6ms\tremaining: 151ms\n",
      "106:\tlearn: 0.3001910\ttotal: 83.3ms\tremaining: 150ms\n",
      "107:\tlearn: 0.2984487\ttotal: 84.6ms\tremaining: 150ms\n",
      "108:\tlearn: 0.2966294\ttotal: 85ms\tremaining: 149ms\n",
      "109:\tlearn: 0.2949123\ttotal: 85.6ms\tremaining: 148ms\n",
      "110:\tlearn: 0.2930703\ttotal: 86.2ms\tremaining: 147ms\n",
      "111:\tlearn: 0.2911130\ttotal: 87ms\tremaining: 146ms\n",
      "112:\tlearn: 0.2896777\ttotal: 87.6ms\tremaining: 145ms\n",
      "113:\tlearn: 0.2887274\ttotal: 87.9ms\tremaining: 143ms\n",
      "114:\tlearn: 0.2868764\ttotal: 88.4ms\tremaining: 142ms\n",
      "115:\tlearn: 0.2861905\ttotal: 88.9ms\tremaining: 141ms\n",
      "116:\tlearn: 0.2848837\ttotal: 89.6ms\tremaining: 140ms\n",
      "117:\tlearn: 0.2831295\ttotal: 90.3ms\tremaining: 139ms\n",
      "118:\tlearn: 0.2816317\ttotal: 90.8ms\tremaining: 138ms\n",
      "119:\tlearn: 0.2802567\ttotal: 91.5ms\tremaining: 137ms\n",
      "120:\tlearn: 0.2801268\ttotal: 91.7ms\tremaining: 136ms\n",
      "121:\tlearn: 0.2793480\ttotal: 92.3ms\tremaining: 135ms\n",
      "122:\tlearn: 0.2782954\ttotal: 92.9ms\tremaining: 134ms\n",
      "123:\tlearn: 0.2781731\ttotal: 93.1ms\tremaining: 132ms\n",
      "124:\tlearn: 0.2763211\ttotal: 94ms\tremaining: 132ms\n",
      "125:\tlearn: 0.2754621\ttotal: 94.5ms\tremaining: 131ms\n",
      "126:\tlearn: 0.2741457\ttotal: 95.1ms\tremaining: 130ms\n",
      "127:\tlearn: 0.2725165\ttotal: 95.8ms\tremaining: 129ms\n",
      "128:\tlearn: 0.2712021\ttotal: 96.3ms\tremaining: 128ms\n",
      "129:\tlearn: 0.2698589\ttotal: 96.9ms\tremaining: 127ms\n",
      "130:\tlearn: 0.2682643\ttotal: 97.5ms\tremaining: 126ms\n",
      "131:\tlearn: 0.2661894\ttotal: 98.4ms\tremaining: 125ms\n",
      "132:\tlearn: 0.2651414\ttotal: 99ms\tremaining: 124ms\n",
      "133:\tlearn: 0.2639196\ttotal: 99.7ms\tremaining: 124ms\n",
      "134:\tlearn: 0.2624954\ttotal: 100ms\tremaining: 123ms\n",
      "135:\tlearn: 0.2605789\ttotal: 101ms\tremaining: 122ms\n",
      "136:\tlearn: 0.2592383\ttotal: 102ms\tremaining: 121ms\n",
      "137:\tlearn: 0.2574177\ttotal: 103ms\tremaining: 121ms\n",
      "138:\tlearn: 0.2558609\ttotal: 103ms\tremaining: 120ms\n",
      "139:\tlearn: 0.2548059\ttotal: 104ms\tremaining: 119ms\n",
      "140:\tlearn: 0.2534091\ttotal: 104ms\tremaining: 117ms\n",
      "141:\tlearn: 0.2519374\ttotal: 105ms\tremaining: 117ms\n",
      "142:\tlearn: 0.2510852\ttotal: 105ms\tremaining: 116ms\n",
      "143:\tlearn: 0.2499352\ttotal: 107ms\tremaining: 116ms\n",
      "144:\tlearn: 0.2484011\ttotal: 107ms\tremaining: 115ms\n",
      "145:\tlearn: 0.2473407\ttotal: 108ms\tremaining: 114ms\n",
      "146:\tlearn: 0.2465717\ttotal: 109ms\tremaining: 113ms\n",
      "147:\tlearn: 0.2452938\ttotal: 109ms\tremaining: 112ms\n",
      "148:\tlearn: 0.2440429\ttotal: 110ms\tremaining: 111ms\n",
      "149:\tlearn: 0.2424554\ttotal: 110ms\tremaining: 110ms\n",
      "150:\tlearn: 0.2411683\ttotal: 111ms\tremaining: 110ms\n",
      "151:\tlearn: 0.2400737\ttotal: 112ms\tremaining: 109ms\n",
      "152:\tlearn: 0.2387242\ttotal: 112ms\tremaining: 108ms\n",
      "153:\tlearn: 0.2371428\ttotal: 113ms\tremaining: 107ms\n",
      "154:\tlearn: 0.2359921\ttotal: 114ms\tremaining: 106ms\n",
      "155:\tlearn: 0.2339717\ttotal: 114ms\tremaining: 105ms\n",
      "156:\tlearn: 0.2327475\ttotal: 115ms\tremaining: 105ms\n",
      "157:\tlearn: 0.2323347\ttotal: 116ms\tremaining: 104ms\n",
      "158:\tlearn: 0.2317770\ttotal: 116ms\tremaining: 103ms\n",
      "159:\tlearn: 0.2307835\ttotal: 117ms\tremaining: 103ms\n",
      "160:\tlearn: 0.2293048\ttotal: 118ms\tremaining: 102ms\n",
      "161:\tlearn: 0.2284566\ttotal: 118ms\tremaining: 101ms\n",
      "162:\tlearn: 0.2274404\ttotal: 120ms\tremaining: 101ms\n",
      "163:\tlearn: 0.2266090\ttotal: 120ms\tremaining: 99.8ms\n",
      "164:\tlearn: 0.2251925\ttotal: 121ms\tremaining: 98.9ms\n",
      "165:\tlearn: 0.2242981\ttotal: 122ms\tremaining: 98.2ms\n",
      "166:\tlearn: 0.2226346\ttotal: 122ms\tremaining: 97.4ms\n",
      "167:\tlearn: 0.2217547\ttotal: 123ms\tremaining: 96.6ms\n",
      "168:\tlearn: 0.2201590\ttotal: 123ms\tremaining: 95.7ms\n",
      "169:\tlearn: 0.2187077\ttotal: 124ms\tremaining: 94.9ms\n",
      "170:\tlearn: 0.2174727\ttotal: 125ms\tremaining: 94.1ms\n",
      "171:\tlearn: 0.2162982\ttotal: 126ms\tremaining: 93.8ms\n",
      "172:\tlearn: 0.2152357\ttotal: 127ms\tremaining: 92.9ms\n",
      "173:\tlearn: 0.2139637\ttotal: 127ms\tremaining: 92.2ms\n",
      "174:\tlearn: 0.2131715\ttotal: 129ms\tremaining: 91.9ms\n",
      "175:\tlearn: 0.2117860\ttotal: 129ms\tremaining: 91.1ms\n",
      "176:\tlearn: 0.2103547\ttotal: 130ms\tremaining: 90.3ms\n",
      "177:\tlearn: 0.2095045\ttotal: 131ms\tremaining: 90.1ms\n",
      "178:\tlearn: 0.2085371\ttotal: 132ms\tremaining: 89.4ms\n",
      "179:\tlearn: 0.2076871\ttotal: 133ms\tremaining: 88.7ms\n",
      "180:\tlearn: 0.2069196\ttotal: 134ms\tremaining: 87.8ms\n",
      "181:\tlearn: 0.2057661\ttotal: 134ms\tremaining: 87ms\n",
      "182:\tlearn: 0.2049518\ttotal: 135ms\tremaining: 86.2ms\n",
      "183:\tlearn: 0.2039785\ttotal: 135ms\tremaining: 85.4ms\n",
      "184:\tlearn: 0.2028990\ttotal: 137ms\tremaining: 85.1ms\n",
      "185:\tlearn: 0.2022738\ttotal: 138ms\tremaining: 84.3ms\n",
      "186:\tlearn: 0.2011514\ttotal: 138ms\tremaining: 83.5ms\n",
      "187:\tlearn: 0.2000736\ttotal: 139ms\tremaining: 82.6ms\n",
      "188:\tlearn: 0.1994958\ttotal: 139ms\tremaining: 81.8ms\n",
      "189:\tlearn: 0.1980829\ttotal: 140ms\tremaining: 81ms\n",
      "190:\tlearn: 0.1967855\ttotal: 141ms\tremaining: 80.6ms\n",
      "191:\tlearn: 0.1959085\ttotal: 142ms\tremaining: 79.8ms\n",
      "192:\tlearn: 0.1951243\ttotal: 143ms\tremaining: 79ms\n",
      "193:\tlearn: 0.1938523\ttotal: 144ms\tremaining: 78.7ms\n",
      "194:\tlearn: 0.1927787\ttotal: 145ms\tremaining: 77.9ms\n",
      "195:\tlearn: 0.1913302\ttotal: 145ms\tremaining: 77.1ms\n",
      "196:\tlearn: 0.1903707\ttotal: 146ms\tremaining: 76.3ms\n",
      "197:\tlearn: 0.1893701\ttotal: 147ms\tremaining: 75.5ms\n",
      "198:\tlearn: 0.1888863\ttotal: 147ms\tremaining: 74.8ms\n",
      "199:\tlearn: 0.1888534\ttotal: 148ms\tremaining: 73.8ms\n",
      "200:\tlearn: 0.1880526\ttotal: 148ms\tremaining: 73ms\n",
      "201:\tlearn: 0.1868814\ttotal: 149ms\tremaining: 72.2ms\n",
      "202:\tlearn: 0.1858194\ttotal: 149ms\tremaining: 71.4ms\n",
      "203:\tlearn: 0.1849503\ttotal: 150ms\tremaining: 70.7ms\n",
      "204:\tlearn: 0.1843011\ttotal: 151ms\tremaining: 69.9ms\n",
      "205:\tlearn: 0.1838630\ttotal: 151ms\tremaining: 69.1ms\n",
      "206:\tlearn: 0.1826978\ttotal: 152ms\tremaining: 68.3ms\n",
      "207:\tlearn: 0.1820413\ttotal: 153ms\tremaining: 67.5ms\n",
      "208:\tlearn: 0.1816040\ttotal: 153ms\tremaining: 66.7ms\n",
      "209:\tlearn: 0.1805655\ttotal: 154ms\tremaining: 65.9ms\n",
      "210:\tlearn: 0.1795613\ttotal: 154ms\tremaining: 65.1ms\n",
      "211:\tlearn: 0.1787146\ttotal: 155ms\tremaining: 64.3ms\n",
      "212:\tlearn: 0.1779588\ttotal: 156ms\tremaining: 63.6ms\n",
      "213:\tlearn: 0.1772331\ttotal: 156ms\tremaining: 62.8ms\n",
      "214:\tlearn: 0.1761640\ttotal: 157ms\tremaining: 62.1ms\n",
      "215:\tlearn: 0.1754123\ttotal: 158ms\tremaining: 61.3ms\n",
      "216:\tlearn: 0.1749025\ttotal: 158ms\tremaining: 60.5ms\n",
      "217:\tlearn: 0.1740403\ttotal: 159ms\tremaining: 59.8ms\n",
      "218:\tlearn: 0.1734355\ttotal: 160ms\tremaining: 59ms\n",
      "219:\tlearn: 0.1728275\ttotal: 160ms\tremaining: 58.2ms\n",
      "220:\tlearn: 0.1719081\ttotal: 161ms\tremaining: 57.5ms\n",
      "221:\tlearn: 0.1712301\ttotal: 162ms\tremaining: 56.7ms\n",
      "222:\tlearn: 0.1708427\ttotal: 162ms\tremaining: 55.9ms\n",
      "223:\tlearn: 0.1705442\ttotal: 163ms\tremaining: 55.1ms\n",
      "224:\tlearn: 0.1700115\ttotal: 163ms\tremaining: 54.4ms\n",
      "225:\tlearn: 0.1695096\ttotal: 164ms\tremaining: 53.6ms\n",
      "226:\tlearn: 0.1685242\ttotal: 164ms\tremaining: 52.9ms\n",
      "227:\tlearn: 0.1677099\ttotal: 165ms\tremaining: 52.1ms\n",
      "228:\tlearn: 0.1672272\ttotal: 166ms\tremaining: 51.4ms\n",
      "229:\tlearn: 0.1664178\ttotal: 166ms\tremaining: 50.7ms\n",
      "230:\tlearn: 0.1659629\ttotal: 167ms\tremaining: 49.9ms\n",
      "231:\tlearn: 0.1651670\ttotal: 168ms\tremaining: 49.1ms\n",
      "232:\tlearn: 0.1642637\ttotal: 168ms\tremaining: 48.4ms\n",
      "233:\tlearn: 0.1637738\ttotal: 169ms\tremaining: 47.6ms\n",
      "234:\tlearn: 0.1628837\ttotal: 169ms\tremaining: 46.9ms\n",
      "235:\tlearn: 0.1616693\ttotal: 170ms\tremaining: 46.1ms\n",
      "236:\tlearn: 0.1609545\ttotal: 171ms\tremaining: 45.4ms\n",
      "237:\tlearn: 0.1601148\ttotal: 171ms\tremaining: 44.6ms\n",
      "238:\tlearn: 0.1596119\ttotal: 172ms\tremaining: 43.8ms\n",
      "239:\tlearn: 0.1592714\ttotal: 172ms\tremaining: 43.1ms\n",
      "240:\tlearn: 0.1587837\ttotal: 173ms\tremaining: 42.3ms\n",
      "241:\tlearn: 0.1577236\ttotal: 174ms\tremaining: 41.6ms\n",
      "242:\tlearn: 0.1570958\ttotal: 174ms\tremaining: 40.9ms\n",
      "243:\tlearn: 0.1565910\ttotal: 175ms\tremaining: 40.1ms\n",
      "244:\tlearn: 0.1558136\ttotal: 175ms\tremaining: 39.4ms\n",
      "245:\tlearn: 0.1553125\ttotal: 176ms\tremaining: 38.6ms\n",
      "246:\tlearn: 0.1548771\ttotal: 177ms\tremaining: 37.9ms\n",
      "247:\tlearn: 0.1541185\ttotal: 177ms\tremaining: 37.2ms\n",
      "248:\tlearn: 0.1534004\ttotal: 178ms\tremaining: 36.4ms\n",
      "249:\tlearn: 0.1527715\ttotal: 178ms\tremaining: 35.7ms\n",
      "250:\tlearn: 0.1520313\ttotal: 179ms\tremaining: 35ms\n",
      "251:\tlearn: 0.1510895\ttotal: 180ms\tremaining: 34.3ms\n",
      "252:\tlearn: 0.1505159\ttotal: 180ms\tremaining: 33.5ms\n",
      "253:\tlearn: 0.1500920\ttotal: 181ms\tremaining: 32.8ms\n",
      "254:\tlearn: 0.1497614\ttotal: 181ms\tremaining: 32ms\n",
      "255:\tlearn: 0.1490193\ttotal: 182ms\tremaining: 31.3ms\n",
      "256:\tlearn: 0.1485181\ttotal: 183ms\tremaining: 30.6ms\n",
      "257:\tlearn: 0.1479840\ttotal: 183ms\tremaining: 29.8ms\n",
      "258:\tlearn: 0.1473297\ttotal: 184ms\tremaining: 29.1ms\n",
      "259:\tlearn: 0.1468847\ttotal: 185ms\tremaining: 28.4ms\n",
      "260:\tlearn: 0.1462213\ttotal: 185ms\tremaining: 27.7ms\n",
      "261:\tlearn: 0.1456687\ttotal: 186ms\tremaining: 27ms\n",
      "262:\tlearn: 0.1448277\ttotal: 187ms\tremaining: 26.3ms\n",
      "263:\tlearn: 0.1439180\ttotal: 187ms\tremaining: 25.6ms\n",
      "264:\tlearn: 0.1430580\ttotal: 188ms\tremaining: 24.9ms\n",
      "265:\tlearn: 0.1424662\ttotal: 189ms\tremaining: 24.1ms\n",
      "266:\tlearn: 0.1417715\ttotal: 189ms\tremaining: 23.4ms\n",
      "267:\tlearn: 0.1414397\ttotal: 190ms\tremaining: 22.7ms\n",
      "268:\tlearn: 0.1407906\ttotal: 191ms\tremaining: 22ms\n",
      "269:\tlearn: 0.1403520\ttotal: 191ms\tremaining: 21.3ms\n",
      "270:\tlearn: 0.1397175\ttotal: 192ms\tremaining: 20.5ms\n",
      "271:\tlearn: 0.1391345\ttotal: 193ms\tremaining: 19.8ms\n",
      "272:\tlearn: 0.1385985\ttotal: 193ms\tremaining: 19.1ms\n",
      "273:\tlearn: 0.1378674\ttotal: 194ms\tremaining: 18.4ms\n",
      "274:\tlearn: 0.1370702\ttotal: 194ms\tremaining: 17.7ms\n",
      "275:\tlearn: 0.1364710\ttotal: 196ms\tremaining: 17ms\n",
      "276:\tlearn: 0.1359655\ttotal: 197ms\tremaining: 16.3ms\n",
      "277:\tlearn: 0.1355036\ttotal: 199ms\tremaining: 15.7ms\n",
      "278:\tlearn: 0.1347745\ttotal: 200ms\tremaining: 15ms\n",
      "279:\tlearn: 0.1345139\ttotal: 201ms\tremaining: 14.3ms\n",
      "280:\tlearn: 0.1340865\ttotal: 201ms\tremaining: 13.6ms\n",
      "281:\tlearn: 0.1334455\ttotal: 202ms\tremaining: 12.9ms\n",
      "282:\tlearn: 0.1329526\ttotal: 203ms\tremaining: 12.2ms\n",
      "283:\tlearn: 0.1328568\ttotal: 203ms\tremaining: 11.4ms\n",
      "284:\tlearn: 0.1321118\ttotal: 204ms\tremaining: 10.7ms\n",
      "285:\tlearn: 0.1318873\ttotal: 204ms\tremaining: 9.99ms\n",
      "286:\tlearn: 0.1312512\ttotal: 206ms\tremaining: 9.31ms\n",
      "287:\tlearn: 0.1308748\ttotal: 206ms\tremaining: 8.59ms\n",
      "288:\tlearn: 0.1301589\ttotal: 207ms\tremaining: 7.87ms\n",
      "289:\tlearn: 0.1295263\ttotal: 208ms\tremaining: 7.16ms\n",
      "290:\tlearn: 0.1288504\ttotal: 209ms\tremaining: 6.45ms\n",
      "291:\tlearn: 0.1284926\ttotal: 209ms\tremaining: 5.73ms\n",
      "292:\tlearn: 0.1281067\ttotal: 210ms\tremaining: 5.02ms\n",
      "293:\tlearn: 0.1276376\ttotal: 212ms\tremaining: 4.32ms\n",
      "294:\tlearn: 0.1273622\ttotal: 213ms\tremaining: 3.6ms\n",
      "295:\tlearn: 0.1269578\ttotal: 214ms\tremaining: 2.89ms\n",
      "296:\tlearn: 0.1263531\ttotal: 215ms\tremaining: 2.17ms\n",
      "297:\tlearn: 0.1258651\ttotal: 215ms\tremaining: 1.44ms\n",
      "298:\tlearn: 0.1251233\ttotal: 216ms\tremaining: 721us\n",
      "299:\tlearn: 0.1246819\ttotal: 217ms\tremaining: 0us\n",
      "Learning rate set to 0.012014\n",
      "0:\tlearn: 0.6846029\ttotal: 2.1ms\tremaining: 629ms\n",
      "1:\tlearn: 0.6765378\ttotal: 2.8ms\tremaining: 418ms\n",
      "2:\tlearn: 0.6685431\ttotal: 3.46ms\tremaining: 343ms\n",
      "3:\tlearn: 0.6618369\ttotal: 4.05ms\tremaining: 300ms\n",
      "4:\tlearn: 0.6531498\ttotal: 4.82ms\tremaining: 284ms\n",
      "5:\tlearn: 0.6463683\ttotal: 5.77ms\tremaining: 283ms\n",
      "6:\tlearn: 0.6374573\ttotal: 6.48ms\tremaining: 271ms\n",
      "7:\tlearn: 0.6307967\ttotal: 6.98ms\tremaining: 255ms\n",
      "8:\tlearn: 0.6237898\ttotal: 7.67ms\tremaining: 248ms\n",
      "9:\tlearn: 0.6181689\ttotal: 8.28ms\tremaining: 240ms\n",
      "10:\tlearn: 0.6126424\ttotal: 9.17ms\tremaining: 241ms\n",
      "11:\tlearn: 0.6061522\ttotal: 9.83ms\tremaining: 236ms\n",
      "12:\tlearn: 0.5995269\ttotal: 10.6ms\tremaining: 233ms\n",
      "13:\tlearn: 0.5944302\ttotal: 12.2ms\tremaining: 249ms\n",
      "14:\tlearn: 0.5883809\ttotal: 12.8ms\tremaining: 243ms\n",
      "15:\tlearn: 0.5815435\ttotal: 13.5ms\tremaining: 239ms\n",
      "16:\tlearn: 0.5756715\ttotal: 14.2ms\tremaining: 236ms\n",
      "17:\tlearn: 0.5725657\ttotal: 14.9ms\tremaining: 234ms\n",
      "18:\tlearn: 0.5666312\ttotal: 15.7ms\tremaining: 233ms\n",
      "19:\tlearn: 0.5613193\ttotal: 16.4ms\tremaining: 230ms\n",
      "20:\tlearn: 0.5554321\ttotal: 17.2ms\tremaining: 229ms\n",
      "21:\tlearn: 0.5492973\ttotal: 17.7ms\tremaining: 224ms\n",
      "22:\tlearn: 0.5428272\ttotal: 18.4ms\tremaining: 221ms\n",
      "23:\tlearn: 0.5377739\ttotal: 18.8ms\tremaining: 217ms\n",
      "24:\tlearn: 0.5330277\ttotal: 19.5ms\tremaining: 214ms\n",
      "25:\tlearn: 0.5317395\ttotal: 19.7ms\tremaining: 207ms\n",
      "26:\tlearn: 0.5272508\ttotal: 21.1ms\tremaining: 213ms\n",
      "27:\tlearn: 0.5219509\ttotal: 22.5ms\tremaining: 219ms\n",
      "28:\tlearn: 0.5174821\ttotal: 23.2ms\tremaining: 216ms\n",
      "29:\tlearn: 0.5128245\ttotal: 23.8ms\tremaining: 214ms\n",
      "30:\tlearn: 0.5076648\ttotal: 24.4ms\tremaining: 212ms\n",
      "31:\tlearn: 0.5046088\ttotal: 25ms\tremaining: 210ms\n",
      "32:\tlearn: 0.5001952\ttotal: 25.8ms\tremaining: 209ms\n",
      "33:\tlearn: 0.4950568\ttotal: 26.5ms\tremaining: 207ms\n",
      "34:\tlearn: 0.4900108\ttotal: 27.1ms\tremaining: 205ms\n",
      "35:\tlearn: 0.4849509\ttotal: 27.6ms\tremaining: 202ms\n",
      "36:\tlearn: 0.4803559\ttotal: 28.4ms\tremaining: 202ms\n",
      "37:\tlearn: 0.4768006\ttotal: 29ms\tremaining: 200ms\n",
      "38:\tlearn: 0.4743216\ttotal: 29.7ms\tremaining: 198ms\n",
      "39:\tlearn: 0.4706815\ttotal: 30.4ms\tremaining: 198ms\n",
      "40:\tlearn: 0.4665849\ttotal: 31.3ms\tremaining: 198ms\n",
      "41:\tlearn: 0.4621163\ttotal: 31.9ms\tremaining: 196ms\n",
      "42:\tlearn: 0.4587551\ttotal: 32.6ms\tremaining: 195ms\n",
      "43:\tlearn: 0.4542693\ttotal: 33.2ms\tremaining: 193ms\n",
      "44:\tlearn: 0.4498899\ttotal: 33.9ms\tremaining: 192ms\n",
      "45:\tlearn: 0.4466178\ttotal: 34.5ms\tremaining: 191ms\n",
      "46:\tlearn: 0.4425325\ttotal: 35.2ms\tremaining: 189ms\n",
      "47:\tlearn: 0.4401340\ttotal: 35.8ms\tremaining: 188ms\n",
      "48:\tlearn: 0.4356923\ttotal: 36.6ms\tremaining: 188ms\n",
      "49:\tlearn: 0.4320495\ttotal: 37.3ms\tremaining: 187ms\n",
      "50:\tlearn: 0.4276993\ttotal: 38.1ms\tremaining: 186ms\n",
      "51:\tlearn: 0.4251268\ttotal: 38.9ms\tremaining: 185ms\n",
      "52:\tlearn: 0.4210245\ttotal: 39.6ms\tremaining: 184ms\n",
      "53:\tlearn: 0.4182352\ttotal: 40.1ms\tremaining: 183ms\n",
      "54:\tlearn: 0.4156707\ttotal: 40.7ms\tremaining: 181ms\n",
      "55:\tlearn: 0.4129794\ttotal: 41.4ms\tremaining: 181ms\n",
      "56:\tlearn: 0.4097416\ttotal: 42.1ms\tremaining: 180ms\n",
      "57:\tlearn: 0.4066429\ttotal: 42.8ms\tremaining: 179ms\n",
      "58:\tlearn: 0.4034978\ttotal: 43.5ms\tremaining: 178ms\n",
      "59:\tlearn: 0.4018949\ttotal: 44.2ms\tremaining: 177ms\n",
      "60:\tlearn: 0.4001716\ttotal: 45.3ms\tremaining: 178ms\n",
      "61:\tlearn: 0.3976630\ttotal: 46ms\tremaining: 176ms\n",
      "62:\tlearn: 0.3944957\ttotal: 46.6ms\tremaining: 175ms\n",
      "63:\tlearn: 0.3923212\ttotal: 47.3ms\tremaining: 174ms\n",
      "64:\tlearn: 0.3894884\ttotal: 48ms\tremaining: 174ms\n",
      "65:\tlearn: 0.3858567\ttotal: 48.7ms\tremaining: 173ms\n",
      "66:\tlearn: 0.3824828\ttotal: 49.4ms\tremaining: 172ms\n",
      "67:\tlearn: 0.3797223\ttotal: 50ms\tremaining: 171ms\n",
      "68:\tlearn: 0.3764244\ttotal: 50.7ms\tremaining: 170ms\n",
      "69:\tlearn: 0.3743762\ttotal: 51.3ms\tremaining: 169ms\n",
      "70:\tlearn: 0.3726699\ttotal: 51.9ms\tremaining: 168ms\n",
      "71:\tlearn: 0.3708157\ttotal: 52.6ms\tremaining: 166ms\n",
      "72:\tlearn: 0.3682496\ttotal: 53.4ms\tremaining: 166ms\n",
      "73:\tlearn: 0.3650170\ttotal: 54.7ms\tremaining: 167ms\n",
      "74:\tlearn: 0.3612412\ttotal: 55.3ms\tremaining: 166ms\n",
      "75:\tlearn: 0.3582257\ttotal: 55.9ms\tremaining: 165ms\n",
      "76:\tlearn: 0.3563921\ttotal: 57.4ms\tremaining: 166ms\n",
      "77:\tlearn: 0.3541988\ttotal: 58.1ms\tremaining: 165ms\n",
      "78:\tlearn: 0.3515865\ttotal: 58.8ms\tremaining: 164ms\n",
      "79:\tlearn: 0.3499711\ttotal: 59.4ms\tremaining: 163ms\n",
      "80:\tlearn: 0.3472457\ttotal: 60ms\tremaining: 162ms\n",
      "81:\tlearn: 0.3444908\ttotal: 60.6ms\tremaining: 161ms\n",
      "82:\tlearn: 0.3412629\ttotal: 61.2ms\tremaining: 160ms\n",
      "83:\tlearn: 0.3399772\ttotal: 61.8ms\tremaining: 159ms\n",
      "84:\tlearn: 0.3381750\ttotal: 62.4ms\tremaining: 158ms\n",
      "85:\tlearn: 0.3352260\ttotal: 63ms\tremaining: 157ms\n",
      "86:\tlearn: 0.3333460\ttotal: 63.7ms\tremaining: 156ms\n",
      "87:\tlearn: 0.3318287\ttotal: 64.2ms\tremaining: 155ms\n",
      "88:\tlearn: 0.3291026\ttotal: 64.9ms\tremaining: 154ms\n",
      "89:\tlearn: 0.3259730\ttotal: 65.5ms\tremaining: 153ms\n",
      "90:\tlearn: 0.3238551\ttotal: 66ms\tremaining: 152ms\n",
      "91:\tlearn: 0.3210516\ttotal: 66.8ms\tremaining: 151ms\n",
      "92:\tlearn: 0.3191778\ttotal: 67.5ms\tremaining: 150ms\n",
      "93:\tlearn: 0.3164095\ttotal: 68.1ms\tremaining: 149ms\n",
      "94:\tlearn: 0.3139023\ttotal: 68.7ms\tremaining: 148ms\n",
      "95:\tlearn: 0.3126918\ttotal: 69.4ms\tremaining: 147ms\n",
      "96:\tlearn: 0.3102876\ttotal: 70.1ms\tremaining: 147ms\n",
      "97:\tlearn: 0.3074671\ttotal: 70.8ms\tremaining: 146ms\n",
      "98:\tlearn: 0.3056386\ttotal: 71.4ms\tremaining: 145ms\n",
      "99:\tlearn: 0.3024529\ttotal: 72.2ms\tremaining: 144ms\n",
      "100:\tlearn: 0.2994537\ttotal: 72.8ms\tremaining: 143ms\n",
      "101:\tlearn: 0.2971514\ttotal: 73.4ms\tremaining: 143ms\n",
      "102:\tlearn: 0.2952694\ttotal: 74ms\tremaining: 142ms\n",
      "103:\tlearn: 0.2940413\ttotal: 74.6ms\tremaining: 141ms\n",
      "104:\tlearn: 0.2926263\ttotal: 75.3ms\tremaining: 140ms\n",
      "105:\tlearn: 0.2907328\ttotal: 76ms\tremaining: 139ms\n",
      "106:\tlearn: 0.2896355\ttotal: 76.7ms\tremaining: 138ms\n",
      "107:\tlearn: 0.2878850\ttotal: 77.5ms\tremaining: 138ms\n",
      "108:\tlearn: 0.2856713\ttotal: 78.2ms\tremaining: 137ms\n",
      "109:\tlearn: 0.2835946\ttotal: 78.8ms\tremaining: 136ms\n",
      "110:\tlearn: 0.2825750\ttotal: 79.5ms\tremaining: 135ms\n",
      "111:\tlearn: 0.2809902\ttotal: 80.3ms\tremaining: 135ms\n",
      "112:\tlearn: 0.2798405\ttotal: 80.9ms\tremaining: 134ms\n",
      "113:\tlearn: 0.2779394\ttotal: 81.7ms\tremaining: 133ms\n",
      "114:\tlearn: 0.2765231\ttotal: 83.1ms\tremaining: 134ms\n",
      "115:\tlearn: 0.2747371\ttotal: 83.6ms\tremaining: 133ms\n",
      "116:\tlearn: 0.2730882\ttotal: 84.3ms\tremaining: 132ms\n",
      "117:\tlearn: 0.2719505\ttotal: 84.9ms\tremaining: 131ms\n",
      "118:\tlearn: 0.2711164\ttotal: 85.5ms\tremaining: 130ms\n",
      "119:\tlearn: 0.2700835\ttotal: 86.1ms\tremaining: 129ms\n",
      "120:\tlearn: 0.2687788\ttotal: 86.8ms\tremaining: 128ms\n",
      "121:\tlearn: 0.2668098\ttotal: 87.5ms\tremaining: 128ms\n",
      "122:\tlearn: 0.2654328\ttotal: 88.4ms\tremaining: 127ms\n",
      "123:\tlearn: 0.2644069\ttotal: 89.1ms\tremaining: 127ms\n",
      "124:\tlearn: 0.2629348\ttotal: 89.7ms\tremaining: 126ms\n",
      "125:\tlearn: 0.2613085\ttotal: 90.4ms\tremaining: 125ms\n",
      "126:\tlearn: 0.2594784\ttotal: 91.3ms\tremaining: 124ms\n",
      "127:\tlearn: 0.2573430\ttotal: 91.9ms\tremaining: 124ms\n",
      "128:\tlearn: 0.2555017\ttotal: 92.6ms\tremaining: 123ms\n",
      "129:\tlearn: 0.2541109\ttotal: 93.4ms\tremaining: 122ms\n",
      "130:\tlearn: 0.2529175\ttotal: 94.1ms\tremaining: 121ms\n",
      "131:\tlearn: 0.2508895\ttotal: 94.8ms\tremaining: 121ms\n",
      "132:\tlearn: 0.2497554\ttotal: 95.3ms\tremaining: 120ms\n",
      "133:\tlearn: 0.2488357\ttotal: 95.9ms\tremaining: 119ms\n",
      "134:\tlearn: 0.2477276\ttotal: 96.6ms\tremaining: 118ms\n",
      "135:\tlearn: 0.2469522\ttotal: 97.4ms\tremaining: 117ms\n",
      "136:\tlearn: 0.2451390\ttotal: 98.1ms\tremaining: 117ms\n",
      "137:\tlearn: 0.2438957\ttotal: 98.9ms\tremaining: 116ms\n",
      "138:\tlearn: 0.2426023\ttotal: 99.5ms\tremaining: 115ms\n",
      "139:\tlearn: 0.2416026\ttotal: 100ms\tremaining: 115ms\n",
      "140:\tlearn: 0.2404133\ttotal: 101ms\tremaining: 114ms\n",
      "141:\tlearn: 0.2398134\ttotal: 102ms\tremaining: 113ms\n",
      "142:\tlearn: 0.2388091\ttotal: 102ms\tremaining: 112ms\n",
      "143:\tlearn: 0.2375932\ttotal: 103ms\tremaining: 112ms\n",
      "144:\tlearn: 0.2365733\ttotal: 104ms\tremaining: 111ms\n",
      "145:\tlearn: 0.2355730\ttotal: 104ms\tremaining: 110ms\n",
      "146:\tlearn: 0.2340597\ttotal: 105ms\tremaining: 109ms\n",
      "147:\tlearn: 0.2328507\ttotal: 106ms\tremaining: 108ms\n",
      "148:\tlearn: 0.2315037\ttotal: 106ms\tremaining: 108ms\n",
      "149:\tlearn: 0.2298433\ttotal: 107ms\tremaining: 107ms\n",
      "150:\tlearn: 0.2287489\ttotal: 107ms\tremaining: 106ms\n",
      "151:\tlearn: 0.2281975\ttotal: 108ms\tremaining: 105ms\n",
      "152:\tlearn: 0.2275484\ttotal: 109ms\tremaining: 104ms\n",
      "153:\tlearn: 0.2266018\ttotal: 109ms\tremaining: 104ms\n",
      "154:\tlearn: 0.2252886\ttotal: 110ms\tremaining: 103ms\n",
      "155:\tlearn: 0.2247827\ttotal: 111ms\tremaining: 102ms\n",
      "156:\tlearn: 0.2231661\ttotal: 111ms\tremaining: 101ms\n",
      "157:\tlearn: 0.2216156\ttotal: 112ms\tremaining: 101ms\n",
      "158:\tlearn: 0.2206588\ttotal: 113ms\tremaining: 100ms\n",
      "159:\tlearn: 0.2196456\ttotal: 113ms\tremaining: 99.2ms\n",
      "160:\tlearn: 0.2182593\ttotal: 114ms\tremaining: 98.4ms\n",
      "161:\tlearn: 0.2174780\ttotal: 115ms\tremaining: 97.7ms\n",
      "162:\tlearn: 0.2160633\ttotal: 115ms\tremaining: 96.9ms\n",
      "163:\tlearn: 0.2160348\ttotal: 116ms\tremaining: 95.8ms\n",
      "164:\tlearn: 0.2152062\ttotal: 116ms\tremaining: 95ms\n",
      "165:\tlearn: 0.2147582\ttotal: 117ms\tremaining: 94.2ms\n",
      "166:\tlearn: 0.2143441\ttotal: 117ms\tremaining: 93.4ms\n",
      "167:\tlearn: 0.2130981\ttotal: 118ms\tremaining: 92.6ms\n",
      "168:\tlearn: 0.2125942\ttotal: 119ms\tremaining: 91.9ms\n",
      "169:\tlearn: 0.2115342\ttotal: 119ms\tremaining: 91.2ms\n",
      "170:\tlearn: 0.2104830\ttotal: 120ms\tremaining: 90.6ms\n",
      "171:\tlearn: 0.2096320\ttotal: 121ms\tremaining: 89.9ms\n",
      "172:\tlearn: 0.2089288\ttotal: 121ms\tremaining: 89.1ms\n",
      "173:\tlearn: 0.2077670\ttotal: 122ms\tremaining: 88.3ms\n",
      "174:\tlearn: 0.2073243\ttotal: 123ms\tremaining: 87.6ms\n",
      "175:\tlearn: 0.2066296\ttotal: 123ms\tremaining: 86.8ms\n",
      "176:\tlearn: 0.2053745\ttotal: 124ms\tremaining: 86.1ms\n",
      "177:\tlearn: 0.2046522\ttotal: 125ms\tremaining: 85.4ms\n",
      "178:\tlearn: 0.2036210\ttotal: 125ms\tremaining: 84.7ms\n",
      "179:\tlearn: 0.2023743\ttotal: 126ms\tremaining: 83.9ms\n",
      "180:\tlearn: 0.2011421\ttotal: 127ms\tremaining: 83.2ms\n",
      "181:\tlearn: 0.2002770\ttotal: 128ms\tremaining: 82.9ms\n",
      "182:\tlearn: 0.1994521\ttotal: 128ms\tremaining: 82.1ms\n",
      "183:\tlearn: 0.1982313\ttotal: 130ms\tremaining: 81.9ms\n",
      "184:\tlearn: 0.1974684\ttotal: 131ms\tremaining: 81.1ms\n",
      "185:\tlearn: 0.1960141\ttotal: 131ms\tremaining: 80.4ms\n",
      "186:\tlearn: 0.1956185\ttotal: 132ms\tremaining: 79.7ms\n",
      "187:\tlearn: 0.1946199\ttotal: 132ms\tremaining: 78.9ms\n",
      "188:\tlearn: 0.1937529\ttotal: 133ms\tremaining: 78.2ms\n",
      "189:\tlearn: 0.1930800\ttotal: 134ms\tremaining: 77.5ms\n",
      "190:\tlearn: 0.1924131\ttotal: 135ms\tremaining: 76.9ms\n",
      "191:\tlearn: 0.1916576\ttotal: 136ms\tremaining: 76.3ms\n",
      "192:\tlearn: 0.1908435\ttotal: 136ms\tremaining: 75.6ms\n",
      "193:\tlearn: 0.1898367\ttotal: 137ms\tremaining: 75ms\n",
      "194:\tlearn: 0.1889181\ttotal: 138ms\tremaining: 74.3ms\n",
      "195:\tlearn: 0.1882279\ttotal: 139ms\tremaining: 73.5ms\n",
      "196:\tlearn: 0.1873366\ttotal: 139ms\tremaining: 72.8ms\n",
      "197:\tlearn: 0.1865922\ttotal: 140ms\tremaining: 72.1ms\n",
      "198:\tlearn: 0.1860494\ttotal: 141ms\tremaining: 71.3ms\n",
      "199:\tlearn: 0.1851711\ttotal: 141ms\tremaining: 70.6ms\n",
      "200:\tlearn: 0.1843875\ttotal: 142ms\tremaining: 70ms\n",
      "201:\tlearn: 0.1834237\ttotal: 143ms\tremaining: 69.2ms\n",
      "202:\tlearn: 0.1826237\ttotal: 143ms\tremaining: 68.5ms\n",
      "203:\tlearn: 0.1819383\ttotal: 144ms\tremaining: 67.8ms\n",
      "204:\tlearn: 0.1810821\ttotal: 145ms\tremaining: 67ms\n",
      "205:\tlearn: 0.1806689\ttotal: 145ms\tremaining: 66.4ms\n",
      "206:\tlearn: 0.1794319\ttotal: 146ms\tremaining: 65.6ms\n",
      "207:\tlearn: 0.1787534\ttotal: 147ms\tremaining: 64.9ms\n",
      "208:\tlearn: 0.1778263\ttotal: 147ms\tremaining: 64.2ms\n",
      "209:\tlearn: 0.1768687\ttotal: 148ms\tremaining: 63.4ms\n",
      "210:\tlearn: 0.1759166\ttotal: 149ms\tremaining: 62.7ms\n",
      "211:\tlearn: 0.1754376\ttotal: 149ms\tremaining: 62ms\n",
      "212:\tlearn: 0.1743072\ttotal: 150ms\tremaining: 61.3ms\n",
      "213:\tlearn: 0.1732293\ttotal: 151ms\tremaining: 60.6ms\n",
      "214:\tlearn: 0.1726443\ttotal: 152ms\tremaining: 60ms\n",
      "215:\tlearn: 0.1721198\ttotal: 152ms\tremaining: 59.3ms\n",
      "216:\tlearn: 0.1710144\ttotal: 153ms\tremaining: 58.6ms\n",
      "217:\tlearn: 0.1703258\ttotal: 154ms\tremaining: 57.9ms\n",
      "218:\tlearn: 0.1697192\ttotal: 154ms\tremaining: 57.1ms\n",
      "219:\tlearn: 0.1685541\ttotal: 155ms\tremaining: 56.4ms\n",
      "220:\tlearn: 0.1679107\ttotal: 156ms\tremaining: 55.7ms\n",
      "221:\tlearn: 0.1669698\ttotal: 157ms\tremaining: 55ms\n",
      "222:\tlearn: 0.1664708\ttotal: 157ms\tremaining: 54.3ms\n",
      "223:\tlearn: 0.1654690\ttotal: 158ms\tremaining: 53.6ms\n",
      "224:\tlearn: 0.1648230\ttotal: 159ms\tremaining: 52.9ms\n",
      "225:\tlearn: 0.1643671\ttotal: 159ms\tremaining: 52.1ms\n",
      "226:\tlearn: 0.1634307\ttotal: 160ms\tremaining: 51.4ms\n",
      "227:\tlearn: 0.1622980\ttotal: 160ms\tremaining: 50.7ms\n",
      "228:\tlearn: 0.1617405\ttotal: 162ms\tremaining: 50.2ms\n",
      "229:\tlearn: 0.1608619\ttotal: 162ms\tremaining: 49.4ms\n",
      "230:\tlearn: 0.1601072\ttotal: 163ms\tremaining: 48.7ms\n",
      "231:\tlearn: 0.1593824\ttotal: 164ms\tremaining: 48ms\n",
      "232:\tlearn: 0.1586642\ttotal: 164ms\tremaining: 47.3ms\n",
      "233:\tlearn: 0.1580075\ttotal: 165ms\tremaining: 46.5ms\n",
      "234:\tlearn: 0.1567225\ttotal: 166ms\tremaining: 45.9ms\n",
      "235:\tlearn: 0.1560781\ttotal: 166ms\tremaining: 45.1ms\n",
      "236:\tlearn: 0.1551817\ttotal: 167ms\tremaining: 44.4ms\n",
      "237:\tlearn: 0.1545521\ttotal: 168ms\tremaining: 43.7ms\n",
      "238:\tlearn: 0.1540451\ttotal: 169ms\tremaining: 43.1ms\n",
      "239:\tlearn: 0.1533290\ttotal: 170ms\tremaining: 42.4ms\n",
      "240:\tlearn: 0.1526189\ttotal: 170ms\tremaining: 41.7ms\n",
      "241:\tlearn: 0.1522623\ttotal: 172ms\tremaining: 41.2ms\n",
      "242:\tlearn: 0.1516013\ttotal: 173ms\tremaining: 40.5ms\n",
      "243:\tlearn: 0.1511169\ttotal: 173ms\tremaining: 39.7ms\n",
      "244:\tlearn: 0.1500375\ttotal: 174ms\tremaining: 39ms\n",
      "245:\tlearn: 0.1495676\ttotal: 175ms\tremaining: 38.3ms\n",
      "246:\tlearn: 0.1488838\ttotal: 175ms\tremaining: 37.6ms\n",
      "247:\tlearn: 0.1485150\ttotal: 176ms\tremaining: 36.9ms\n",
      "248:\tlearn: 0.1481498\ttotal: 177ms\tremaining: 36.2ms\n",
      "249:\tlearn: 0.1471905\ttotal: 177ms\tremaining: 35.5ms\n",
      "250:\tlearn: 0.1467148\ttotal: 178ms\tremaining: 34.7ms\n",
      "251:\tlearn: 0.1459766\ttotal: 179ms\tremaining: 34ms\n",
      "252:\tlearn: 0.1450700\ttotal: 180ms\tremaining: 33.3ms\n",
      "253:\tlearn: 0.1446599\ttotal: 180ms\tremaining: 32.7ms\n",
      "254:\tlearn: 0.1443356\ttotal: 181ms\tremaining: 31.9ms\n",
      "255:\tlearn: 0.1436845\ttotal: 182ms\tremaining: 31.2ms\n",
      "256:\tlearn: 0.1430533\ttotal: 182ms\tremaining: 30.5ms\n",
      "257:\tlearn: 0.1428977\ttotal: 183ms\tremaining: 29.8ms\n",
      "258:\tlearn: 0.1419699\ttotal: 184ms\tremaining: 29.1ms\n",
      "259:\tlearn: 0.1416025\ttotal: 185ms\tremaining: 28.4ms\n",
      "260:\tlearn: 0.1411987\ttotal: 186ms\tremaining: 27.7ms\n",
      "261:\tlearn: 0.1407290\ttotal: 186ms\tremaining: 27ms\n",
      "262:\tlearn: 0.1401634\ttotal: 187ms\tremaining: 26.3ms\n",
      "263:\tlearn: 0.1396341\ttotal: 188ms\tremaining: 25.6ms\n",
      "264:\tlearn: 0.1390830\ttotal: 189ms\tremaining: 24.9ms\n",
      "265:\tlearn: 0.1383842\ttotal: 189ms\tremaining: 24.2ms\n",
      "266:\tlearn: 0.1376862\ttotal: 190ms\tremaining: 23.5ms\n",
      "267:\tlearn: 0.1370454\ttotal: 191ms\tremaining: 22.8ms\n",
      "268:\tlearn: 0.1362985\ttotal: 191ms\tremaining: 22ms\n",
      "269:\tlearn: 0.1357254\ttotal: 192ms\tremaining: 21.3ms\n",
      "270:\tlearn: 0.1352697\ttotal: 192ms\tremaining: 20.6ms\n",
      "271:\tlearn: 0.1346198\ttotal: 193ms\tremaining: 19.9ms\n",
      "272:\tlearn: 0.1340902\ttotal: 194ms\tremaining: 19.2ms\n",
      "273:\tlearn: 0.1332927\ttotal: 195ms\tremaining: 18.5ms\n",
      "274:\tlearn: 0.1329590\ttotal: 195ms\tremaining: 17.8ms\n",
      "275:\tlearn: 0.1323472\ttotal: 196ms\tremaining: 17ms\n",
      "276:\tlearn: 0.1320989\ttotal: 197ms\tremaining: 16.3ms\n",
      "277:\tlearn: 0.1315525\ttotal: 197ms\tremaining: 15.6ms\n",
      "278:\tlearn: 0.1313294\ttotal: 198ms\tremaining: 14.9ms\n",
      "279:\tlearn: 0.1307851\ttotal: 199ms\tremaining: 14.2ms\n",
      "280:\tlearn: 0.1305573\ttotal: 200ms\tremaining: 13.5ms\n",
      "281:\tlearn: 0.1295150\ttotal: 201ms\tremaining: 12.8ms\n",
      "282:\tlearn: 0.1290447\ttotal: 202ms\tremaining: 12.1ms\n",
      "283:\tlearn: 0.1287926\ttotal: 203ms\tremaining: 11.4ms\n",
      "284:\tlearn: 0.1285011\ttotal: 203ms\tremaining: 10.7ms\n",
      "285:\tlearn: 0.1276577\ttotal: 204ms\tremaining: 9.99ms\n",
      "286:\tlearn: 0.1273731\ttotal: 205ms\tremaining: 9.28ms\n",
      "287:\tlearn: 0.1268323\ttotal: 206ms\tremaining: 8.59ms\n",
      "288:\tlearn: 0.1262106\ttotal: 207ms\tremaining: 7.87ms\n",
      "289:\tlearn: 0.1258028\ttotal: 208ms\tremaining: 7.17ms\n",
      "290:\tlearn: 0.1252549\ttotal: 209ms\tremaining: 6.46ms\n",
      "291:\tlearn: 0.1248484\ttotal: 210ms\tremaining: 5.74ms\n",
      "292:\tlearn: 0.1241857\ttotal: 210ms\tremaining: 5.02ms\n",
      "293:\tlearn: 0.1237299\ttotal: 211ms\tremaining: 4.3ms\n",
      "294:\tlearn: 0.1234005\ttotal: 212ms\tremaining: 3.59ms\n",
      "295:\tlearn: 0.1227977\ttotal: 212ms\tremaining: 2.87ms\n",
      "296:\tlearn: 0.1222145\ttotal: 213ms\tremaining: 2.15ms\n",
      "297:\tlearn: 0.1219459\ttotal: 214ms\tremaining: 1.44ms\n",
      "298:\tlearn: 0.1214276\ttotal: 215ms\tremaining: 718us\n",
      "299:\tlearn: 0.1208263\ttotal: 215ms\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "sim.catboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CatBoostClassifier.score() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: CatBoostClassifier.score() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "sim.fit_model.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.010625\n",
      "0:\tlearn: 0.6852129\ttotal: 60.9ms\tremaining: 18.2s\n",
      "1:\tlearn: 0.6775699\ttotal: 62.4ms\tremaining: 9.3s\n",
      "2:\tlearn: 0.6707844\ttotal: 64.5ms\tremaining: 6.39s\n",
      "3:\tlearn: 0.6628242\ttotal: 65.8ms\tremaining: 4.87s\n",
      "4:\tlearn: 0.6586915\ttotal: 67.8ms\tremaining: 4s\n",
      "5:\tlearn: 0.6521197\ttotal: 69.9ms\tremaining: 3.42s\n",
      "6:\tlearn: 0.6454259\ttotal: 71.1ms\tremaining: 2.98s\n",
      "7:\tlearn: 0.6401258\ttotal: 72.1ms\tremaining: 2.63s\n",
      "8:\tlearn: 0.6352136\ttotal: 73.3ms\tremaining: 2.37s\n",
      "9:\tlearn: 0.6282357\ttotal: 74.5ms\tremaining: 2.16s\n",
      "10:\tlearn: 0.6239145\ttotal: 75.4ms\tremaining: 1.98s\n",
      "11:\tlearn: 0.6194098\ttotal: 75.9ms\tremaining: 1.82s\n",
      "12:\tlearn: 0.6154147\ttotal: 76.5ms\tremaining: 1.69s\n",
      "13:\tlearn: 0.6099335\ttotal: 77.7ms\tremaining: 1.59s\n",
      "14:\tlearn: 0.6042443\ttotal: 78.9ms\tremaining: 1.5s\n",
      "15:\tlearn: 0.5979067\ttotal: 79.9ms\tremaining: 1.42s\n",
      "16:\tlearn: 0.5922089\ttotal: 80.9ms\tremaining: 1.35s\n",
      "17:\tlearn: 0.5870287\ttotal: 81.8ms\tremaining: 1.28s\n",
      "18:\tlearn: 0.5815895\ttotal: 82.6ms\tremaining: 1.22s\n",
      "19:\tlearn: 0.5772209\ttotal: 83.4ms\tremaining: 1.17s\n",
      "20:\tlearn: 0.5734577\ttotal: 84.2ms\tremaining: 1.12s\n",
      "21:\tlearn: 0.5681447\ttotal: 85.1ms\tremaining: 1.07s\n",
      "22:\tlearn: 0.5612580\ttotal: 86.6ms\tremaining: 1.04s\n",
      "23:\tlearn: 0.5565171\ttotal: 87.5ms\tremaining: 1.01s\n",
      "24:\tlearn: 0.5529483\ttotal: 88.4ms\tremaining: 972ms\n",
      "25:\tlearn: 0.5474920\ttotal: 89.3ms\tremaining: 941ms\n",
      "26:\tlearn: 0.5449934\ttotal: 90.2ms\tremaining: 912ms\n",
      "27:\tlearn: 0.5397018\ttotal: 91.3ms\tremaining: 887ms\n",
      "28:\tlearn: 0.5339462\ttotal: 92.4ms\tremaining: 863ms\n",
      "29:\tlearn: 0.5328347\ttotal: 92.7ms\tremaining: 834ms\n",
      "30:\tlearn: 0.5282740\ttotal: 93.6ms\tremaining: 812ms\n",
      "31:\tlearn: 0.5247046\ttotal: 94.4ms\tremaining: 791ms\n",
      "32:\tlearn: 0.5207727\ttotal: 95.3ms\tremaining: 771ms\n",
      "33:\tlearn: 0.5167016\ttotal: 95.8ms\tremaining: 750ms\n",
      "34:\tlearn: 0.5124685\ttotal: 96.6ms\tremaining: 732ms\n",
      "35:\tlearn: 0.5078874\ttotal: 97.3ms\tremaining: 714ms\n",
      "36:\tlearn: 0.5030360\ttotal: 98.1ms\tremaining: 697ms\n",
      "37:\tlearn: 0.5010005\ttotal: 98.5ms\tremaining: 679ms\n",
      "38:\tlearn: 0.4978862\ttotal: 99.9ms\tremaining: 668ms\n",
      "39:\tlearn: 0.4936634\ttotal: 101ms\tremaining: 654ms\n",
      "40:\tlearn: 0.4895817\ttotal: 101ms\tremaining: 640ms\n",
      "41:\tlearn: 0.4847478\ttotal: 102ms\tremaining: 627ms\n",
      "42:\tlearn: 0.4826953\ttotal: 103ms\tremaining: 614ms\n",
      "43:\tlearn: 0.4784359\ttotal: 104ms\tremaining: 602ms\n",
      "44:\tlearn: 0.4744074\ttotal: 104ms\tremaining: 591ms\n",
      "45:\tlearn: 0.4701132\ttotal: 105ms\tremaining: 580ms\n",
      "46:\tlearn: 0.4670180\ttotal: 106ms\tremaining: 571ms\n",
      "47:\tlearn: 0.4640474\ttotal: 107ms\tremaining: 560ms\n",
      "48:\tlearn: 0.4605014\ttotal: 108ms\tremaining: 554ms\n",
      "49:\tlearn: 0.4571705\ttotal: 109ms\tremaining: 545ms\n",
      "50:\tlearn: 0.4539639\ttotal: 110ms\tremaining: 535ms\n",
      "51:\tlearn: 0.4508636\ttotal: 110ms\tremaining: 527ms\n",
      "52:\tlearn: 0.4475152\ttotal: 111ms\tremaining: 518ms\n",
      "53:\tlearn: 0.4435729\ttotal: 112ms\tremaining: 510ms\n",
      "54:\tlearn: 0.4394994\ttotal: 113ms\tremaining: 501ms\n",
      "55:\tlearn: 0.4357934\ttotal: 113ms\tremaining: 494ms\n",
      "56:\tlearn: 0.4320338\ttotal: 114ms\tremaining: 486ms\n",
      "57:\tlearn: 0.4285896\ttotal: 115ms\tremaining: 478ms\n",
      "58:\tlearn: 0.4265055\ttotal: 115ms\tremaining: 471ms\n",
      "59:\tlearn: 0.4236146\ttotal: 116ms\tremaining: 464ms\n",
      "60:\tlearn: 0.4200178\ttotal: 117ms\tremaining: 457ms\n",
      "61:\tlearn: 0.4166400\ttotal: 117ms\tremaining: 450ms\n",
      "62:\tlearn: 0.4142666\ttotal: 118ms\tremaining: 444ms\n",
      "63:\tlearn: 0.4115368\ttotal: 119ms\tremaining: 438ms\n",
      "64:\tlearn: 0.4095780\ttotal: 120ms\tremaining: 433ms\n",
      "65:\tlearn: 0.4063057\ttotal: 120ms\tremaining: 427ms\n",
      "66:\tlearn: 0.4035314\ttotal: 121ms\tremaining: 422ms\n",
      "67:\tlearn: 0.4018263\ttotal: 122ms\tremaining: 416ms\n",
      "68:\tlearn: 0.3986538\ttotal: 123ms\tremaining: 410ms\n",
      "69:\tlearn: 0.3952129\ttotal: 123ms\tremaining: 405ms\n",
      "70:\tlearn: 0.3928223\ttotal: 124ms\tremaining: 400ms\n",
      "71:\tlearn: 0.3897533\ttotal: 125ms\tremaining: 395ms\n",
      "72:\tlearn: 0.3869911\ttotal: 125ms\tremaining: 390ms\n",
      "73:\tlearn: 0.3851620\ttotal: 126ms\tremaining: 385ms\n",
      "74:\tlearn: 0.3831536\ttotal: 127ms\tremaining: 381ms\n",
      "75:\tlearn: 0.3810990\ttotal: 128ms\tremaining: 376ms\n",
      "76:\tlearn: 0.3782301\ttotal: 128ms\tremaining: 371ms\n",
      "77:\tlearn: 0.3765860\ttotal: 129ms\tremaining: 367ms\n",
      "78:\tlearn: 0.3737041\ttotal: 130ms\tremaining: 363ms\n",
      "79:\tlearn: 0.3717324\ttotal: 130ms\tremaining: 359ms\n",
      "80:\tlearn: 0.3697424\ttotal: 131ms\tremaining: 354ms\n",
      "81:\tlearn: 0.3671961\ttotal: 132ms\tremaining: 350ms\n",
      "82:\tlearn: 0.3649472\ttotal: 132ms\tremaining: 346ms\n",
      "83:\tlearn: 0.3613996\ttotal: 133ms\tremaining: 342ms\n",
      "84:\tlearn: 0.3585493\ttotal: 134ms\tremaining: 338ms\n",
      "85:\tlearn: 0.3571278\ttotal: 134ms\tremaining: 334ms\n",
      "86:\tlearn: 0.3549038\ttotal: 135ms\tremaining: 330ms\n",
      "87:\tlearn: 0.3519606\ttotal: 136ms\tremaining: 327ms\n",
      "88:\tlearn: 0.3490309\ttotal: 136ms\tremaining: 323ms\n",
      "89:\tlearn: 0.3468736\ttotal: 137ms\tremaining: 320ms\n",
      "90:\tlearn: 0.3438070\ttotal: 138ms\tremaining: 316ms\n",
      "91:\tlearn: 0.3415245\ttotal: 138ms\tremaining: 313ms\n",
      "92:\tlearn: 0.3394707\ttotal: 139ms\tremaining: 309ms\n",
      "93:\tlearn: 0.3374091\ttotal: 140ms\tremaining: 308ms\n",
      "94:\tlearn: 0.3346044\ttotal: 141ms\tremaining: 304ms\n",
      "95:\tlearn: 0.3330305\ttotal: 142ms\tremaining: 301ms\n",
      "96:\tlearn: 0.3307224\ttotal: 142ms\tremaining: 298ms\n",
      "97:\tlearn: 0.3284235\ttotal: 143ms\tremaining: 295ms\n",
      "98:\tlearn: 0.3262762\ttotal: 144ms\tremaining: 292ms\n",
      "99:\tlearn: 0.3247432\ttotal: 145ms\tremaining: 289ms\n",
      "100:\tlearn: 0.3220269\ttotal: 145ms\tremaining: 286ms\n",
      "101:\tlearn: 0.3202033\ttotal: 146ms\tremaining: 283ms\n",
      "102:\tlearn: 0.3183692\ttotal: 147ms\tremaining: 281ms\n",
      "103:\tlearn: 0.3155968\ttotal: 148ms\tremaining: 279ms\n",
      "104:\tlearn: 0.3131830\ttotal: 149ms\tremaining: 276ms\n",
      "105:\tlearn: 0.3109495\ttotal: 149ms\tremaining: 273ms\n",
      "106:\tlearn: 0.3093617\ttotal: 150ms\tremaining: 271ms\n",
      "107:\tlearn: 0.3071652\ttotal: 151ms\tremaining: 268ms\n",
      "108:\tlearn: 0.3065078\ttotal: 152ms\tremaining: 266ms\n",
      "109:\tlearn: 0.3042277\ttotal: 152ms\tremaining: 263ms\n",
      "110:\tlearn: 0.3025427\ttotal: 154ms\tremaining: 262ms\n",
      "111:\tlearn: 0.3005915\ttotal: 154ms\tremaining: 259ms\n",
      "112:\tlearn: 0.2991349\ttotal: 155ms\tremaining: 257ms\n",
      "113:\tlearn: 0.2967500\ttotal: 156ms\tremaining: 254ms\n",
      "114:\tlearn: 0.2960848\ttotal: 156ms\tremaining: 251ms\n",
      "115:\tlearn: 0.2935405\ttotal: 157ms\tremaining: 250ms\n",
      "116:\tlearn: 0.2924557\ttotal: 158ms\tremaining: 248ms\n",
      "117:\tlearn: 0.2909870\ttotal: 159ms\tremaining: 245ms\n",
      "118:\tlearn: 0.2893737\ttotal: 160ms\tremaining: 243ms\n",
      "119:\tlearn: 0.2873170\ttotal: 160ms\tremaining: 241ms\n",
      "120:\tlearn: 0.2856616\ttotal: 161ms\tremaining: 238ms\n",
      "121:\tlearn: 0.2841130\ttotal: 162ms\tremaining: 236ms\n",
      "122:\tlearn: 0.2825976\ttotal: 163ms\tremaining: 234ms\n",
      "123:\tlearn: 0.2815259\ttotal: 163ms\tremaining: 231ms\n",
      "124:\tlearn: 0.2796512\ttotal: 164ms\tremaining: 229ms\n",
      "125:\tlearn: 0.2776673\ttotal: 164ms\tremaining: 227ms\n",
      "126:\tlearn: 0.2761645\ttotal: 165ms\tremaining: 225ms\n",
      "127:\tlearn: 0.2739920\ttotal: 166ms\tremaining: 223ms\n",
      "128:\tlearn: 0.2724222\ttotal: 166ms\tremaining: 221ms\n",
      "129:\tlearn: 0.2707469\ttotal: 167ms\tremaining: 219ms\n",
      "130:\tlearn: 0.2689728\ttotal: 168ms\tremaining: 217ms\n",
      "131:\tlearn: 0.2672466\ttotal: 169ms\tremaining: 215ms\n",
      "132:\tlearn: 0.2655209\ttotal: 170ms\tremaining: 213ms\n",
      "133:\tlearn: 0.2636280\ttotal: 171ms\tremaining: 211ms\n",
      "134:\tlearn: 0.2614258\ttotal: 171ms\tremaining: 209ms\n",
      "135:\tlearn: 0.2609400\ttotal: 172ms\tremaining: 207ms\n",
      "136:\tlearn: 0.2591781\ttotal: 172ms\tremaining: 205ms\n",
      "137:\tlearn: 0.2579704\ttotal: 173ms\tremaining: 203ms\n",
      "138:\tlearn: 0.2565690\ttotal: 174ms\tremaining: 201ms\n",
      "139:\tlearn: 0.2548773\ttotal: 174ms\tremaining: 199ms\n",
      "140:\tlearn: 0.2538787\ttotal: 175ms\tremaining: 197ms\n",
      "141:\tlearn: 0.2523724\ttotal: 176ms\tremaining: 196ms\n",
      "142:\tlearn: 0.2511521\ttotal: 176ms\tremaining: 194ms\n",
      "143:\tlearn: 0.2505298\ttotal: 177ms\tremaining: 192ms\n",
      "144:\tlearn: 0.2495994\ttotal: 178ms\tremaining: 190ms\n",
      "145:\tlearn: 0.2492455\ttotal: 178ms\tremaining: 188ms\n",
      "146:\tlearn: 0.2480185\ttotal: 179ms\tremaining: 186ms\n",
      "147:\tlearn: 0.2464651\ttotal: 180ms\tremaining: 185ms\n",
      "148:\tlearn: 0.2459173\ttotal: 180ms\tremaining: 183ms\n",
      "149:\tlearn: 0.2446080\ttotal: 181ms\tremaining: 181ms\n",
      "150:\tlearn: 0.2430202\ttotal: 182ms\tremaining: 179ms\n",
      "151:\tlearn: 0.2416528\ttotal: 182ms\tremaining: 178ms\n",
      "152:\tlearn: 0.2400896\ttotal: 183ms\tremaining: 176ms\n",
      "153:\tlearn: 0.2386576\ttotal: 184ms\tremaining: 174ms\n",
      "154:\tlearn: 0.2372514\ttotal: 185ms\tremaining: 173ms\n",
      "155:\tlearn: 0.2361041\ttotal: 185ms\tremaining: 171ms\n",
      "156:\tlearn: 0.2347943\ttotal: 186ms\tremaining: 169ms\n",
      "157:\tlearn: 0.2339571\ttotal: 187ms\tremaining: 168ms\n",
      "158:\tlearn: 0.2321934\ttotal: 187ms\tremaining: 166ms\n",
      "159:\tlearn: 0.2308757\ttotal: 188ms\tremaining: 165ms\n",
      "160:\tlearn: 0.2297860\ttotal: 189ms\tremaining: 163ms\n",
      "161:\tlearn: 0.2283719\ttotal: 190ms\tremaining: 162ms\n",
      "162:\tlearn: 0.2270632\ttotal: 190ms\tremaining: 160ms\n",
      "163:\tlearn: 0.2259522\ttotal: 191ms\tremaining: 159ms\n",
      "164:\tlearn: 0.2247999\ttotal: 192ms\tremaining: 157ms\n",
      "165:\tlearn: 0.2234065\ttotal: 193ms\tremaining: 155ms\n",
      "166:\tlearn: 0.2219139\ttotal: 193ms\tremaining: 154ms\n",
      "167:\tlearn: 0.2203460\ttotal: 194ms\tremaining: 152ms\n",
      "168:\tlearn: 0.2190989\ttotal: 195ms\tremaining: 151ms\n",
      "169:\tlearn: 0.2177756\ttotal: 195ms\tremaining: 149ms\n",
      "170:\tlearn: 0.2165547\ttotal: 196ms\tremaining: 148ms\n",
      "171:\tlearn: 0.2161599\ttotal: 197ms\tremaining: 147ms\n",
      "172:\tlearn: 0.2154968\ttotal: 198ms\tremaining: 145ms\n",
      "173:\tlearn: 0.2143610\ttotal: 199ms\tremaining: 144ms\n",
      "174:\tlearn: 0.2131530\ttotal: 199ms\tremaining: 142ms\n",
      "175:\tlearn: 0.2116314\ttotal: 200ms\tremaining: 141ms\n",
      "176:\tlearn: 0.2103585\ttotal: 201ms\tremaining: 139ms\n",
      "177:\tlearn: 0.2088010\ttotal: 201ms\tremaining: 138ms\n",
      "178:\tlearn: 0.2076326\ttotal: 203ms\tremaining: 137ms\n",
      "179:\tlearn: 0.2063373\ttotal: 204ms\tremaining: 136ms\n",
      "180:\tlearn: 0.2052967\ttotal: 205ms\tremaining: 135ms\n",
      "181:\tlearn: 0.2040239\ttotal: 205ms\tremaining: 133ms\n",
      "182:\tlearn: 0.2025681\ttotal: 206ms\tremaining: 132ms\n",
      "183:\tlearn: 0.2014780\ttotal: 208ms\tremaining: 131ms\n",
      "184:\tlearn: 0.2006595\ttotal: 208ms\tremaining: 130ms\n",
      "185:\tlearn: 0.1995995\ttotal: 210ms\tremaining: 128ms\n",
      "186:\tlearn: 0.1988188\ttotal: 211ms\tremaining: 127ms\n",
      "187:\tlearn: 0.1976094\ttotal: 212ms\tremaining: 126ms\n",
      "188:\tlearn: 0.1964087\ttotal: 213ms\tremaining: 125ms\n",
      "189:\tlearn: 0.1957069\ttotal: 213ms\tremaining: 123ms\n",
      "190:\tlearn: 0.1947645\ttotal: 214ms\tremaining: 122ms\n",
      "191:\tlearn: 0.1938033\ttotal: 216ms\tremaining: 121ms\n",
      "192:\tlearn: 0.1930228\ttotal: 216ms\tremaining: 120ms\n",
      "193:\tlearn: 0.1918844\ttotal: 217ms\tremaining: 119ms\n",
      "194:\tlearn: 0.1914199\ttotal: 218ms\tremaining: 117ms\n",
      "195:\tlearn: 0.1906849\ttotal: 219ms\tremaining: 116ms\n",
      "196:\tlearn: 0.1899441\ttotal: 220ms\tremaining: 115ms\n",
      "197:\tlearn: 0.1887767\ttotal: 221ms\tremaining: 114ms\n",
      "198:\tlearn: 0.1876327\ttotal: 221ms\tremaining: 112ms\n",
      "199:\tlearn: 0.1869851\ttotal: 222ms\tremaining: 111ms\n",
      "200:\tlearn: 0.1864449\ttotal: 223ms\tremaining: 110ms\n",
      "201:\tlearn: 0.1852464\ttotal: 224ms\tremaining: 108ms\n",
      "202:\tlearn: 0.1840188\ttotal: 224ms\tremaining: 107ms\n",
      "203:\tlearn: 0.1833862\ttotal: 225ms\tremaining: 106ms\n",
      "204:\tlearn: 0.1826895\ttotal: 226ms\tremaining: 105ms\n",
      "205:\tlearn: 0.1820751\ttotal: 226ms\tremaining: 103ms\n",
      "206:\tlearn: 0.1813689\ttotal: 227ms\tremaining: 102ms\n",
      "207:\tlearn: 0.1811789\ttotal: 227ms\tremaining: 101ms\n",
      "208:\tlearn: 0.1808745\ttotal: 228ms\tremaining: 99.3ms\n",
      "209:\tlearn: 0.1799057\ttotal: 229ms\tremaining: 98ms\n",
      "210:\tlearn: 0.1790226\ttotal: 229ms\tremaining: 96.7ms\n",
      "211:\tlearn: 0.1783591\ttotal: 230ms\tremaining: 95.5ms\n",
      "212:\tlearn: 0.1775277\ttotal: 231ms\tremaining: 94.3ms\n",
      "213:\tlearn: 0.1764357\ttotal: 232ms\tremaining: 93ms\n",
      "214:\tlearn: 0.1755010\ttotal: 232ms\tremaining: 91.8ms\n",
      "215:\tlearn: 0.1743628\ttotal: 233ms\tremaining: 90.6ms\n",
      "216:\tlearn: 0.1738172\ttotal: 234ms\tremaining: 89.3ms\n",
      "217:\tlearn: 0.1731290\ttotal: 234ms\tremaining: 88.2ms\n",
      "218:\tlearn: 0.1724432\ttotal: 235ms\tremaining: 87ms\n",
      "219:\tlearn: 0.1714670\ttotal: 236ms\tremaining: 85.7ms\n",
      "220:\tlearn: 0.1708124\ttotal: 236ms\tremaining: 84.5ms\n",
      "221:\tlearn: 0.1701833\ttotal: 238ms\tremaining: 83.6ms\n",
      "222:\tlearn: 0.1695497\ttotal: 239ms\tremaining: 82.4ms\n",
      "223:\tlearn: 0.1689868\ttotal: 239ms\tremaining: 81.1ms\n",
      "224:\tlearn: 0.1681447\ttotal: 240ms\tremaining: 79.9ms\n",
      "225:\tlearn: 0.1673033\ttotal: 241ms\tremaining: 78.8ms\n",
      "226:\tlearn: 0.1664972\ttotal: 241ms\tremaining: 77.6ms\n",
      "227:\tlearn: 0.1655299\ttotal: 242ms\tremaining: 76.4ms\n",
      "228:\tlearn: 0.1646706\ttotal: 243ms\tremaining: 75.2ms\n",
      "229:\tlearn: 0.1640254\ttotal: 243ms\tremaining: 74.1ms\n",
      "230:\tlearn: 0.1634036\ttotal: 244ms\tremaining: 72.9ms\n",
      "231:\tlearn: 0.1629005\ttotal: 245ms\tremaining: 71.7ms\n",
      "232:\tlearn: 0.1623489\ttotal: 245ms\tremaining: 70.6ms\n",
      "233:\tlearn: 0.1616988\ttotal: 246ms\tremaining: 69.4ms\n",
      "234:\tlearn: 0.1607314\ttotal: 247ms\tremaining: 68.2ms\n",
      "235:\tlearn: 0.1598017\ttotal: 247ms\tremaining: 67ms\n",
      "236:\tlearn: 0.1593978\ttotal: 248ms\tremaining: 65.9ms\n",
      "237:\tlearn: 0.1584885\ttotal: 249ms\tremaining: 64.7ms\n",
      "238:\tlearn: 0.1576806\ttotal: 249ms\tremaining: 63.6ms\n",
      "239:\tlearn: 0.1576677\ttotal: 249ms\tremaining: 62.3ms\n",
      "240:\tlearn: 0.1568757\ttotal: 250ms\tremaining: 61.2ms\n",
      "241:\tlearn: 0.1562333\ttotal: 251ms\tremaining: 60.1ms\n",
      "242:\tlearn: 0.1557990\ttotal: 252ms\tremaining: 59ms\n",
      "243:\tlearn: 0.1551743\ttotal: 252ms\tremaining: 57.9ms\n",
      "244:\tlearn: 0.1547226\ttotal: 254ms\tremaining: 57ms\n",
      "245:\tlearn: 0.1535189\ttotal: 255ms\tremaining: 55.9ms\n",
      "246:\tlearn: 0.1528574\ttotal: 255ms\tremaining: 54.8ms\n",
      "247:\tlearn: 0.1519661\ttotal: 256ms\tremaining: 53.7ms\n",
      "248:\tlearn: 0.1512729\ttotal: 257ms\tremaining: 52.6ms\n",
      "249:\tlearn: 0.1505194\ttotal: 257ms\tremaining: 51.5ms\n",
      "250:\tlearn: 0.1497824\ttotal: 258ms\tremaining: 50.4ms\n",
      "251:\tlearn: 0.1494411\ttotal: 259ms\tremaining: 49.3ms\n",
      "252:\tlearn: 0.1488225\ttotal: 259ms\tremaining: 48.2ms\n",
      "253:\tlearn: 0.1482942\ttotal: 260ms\tremaining: 47.1ms\n",
      "254:\tlearn: 0.1477273\ttotal: 261ms\tremaining: 46ms\n",
      "255:\tlearn: 0.1475167\ttotal: 262ms\tremaining: 45ms\n",
      "256:\tlearn: 0.1467434\ttotal: 262ms\tremaining: 43.9ms\n",
      "257:\tlearn: 0.1458800\ttotal: 263ms\tremaining: 42.8ms\n",
      "258:\tlearn: 0.1453954\ttotal: 264ms\tremaining: 41.8ms\n",
      "259:\tlearn: 0.1446846\ttotal: 264ms\tremaining: 40.7ms\n",
      "260:\tlearn: 0.1440251\ttotal: 265ms\tremaining: 39.6ms\n",
      "261:\tlearn: 0.1434507\ttotal: 266ms\tremaining: 38.5ms\n",
      "262:\tlearn: 0.1430232\ttotal: 266ms\tremaining: 37.5ms\n",
      "263:\tlearn: 0.1423145\ttotal: 268ms\tremaining: 36.5ms\n",
      "264:\tlearn: 0.1419163\ttotal: 269ms\tremaining: 35.5ms\n",
      "265:\tlearn: 0.1414291\ttotal: 269ms\tremaining: 34.4ms\n",
      "266:\tlearn: 0.1409492\ttotal: 270ms\tremaining: 33.4ms\n",
      "267:\tlearn: 0.1402795\ttotal: 271ms\tremaining: 32.3ms\n",
      "268:\tlearn: 0.1396154\ttotal: 271ms\tremaining: 31.3ms\n",
      "269:\tlearn: 0.1389521\ttotal: 272ms\tremaining: 30.3ms\n",
      "270:\tlearn: 0.1385790\ttotal: 273ms\tremaining: 29.2ms\n",
      "271:\tlearn: 0.1380386\ttotal: 274ms\tremaining: 28.2ms\n",
      "272:\tlearn: 0.1375065\ttotal: 275ms\tremaining: 27.2ms\n",
      "273:\tlearn: 0.1368125\ttotal: 275ms\tremaining: 26.1ms\n",
      "274:\tlearn: 0.1365292\ttotal: 276ms\tremaining: 25.1ms\n",
      "275:\tlearn: 0.1355452\ttotal: 277ms\tremaining: 24ms\n",
      "276:\tlearn: 0.1350120\ttotal: 277ms\tremaining: 23ms\n",
      "277:\tlearn: 0.1345714\ttotal: 278ms\tremaining: 22ms\n",
      "278:\tlearn: 0.1342170\ttotal: 279ms\tremaining: 21ms\n",
      "279:\tlearn: 0.1337817\ttotal: 280ms\tremaining: 20ms\n",
      "280:\tlearn: 0.1335042\ttotal: 280ms\tremaining: 18.9ms\n",
      "281:\tlearn: 0.1328401\ttotal: 281ms\tremaining: 17.9ms\n",
      "282:\tlearn: 0.1325145\ttotal: 282ms\tremaining: 16.9ms\n",
      "283:\tlearn: 0.1320028\ttotal: 282ms\tremaining: 15.9ms\n",
      "284:\tlearn: 0.1314895\ttotal: 283ms\tremaining: 14.9ms\n",
      "285:\tlearn: 0.1310461\ttotal: 284ms\tremaining: 13.9ms\n",
      "286:\tlearn: 0.1306863\ttotal: 284ms\tremaining: 12.9ms\n",
      "287:\tlearn: 0.1302369\ttotal: 285ms\tremaining: 11.9ms\n",
      "288:\tlearn: 0.1296086\ttotal: 286ms\tremaining: 10.9ms\n",
      "289:\tlearn: 0.1292298\ttotal: 286ms\tremaining: 9.87ms\n",
      "290:\tlearn: 0.1284829\ttotal: 287ms\tremaining: 8.88ms\n",
      "291:\tlearn: 0.1281592\ttotal: 288ms\tremaining: 7.89ms\n",
      "292:\tlearn: 0.1278790\ttotal: 289ms\tremaining: 6.89ms\n",
      "293:\tlearn: 0.1272380\ttotal: 289ms\tremaining: 5.9ms\n",
      "294:\tlearn: 0.1267145\ttotal: 290ms\tremaining: 4.92ms\n",
      "295:\tlearn: 0.1263391\ttotal: 292ms\tremaining: 3.94ms\n",
      "296:\tlearn: 0.1258585\ttotal: 292ms\tremaining: 2.95ms\n",
      "297:\tlearn: 0.1254201\ttotal: 293ms\tremaining: 1.97ms\n",
      "298:\tlearn: 0.1247872\ttotal: 294ms\tremaining: 984us\n",
      "299:\tlearn: 0.1242381\ttotal: 295ms\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "NFL.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy        f1   roc_auc\n",
       "0  0.814815  0.545455  0.885714"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NFL.model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mfldata.load_qb_data_cleaned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics_only = df.select_dtypes(include='float').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>Cmp</th>\n",
       "      <th>Att</th>\n",
       "      <th>Cmp%</th>\n",
       "      <th>Yds</th>\n",
       "      <th>TD</th>\n",
       "      <th>TD%</th>\n",
       "      <th>Int</th>\n",
       "      <th>Int%</th>\n",
       "      <th>Y/A</th>\n",
       "      <th>AY/A</th>\n",
       "      <th>Y/C</th>\n",
       "      <th>Y/G</th>\n",
       "      <th>Rate</th>\n",
       "      <th>seasons</th>\n",
       "      <th>seasons_with_draft_team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>67.6</td>\n",
       "      <td>8403.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>10.57</td>\n",
       "      <td>13.9</td>\n",
       "      <td>271.1</td>\n",
       "      <td>175.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>995.0</td>\n",
       "      <td>66.4</td>\n",
       "      <td>9285.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>9.3</td>\n",
       "      <td>10.38</td>\n",
       "      <td>14.0</td>\n",
       "      <td>168.8</td>\n",
       "      <td>170.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.0</td>\n",
       "      <td>695.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>62.6</td>\n",
       "      <td>8148.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.33</td>\n",
       "      <td>11.7</td>\n",
       "      <td>232.8</td>\n",
       "      <td>137.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>1645.0</td>\n",
       "      <td>70.3</td>\n",
       "      <td>13253.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>8.19</td>\n",
       "      <td>11.5</td>\n",
       "      <td>250.1</td>\n",
       "      <td>155.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>64.1</td>\n",
       "      <td>4265.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.88</td>\n",
       "      <td>10.5</td>\n",
       "      <td>142.2</td>\n",
       "      <td>123.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "0  31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "1  55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "2  35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "3  53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "4  30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "\n",
       "[5 rows x 16 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerics_only.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\n",
    "                                                                       axis=1),\n",
    "                                                                       df['seasons_with_draft_team'].apply(map_response),\n",
    "                                                                       test_size=.25,\n",
    "                                                                       shuffle=True,\n",
    "                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\n",
    "                                                                       axis=1),\n",
    "                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\n",
    "                                                                       test_size=.25,\n",
    "                                                                       shuffle=True,\n",
    "                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.010625\n",
      "0:\tlearn: 0.6860037\ttotal: 2.22ms\tremaining: 664ms\n",
      "1:\tlearn: 0.6777927\ttotal: 4.22ms\tremaining: 629ms\n",
      "2:\tlearn: 0.6695467\ttotal: 5.27ms\tremaining: 522ms\n",
      "3:\tlearn: 0.6632005\ttotal: 7.8ms\tremaining: 578ms\n",
      "4:\tlearn: 0.6562832\ttotal: 9.6ms\tremaining: 567ms\n",
      "5:\tlearn: 0.6509253\ttotal: 10.7ms\tremaining: 524ms\n",
      "6:\tlearn: 0.6448863\ttotal: 11.7ms\tremaining: 488ms\n",
      "7:\tlearn: 0.6415797\ttotal: 12.2ms\tremaining: 446ms\n",
      "8:\tlearn: 0.6364113\ttotal: 13.4ms\tremaining: 433ms\n",
      "9:\tlearn: 0.6290033\ttotal: 14.4ms\tremaining: 419ms\n",
      "10:\tlearn: 0.6220800\ttotal: 15.3ms\tremaining: 403ms\n",
      "11:\tlearn: 0.6148267\ttotal: 16.5ms\tremaining: 397ms\n",
      "12:\tlearn: 0.6101433\ttotal: 18.4ms\tremaining: 406ms\n",
      "13:\tlearn: 0.6037479\ttotal: 19.6ms\tremaining: 400ms\n",
      "14:\tlearn: 0.5977267\ttotal: 20.6ms\tremaining: 391ms\n",
      "15:\tlearn: 0.5923923\ttotal: 21.5ms\tremaining: 381ms\n",
      "16:\tlearn: 0.5867612\ttotal: 22.4ms\tremaining: 373ms\n",
      "17:\tlearn: 0.5827596\ttotal: 23.7ms\tremaining: 372ms\n",
      "18:\tlearn: 0.5771357\ttotal: 24.6ms\tremaining: 364ms\n",
      "19:\tlearn: 0.5694309\ttotal: 25.4ms\tremaining: 356ms\n",
      "20:\tlearn: 0.5658455\ttotal: 25.8ms\tremaining: 343ms\n",
      "21:\tlearn: 0.5610902\ttotal: 26.8ms\tremaining: 338ms\n",
      "22:\tlearn: 0.5597035\ttotal: 27.1ms\tremaining: 326ms\n",
      "23:\tlearn: 0.5533159\ttotal: 28.6ms\tremaining: 329ms\n",
      "24:\tlearn: 0.5480409\ttotal: 29.3ms\tremaining: 323ms\n",
      "25:\tlearn: 0.5427419\ttotal: 30.1ms\tremaining: 317ms\n",
      "26:\tlearn: 0.5362003\ttotal: 30.9ms\tremaining: 312ms\n",
      "27:\tlearn: 0.5318121\ttotal: 31.5ms\tremaining: 306ms\n",
      "28:\tlearn: 0.5277533\ttotal: 32.1ms\tremaining: 300ms\n",
      "29:\tlearn: 0.5216916\ttotal: 32.8ms\tremaining: 295ms\n",
      "30:\tlearn: 0.5164736\ttotal: 33.5ms\tremaining: 290ms\n",
      "31:\tlearn: 0.5120956\ttotal: 34.2ms\tremaining: 287ms\n",
      "32:\tlearn: 0.5082404\ttotal: 34.9ms\tremaining: 283ms\n",
      "33:\tlearn: 0.5029867\ttotal: 35.5ms\tremaining: 278ms\n",
      "34:\tlearn: 0.4965903\ttotal: 36.3ms\tremaining: 275ms\n",
      "35:\tlearn: 0.4911687\ttotal: 37.1ms\tremaining: 272ms\n",
      "36:\tlearn: 0.4863159\ttotal: 38.3ms\tremaining: 272ms\n",
      "37:\tlearn: 0.4831417\ttotal: 39.8ms\tremaining: 274ms\n",
      "38:\tlearn: 0.4787644\ttotal: 40.6ms\tremaining: 272ms\n",
      "39:\tlearn: 0.4760859\ttotal: 41.3ms\tremaining: 268ms\n",
      "40:\tlearn: 0.4746385\ttotal: 41.7ms\tremaining: 263ms\n",
      "41:\tlearn: 0.4687500\ttotal: 42.3ms\tremaining: 260ms\n",
      "42:\tlearn: 0.4660409\ttotal: 43ms\tremaining: 257ms\n",
      "43:\tlearn: 0.4625528\ttotal: 43.8ms\tremaining: 255ms\n",
      "44:\tlearn: 0.4585145\ttotal: 44.5ms\tremaining: 252ms\n",
      "45:\tlearn: 0.4542572\ttotal: 45.3ms\tremaining: 250ms\n",
      "46:\tlearn: 0.4517845\ttotal: 45.9ms\tremaining: 247ms\n",
      "47:\tlearn: 0.4485624\ttotal: 46.5ms\tremaining: 244ms\n",
      "48:\tlearn: 0.4443866\ttotal: 47.1ms\tremaining: 241ms\n",
      "49:\tlearn: 0.4401483\ttotal: 48.4ms\tremaining: 242ms\n",
      "50:\tlearn: 0.4367274\ttotal: 49.1ms\tremaining: 240ms\n",
      "51:\tlearn: 0.4329021\ttotal: 49.8ms\tremaining: 237ms\n",
      "52:\tlearn: 0.4294201\ttotal: 50.4ms\tremaining: 235ms\n",
      "53:\tlearn: 0.4258224\ttotal: 51.2ms\tremaining: 233ms\n",
      "54:\tlearn: 0.4231774\ttotal: 51.8ms\tremaining: 231ms\n",
      "55:\tlearn: 0.4193949\ttotal: 52.5ms\tremaining: 229ms\n",
      "56:\tlearn: 0.4157227\ttotal: 53.1ms\tremaining: 226ms\n",
      "57:\tlearn: 0.4127078\ttotal: 53.9ms\tremaining: 225ms\n",
      "58:\tlearn: 0.4100389\ttotal: 55ms\tremaining: 225ms\n",
      "59:\tlearn: 0.4069064\ttotal: 56.5ms\tremaining: 226ms\n",
      "60:\tlearn: 0.4041301\ttotal: 57.3ms\tremaining: 225ms\n",
      "61:\tlearn: 0.4004144\ttotal: 58.1ms\tremaining: 223ms\n",
      "62:\tlearn: 0.3965323\ttotal: 58.9ms\tremaining: 222ms\n",
      "63:\tlearn: 0.3943053\ttotal: 59.3ms\tremaining: 219ms\n",
      "64:\tlearn: 0.3912832\ttotal: 60ms\tremaining: 217ms\n",
      "65:\tlearn: 0.3880797\ttotal: 60.7ms\tremaining: 215ms\n",
      "66:\tlearn: 0.3851302\ttotal: 61.4ms\tremaining: 214ms\n",
      "67:\tlearn: 0.3834912\ttotal: 62.2ms\tremaining: 212ms\n",
      "68:\tlearn: 0.3806593\ttotal: 62.8ms\tremaining: 210ms\n",
      "69:\tlearn: 0.3778226\ttotal: 63.5ms\tremaining: 209ms\n",
      "70:\tlearn: 0.3747209\ttotal: 64.4ms\tremaining: 208ms\n",
      "71:\tlearn: 0.3715683\ttotal: 65.1ms\tremaining: 206ms\n",
      "72:\tlearn: 0.3684941\ttotal: 65.6ms\tremaining: 204ms\n",
      "73:\tlearn: 0.3657287\ttotal: 66.2ms\tremaining: 202ms\n",
      "74:\tlearn: 0.3644687\ttotal: 66.9ms\tremaining: 201ms\n",
      "75:\tlearn: 0.3616780\ttotal: 67.6ms\tremaining: 199ms\n",
      "76:\tlearn: 0.3588888\ttotal: 68.2ms\tremaining: 198ms\n",
      "77:\tlearn: 0.3560348\ttotal: 68.9ms\tremaining: 196ms\n",
      "78:\tlearn: 0.3534702\ttotal: 69.6ms\tremaining: 195ms\n",
      "79:\tlearn: 0.3512633\ttotal: 70.5ms\tremaining: 194ms\n",
      "80:\tlearn: 0.3483155\ttotal: 71.3ms\tremaining: 193ms\n",
      "81:\tlearn: 0.3451383\ttotal: 72.2ms\tremaining: 192ms\n",
      "82:\tlearn: 0.3430613\ttotal: 73.5ms\tremaining: 192ms\n",
      "83:\tlearn: 0.3405574\ttotal: 74.3ms\tremaining: 191ms\n",
      "84:\tlearn: 0.3387700\ttotal: 75.2ms\tremaining: 190ms\n",
      "85:\tlearn: 0.3359461\ttotal: 75.9ms\tremaining: 189ms\n",
      "86:\tlearn: 0.3332326\ttotal: 76.7ms\tremaining: 188ms\n",
      "87:\tlearn: 0.3312779\ttotal: 77.5ms\tremaining: 187ms\n",
      "88:\tlearn: 0.3289833\ttotal: 78.2ms\tremaining: 185ms\n",
      "89:\tlearn: 0.3276879\ttotal: 78.9ms\tremaining: 184ms\n",
      "90:\tlearn: 0.3245574\ttotal: 79.5ms\tremaining: 183ms\n",
      "91:\tlearn: 0.3227922\ttotal: 80.1ms\tremaining: 181ms\n",
      "92:\tlearn: 0.3207355\ttotal: 80.8ms\tremaining: 180ms\n",
      "93:\tlearn: 0.3185965\ttotal: 81.5ms\tremaining: 179ms\n",
      "94:\tlearn: 0.3168189\ttotal: 82.2ms\tremaining: 177ms\n",
      "95:\tlearn: 0.3153263\ttotal: 82.9ms\tremaining: 176ms\n",
      "96:\tlearn: 0.3130134\ttotal: 84.4ms\tremaining: 177ms\n",
      "97:\tlearn: 0.3118534\ttotal: 85.1ms\tremaining: 175ms\n",
      "98:\tlearn: 0.3102042\ttotal: 85.8ms\tremaining: 174ms\n",
      "99:\tlearn: 0.3086383\ttotal: 86.7ms\tremaining: 173ms\n",
      "100:\tlearn: 0.3059442\ttotal: 87.7ms\tremaining: 173ms\n",
      "101:\tlearn: 0.3044225\ttotal: 88.6ms\tremaining: 172ms\n",
      "102:\tlearn: 0.3027919\ttotal: 89.5ms\tremaining: 171ms\n",
      "103:\tlearn: 0.3007658\ttotal: 90.2ms\tremaining: 170ms\n",
      "104:\tlearn: 0.2980790\ttotal: 90.9ms\tremaining: 169ms\n",
      "105:\tlearn: 0.2959221\ttotal: 91.7ms\tremaining: 168ms\n",
      "106:\tlearn: 0.2942563\ttotal: 92.4ms\tremaining: 167ms\n",
      "107:\tlearn: 0.2935584\ttotal: 92.9ms\tremaining: 165ms\n",
      "108:\tlearn: 0.2920227\ttotal: 93.7ms\tremaining: 164ms\n",
      "109:\tlearn: 0.2902555\ttotal: 94.4ms\tremaining: 163ms\n",
      "110:\tlearn: 0.2900891\ttotal: 94.6ms\tremaining: 161ms\n",
      "111:\tlearn: 0.2883565\ttotal: 95.2ms\tremaining: 160ms\n",
      "112:\tlearn: 0.2864017\ttotal: 96ms\tremaining: 159ms\n",
      "113:\tlearn: 0.2842692\ttotal: 96.8ms\tremaining: 158ms\n",
      "114:\tlearn: 0.2830815\ttotal: 97.3ms\tremaining: 157ms\n",
      "115:\tlearn: 0.2817191\ttotal: 97.9ms\tremaining: 155ms\n",
      "116:\tlearn: 0.2795842\ttotal: 98.6ms\tremaining: 154ms\n",
      "117:\tlearn: 0.2773834\ttotal: 99.3ms\tremaining: 153ms\n",
      "118:\tlearn: 0.2754737\ttotal: 101ms\tremaining: 153ms\n",
      "119:\tlearn: 0.2742263\ttotal: 101ms\tremaining: 152ms\n",
      "120:\tlearn: 0.2727494\ttotal: 102ms\tremaining: 151ms\n",
      "121:\tlearn: 0.2714519\ttotal: 103ms\tremaining: 150ms\n",
      "122:\tlearn: 0.2703914\ttotal: 104ms\tremaining: 150ms\n",
      "123:\tlearn: 0.2683544\ttotal: 105ms\tremaining: 149ms\n",
      "124:\tlearn: 0.2670571\ttotal: 105ms\tremaining: 148ms\n",
      "125:\tlearn: 0.2656571\ttotal: 106ms\tremaining: 147ms\n",
      "126:\tlearn: 0.2637953\ttotal: 107ms\tremaining: 146ms\n",
      "127:\tlearn: 0.2623808\ttotal: 108ms\tremaining: 145ms\n",
      "128:\tlearn: 0.2605900\ttotal: 108ms\tremaining: 143ms\n",
      "129:\tlearn: 0.2589565\ttotal: 109ms\tremaining: 142ms\n",
      "130:\tlearn: 0.2577577\ttotal: 110ms\tremaining: 141ms\n",
      "131:\tlearn: 0.2565566\ttotal: 111ms\tremaining: 141ms\n",
      "132:\tlearn: 0.2551464\ttotal: 112ms\tremaining: 140ms\n",
      "133:\tlearn: 0.2531911\ttotal: 112ms\tremaining: 139ms\n",
      "134:\tlearn: 0.2513381\ttotal: 113ms\tremaining: 138ms\n",
      "135:\tlearn: 0.2502180\ttotal: 114ms\tremaining: 137ms\n",
      "136:\tlearn: 0.2484547\ttotal: 115ms\tremaining: 136ms\n",
      "137:\tlearn: 0.2478077\ttotal: 115ms\tremaining: 136ms\n",
      "138:\tlearn: 0.2467110\ttotal: 116ms\tremaining: 135ms\n",
      "139:\tlearn: 0.2457368\ttotal: 117ms\tremaining: 134ms\n",
      "140:\tlearn: 0.2455714\ttotal: 117ms\tremaining: 132ms\n",
      "141:\tlearn: 0.2442697\ttotal: 118ms\tremaining: 131ms\n",
      "142:\tlearn: 0.2430402\ttotal: 119ms\tremaining: 130ms\n",
      "143:\tlearn: 0.2412092\ttotal: 120ms\tremaining: 130ms\n",
      "144:\tlearn: 0.2398108\ttotal: 120ms\tremaining: 129ms\n",
      "145:\tlearn: 0.2382954\ttotal: 123ms\tremaining: 130ms\n",
      "146:\tlearn: 0.2369953\ttotal: 124ms\tremaining: 129ms\n",
      "147:\tlearn: 0.2362595\ttotal: 125ms\tremaining: 128ms\n",
      "148:\tlearn: 0.2351744\ttotal: 126ms\tremaining: 127ms\n",
      "149:\tlearn: 0.2339612\ttotal: 126ms\tremaining: 126ms\n",
      "150:\tlearn: 0.2338796\ttotal: 127ms\tremaining: 125ms\n",
      "151:\tlearn: 0.2325673\ttotal: 127ms\tremaining: 124ms\n",
      "152:\tlearn: 0.2310790\ttotal: 128ms\tremaining: 123ms\n",
      "153:\tlearn: 0.2302860\ttotal: 129ms\tremaining: 122ms\n",
      "154:\tlearn: 0.2284060\ttotal: 129ms\tremaining: 121ms\n",
      "155:\tlearn: 0.2272278\ttotal: 130ms\tremaining: 120ms\n",
      "156:\tlearn: 0.2258567\ttotal: 131ms\tremaining: 119ms\n",
      "157:\tlearn: 0.2250139\ttotal: 131ms\tremaining: 118ms\n",
      "158:\tlearn: 0.2240219\ttotal: 132ms\tremaining: 117ms\n",
      "159:\tlearn: 0.2225017\ttotal: 133ms\tremaining: 116ms\n",
      "160:\tlearn: 0.2213832\ttotal: 133ms\tremaining: 115ms\n",
      "161:\tlearn: 0.2203321\ttotal: 134ms\tremaining: 114ms\n",
      "162:\tlearn: 0.2186189\ttotal: 135ms\tremaining: 113ms\n",
      "163:\tlearn: 0.2178334\ttotal: 135ms\tremaining: 112ms\n",
      "164:\tlearn: 0.2167397\ttotal: 136ms\tremaining: 111ms\n",
      "165:\tlearn: 0.2155833\ttotal: 137ms\tremaining: 110ms\n",
      "166:\tlearn: 0.2147377\ttotal: 138ms\tremaining: 110ms\n",
      "167:\tlearn: 0.2138558\ttotal: 139ms\tremaining: 109ms\n",
      "168:\tlearn: 0.2121823\ttotal: 139ms\tremaining: 108ms\n",
      "169:\tlearn: 0.2114470\ttotal: 140ms\tremaining: 107ms\n",
      "170:\tlearn: 0.2103092\ttotal: 140ms\tremaining: 106ms\n",
      "171:\tlearn: 0.2102554\ttotal: 141ms\tremaining: 105ms\n",
      "172:\tlearn: 0.2095748\ttotal: 141ms\tremaining: 104ms\n",
      "173:\tlearn: 0.2084638\ttotal: 142ms\tremaining: 103ms\n",
      "174:\tlearn: 0.2070404\ttotal: 143ms\tremaining: 102ms\n",
      "175:\tlearn: 0.2057900\ttotal: 143ms\tremaining: 101ms\n",
      "176:\tlearn: 0.2057418\ttotal: 143ms\tremaining: 99.6ms\n",
      "177:\tlearn: 0.2045917\ttotal: 144ms\tremaining: 98.7ms\n",
      "178:\tlearn: 0.2031833\ttotal: 145ms\tremaining: 98ms\n",
      "179:\tlearn: 0.2025743\ttotal: 145ms\tremaining: 96.8ms\n",
      "180:\tlearn: 0.2017798\ttotal: 146ms\tremaining: 95.9ms\n",
      "181:\tlearn: 0.2002762\ttotal: 147ms\tremaining: 95.1ms\n",
      "182:\tlearn: 0.1991014\ttotal: 147ms\tremaining: 94.3ms\n",
      "183:\tlearn: 0.1981233\ttotal: 148ms\tremaining: 93.4ms\n",
      "184:\tlearn: 0.1969890\ttotal: 149ms\tremaining: 92.6ms\n",
      "185:\tlearn: 0.1956314\ttotal: 150ms\tremaining: 91.7ms\n",
      "186:\tlearn: 0.1944889\ttotal: 151ms\tremaining: 91.4ms\n",
      "187:\tlearn: 0.1936778\ttotal: 152ms\tremaining: 90.6ms\n",
      "188:\tlearn: 0.1925399\ttotal: 153ms\tremaining: 89.6ms\n",
      "189:\tlearn: 0.1918157\ttotal: 153ms\tremaining: 88.8ms\n",
      "190:\tlearn: 0.1902695\ttotal: 154ms\tremaining: 87.9ms\n",
      "191:\tlearn: 0.1890875\ttotal: 155ms\tremaining: 87ms\n",
      "192:\tlearn: 0.1881684\ttotal: 155ms\tremaining: 86.2ms\n",
      "193:\tlearn: 0.1875198\ttotal: 156ms\tremaining: 85.3ms\n",
      "194:\tlearn: 0.1862718\ttotal: 157ms\tremaining: 84.5ms\n",
      "195:\tlearn: 0.1855233\ttotal: 158ms\tremaining: 83.6ms\n",
      "196:\tlearn: 0.1843243\ttotal: 159ms\tremaining: 83.2ms\n",
      "197:\tlearn: 0.1835263\ttotal: 160ms\tremaining: 82.3ms\n",
      "198:\tlearn: 0.1825861\ttotal: 161ms\tremaining: 81.5ms\n",
      "199:\tlearn: 0.1820098\ttotal: 161ms\tremaining: 80.6ms\n",
      "200:\tlearn: 0.1815209\ttotal: 162ms\tremaining: 79.8ms\n",
      "201:\tlearn: 0.1804775\ttotal: 163ms\tremaining: 78.9ms\n",
      "202:\tlearn: 0.1796041\ttotal: 163ms\tremaining: 78.1ms\n",
      "203:\tlearn: 0.1785366\ttotal: 164ms\tremaining: 77.2ms\n",
      "204:\tlearn: 0.1775796\ttotal: 165ms\tremaining: 76.3ms\n",
      "205:\tlearn: 0.1768129\ttotal: 165ms\tremaining: 75.5ms\n",
      "206:\tlearn: 0.1760751\ttotal: 166ms\tremaining: 74.6ms\n",
      "207:\tlearn: 0.1760439\ttotal: 166ms\tremaining: 73.5ms\n",
      "208:\tlearn: 0.1749725\ttotal: 167ms\tremaining: 72.7ms\n",
      "209:\tlearn: 0.1740533\ttotal: 168ms\tremaining: 71.9ms\n",
      "210:\tlearn: 0.1731028\ttotal: 169ms\tremaining: 71.4ms\n",
      "211:\tlearn: 0.1725858\ttotal: 170ms\tremaining: 70.5ms\n",
      "212:\tlearn: 0.1716198\ttotal: 171ms\tremaining: 69.7ms\n",
      "213:\tlearn: 0.1715622\ttotal: 171ms\tremaining: 68.7ms\n",
      "214:\tlearn: 0.1705225\ttotal: 172ms\tremaining: 67.9ms\n",
      "215:\tlearn: 0.1694816\ttotal: 172ms\tremaining: 67.1ms\n",
      "216:\tlearn: 0.1690974\ttotal: 173ms\tremaining: 66.1ms\n",
      "217:\tlearn: 0.1679222\ttotal: 173ms\tremaining: 65.3ms\n",
      "218:\tlearn: 0.1673266\ttotal: 174ms\tremaining: 64.4ms\n",
      "219:\tlearn: 0.1667782\ttotal: 175ms\tremaining: 63.5ms\n",
      "220:\tlearn: 0.1660783\ttotal: 175ms\tremaining: 62.7ms\n",
      "221:\tlearn: 0.1652618\ttotal: 176ms\tremaining: 61.8ms\n",
      "222:\tlearn: 0.1644115\ttotal: 177ms\tremaining: 61.2ms\n",
      "223:\tlearn: 0.1639197\ttotal: 179ms\tremaining: 60.6ms\n",
      "224:\tlearn: 0.1632194\ttotal: 179ms\tremaining: 59.8ms\n",
      "225:\tlearn: 0.1625594\ttotal: 180ms\tremaining: 59ms\n",
      "226:\tlearn: 0.1625347\ttotal: 180ms\tremaining: 58ms\n",
      "227:\tlearn: 0.1618872\ttotal: 181ms\tremaining: 57.2ms\n",
      "228:\tlearn: 0.1612063\ttotal: 182ms\tremaining: 56.4ms\n",
      "229:\tlearn: 0.1610155\ttotal: 182ms\tremaining: 55.5ms\n",
      "230:\tlearn: 0.1607183\ttotal: 183ms\tremaining: 54.6ms\n",
      "231:\tlearn: 0.1605100\ttotal: 183ms\tremaining: 53.6ms\n",
      "232:\tlearn: 0.1592134\ttotal: 184ms\tremaining: 52.8ms\n",
      "233:\tlearn: 0.1582837\ttotal: 185ms\tremaining: 52.1ms\n",
      "234:\tlearn: 0.1575475\ttotal: 185ms\tremaining: 51.3ms\n",
      "235:\tlearn: 0.1569659\ttotal: 186ms\tremaining: 50.5ms\n",
      "236:\tlearn: 0.1566963\ttotal: 187ms\tremaining: 49.7ms\n",
      "237:\tlearn: 0.1555236\ttotal: 188ms\tremaining: 49ms\n",
      "238:\tlearn: 0.1552997\ttotal: 189ms\tremaining: 48.1ms\n",
      "239:\tlearn: 0.1545407\ttotal: 190ms\tremaining: 47.5ms\n",
      "240:\tlearn: 0.1540820\ttotal: 191ms\tremaining: 46.6ms\n",
      "241:\tlearn: 0.1534282\ttotal: 191ms\tremaining: 45.9ms\n",
      "242:\tlearn: 0.1527701\ttotal: 192ms\tremaining: 45.1ms\n",
      "243:\tlearn: 0.1518453\ttotal: 194ms\tremaining: 44.5ms\n",
      "244:\tlearn: 0.1513219\ttotal: 194ms\tremaining: 43.7ms\n",
      "245:\tlearn: 0.1511386\ttotal: 195ms\tremaining: 42.7ms\n",
      "246:\tlearn: 0.1503077\ttotal: 196ms\tremaining: 42ms\n",
      "247:\tlearn: 0.1496699\ttotal: 196ms\tremaining: 41.2ms\n",
      "248:\tlearn: 0.1487662\ttotal: 197ms\tremaining: 40.4ms\n",
      "249:\tlearn: 0.1480262\ttotal: 198ms\tremaining: 39.6ms\n",
      "250:\tlearn: 0.1477591\ttotal: 198ms\tremaining: 38.7ms\n",
      "251:\tlearn: 0.1470499\ttotal: 199ms\tremaining: 37.9ms\n",
      "252:\tlearn: 0.1461304\ttotal: 200ms\tremaining: 37.1ms\n",
      "253:\tlearn: 0.1455850\ttotal: 200ms\tremaining: 36.3ms\n",
      "254:\tlearn: 0.1454419\ttotal: 201ms\tremaining: 35.4ms\n",
      "255:\tlearn: 0.1449629\ttotal: 202ms\tremaining: 34.6ms\n",
      "256:\tlearn: 0.1441504\ttotal: 202ms\tremaining: 33.9ms\n",
      "257:\tlearn: 0.1437663\ttotal: 203ms\tremaining: 33ms\n",
      "258:\tlearn: 0.1431426\ttotal: 204ms\tremaining: 32.2ms\n",
      "259:\tlearn: 0.1428482\ttotal: 204ms\tremaining: 31.4ms\n",
      "260:\tlearn: 0.1426916\ttotal: 204ms\tremaining: 30.5ms\n",
      "261:\tlearn: 0.1418239\ttotal: 205ms\tremaining: 29.7ms\n",
      "262:\tlearn: 0.1412539\ttotal: 206ms\tremaining: 28.9ms\n",
      "263:\tlearn: 0.1408203\ttotal: 207ms\tremaining: 28.2ms\n",
      "264:\tlearn: 0.1400394\ttotal: 208ms\tremaining: 27.5ms\n",
      "265:\tlearn: 0.1395601\ttotal: 209ms\tremaining: 26.7ms\n",
      "266:\tlearn: 0.1390306\ttotal: 210ms\tremaining: 25.9ms\n",
      "267:\tlearn: 0.1387121\ttotal: 210ms\tremaining: 25.1ms\n",
      "268:\tlearn: 0.1381401\ttotal: 211ms\tremaining: 24.3ms\n",
      "269:\tlearn: 0.1374756\ttotal: 211ms\tremaining: 23.5ms\n",
      "270:\tlearn: 0.1371511\ttotal: 213ms\tremaining: 22.8ms\n",
      "271:\tlearn: 0.1367263\ttotal: 213ms\tremaining: 22ms\n",
      "272:\tlearn: 0.1360371\ttotal: 214ms\tremaining: 21.2ms\n",
      "273:\tlearn: 0.1353967\ttotal: 215ms\tremaining: 20.4ms\n",
      "274:\tlearn: 0.1345073\ttotal: 216ms\tremaining: 19.6ms\n",
      "275:\tlearn: 0.1339782\ttotal: 217ms\tremaining: 18.9ms\n",
      "276:\tlearn: 0.1335254\ttotal: 218ms\tremaining: 18.1ms\n",
      "277:\tlearn: 0.1330796\ttotal: 219ms\tremaining: 17.3ms\n",
      "278:\tlearn: 0.1326663\ttotal: 220ms\tremaining: 16.5ms\n",
      "279:\tlearn: 0.1324111\ttotal: 221ms\tremaining: 15.8ms\n",
      "280:\tlearn: 0.1318211\ttotal: 221ms\tremaining: 15ms\n",
      "281:\tlearn: 0.1310334\ttotal: 222ms\tremaining: 14.2ms\n",
      "282:\tlearn: 0.1305866\ttotal: 223ms\tremaining: 13.4ms\n",
      "283:\tlearn: 0.1302007\ttotal: 223ms\tremaining: 12.6ms\n",
      "284:\tlearn: 0.1297652\ttotal: 224ms\tremaining: 11.8ms\n",
      "285:\tlearn: 0.1290486\ttotal: 224ms\tremaining: 11ms\n",
      "286:\tlearn: 0.1290219\ttotal: 224ms\tremaining: 10.2ms\n",
      "287:\tlearn: 0.1282893\ttotal: 225ms\tremaining: 9.38ms\n",
      "288:\tlearn: 0.1277381\ttotal: 226ms\tremaining: 8.6ms\n",
      "289:\tlearn: 0.1273478\ttotal: 227ms\tremaining: 7.81ms\n",
      "290:\tlearn: 0.1271250\ttotal: 227ms\tremaining: 7.02ms\n",
      "291:\tlearn: 0.1265971\ttotal: 228ms\tremaining: 6.24ms\n",
      "292:\tlearn: 0.1264147\ttotal: 228ms\tremaining: 5.45ms\n",
      "293:\tlearn: 0.1259385\ttotal: 229ms\tremaining: 4.67ms\n",
      "294:\tlearn: 0.1256171\ttotal: 229ms\tremaining: 3.88ms\n",
      "295:\tlearn: 0.1252767\ttotal: 230ms\tremaining: 3.11ms\n",
      "296:\tlearn: 0.1249303\ttotal: 231ms\tremaining: 2.33ms\n",
      "297:\tlearn: 0.1246539\ttotal: 231ms\tremaining: 1.55ms\n",
      "298:\tlearn: 0.1239238\ttotal: 232ms\tremaining: 776us\n",
      "299:\tlearn: 0.1232792\ttotal: 233ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7928571428571428"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\n",
    "model.fit(X_train, y_train)\n",
    "y_preds = model.predict(X_test)\n",
    "y_probs = model.predict_proba(X_test)[:,1]\n",
    "accuracy_score(y_test, y_preds)\n",
    "roc_auc_score(y_test, y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pfr_player_name</th>\n",
       "      <th>round</th>\n",
       "      <th>pick</th>\n",
       "      <th>season</th>\n",
       "      <th>G</th>\n",
       "      <th>Cmp</th>\n",
       "      <th>Att</th>\n",
       "      <th>Cmp%</th>\n",
       "      <th>Yds</th>\n",
       "      <th>TD</th>\n",
       "      <th>TD%</th>\n",
       "      <th>Int</th>\n",
       "      <th>Int%</th>\n",
       "      <th>Y/A</th>\n",
       "      <th>AY/A</th>\n",
       "      <th>Y/C</th>\n",
       "      <th>Y/G</th>\n",
       "      <th>Rate</th>\n",
       "      <th>seasons</th>\n",
       "      <th>name</th>\n",
       "      <th>recent_team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Joe Burrow</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>74.4</td>\n",
       "      <td>287.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>8.38</td>\n",
       "      <td>9.9</td>\n",
       "      <td>26.1</td>\n",
       "      <td>153.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Joe Burrow</td>\n",
       "      <td>CIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Kyle Trask</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>2021</td>\n",
       "      <td>29.0</td>\n",
       "      <td>552.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>67.9</td>\n",
       "      <td>7386.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>9.1</td>\n",
       "      <td>9.95</td>\n",
       "      <td>13.4</td>\n",
       "      <td>254.7</td>\n",
       "      <td>168.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Kyle Trask</td>\n",
       "      <td>TB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Jordan Love</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2020</td>\n",
       "      <td>38.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>61.2</td>\n",
       "      <td>8600.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.55</td>\n",
       "      <td>12.5</td>\n",
       "      <td>226.3</td>\n",
       "      <td>137.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Jordan Love</td>\n",
       "      <td>GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Nathan Peterman</td>\n",
       "      <td>5</td>\n",
       "      <td>171</td>\n",
       "      <td>2017</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>46.5</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.7</td>\n",
       "      <td>9.4</td>\n",
       "      <td>55.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Nathan Peterman</td>\n",
       "      <td>BUF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Will Grier</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>2019</td>\n",
       "      <td>22.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>7354.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>9.4</td>\n",
       "      <td>10.03</td>\n",
       "      <td>14.3</td>\n",
       "      <td>334.3</td>\n",
       "      <td>169.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Will Grier</td>\n",
       "      <td>CAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Jameis Winston</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>27.0</td>\n",
       "      <td>562.0</td>\n",
       "      <td>851.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>7964.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>9.4</td>\n",
       "      <td>9.41</td>\n",
       "      <td>14.2</td>\n",
       "      <td>295.0</td>\n",
       "      <td>163.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jameis Winston</td>\n",
       "      <td>TB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Nick Foles</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>2012</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.13</td>\n",
       "      <td>11.4</td>\n",
       "      <td>57.0</td>\n",
       "      <td>122.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Nick Foles</td>\n",
       "      <td>PHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Cam Newton</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.50</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>87.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Cam Newton</td>\n",
       "      <td>CAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Marcus Mariota</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>41.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>66.8</td>\n",
       "      <td>10796.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>10.51</td>\n",
       "      <td>13.9</td>\n",
       "      <td>263.3</td>\n",
       "      <td>171.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Marcus Mariota</td>\n",
       "      <td>TEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Ben DiNucci</td>\n",
       "      <td>7</td>\n",
       "      <td>231</td>\n",
       "      <td>2020</td>\n",
       "      <td>12.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>54.5</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>6.6</td>\n",
       "      <td>5.46</td>\n",
       "      <td>12.2</td>\n",
       "      <td>92.3</td>\n",
       "      <td>113.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Ben DiNucci</td>\n",
       "      <td>DAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pfr_player_name  round  pick  ...  seasons             name  recent_team\n",
       "102       Joe Burrow      1     1  ...      7.0       Joe Burrow          CIN\n",
       "97        Kyle Trask      2    64  ...      5.0       Kyle Trask           TB\n",
       "105      Jordan Love      1    26  ...      3.0      Jordan Love           GB\n",
       "146  Nathan Peterman      5   171  ...      7.0  Nathan Peterman          BUF\n",
       "119       Will Grier      3   100  ...      7.0       Will Grier          CAR\n",
       "..               ...    ...   ...  ...      ...              ...          ...\n",
       "61    Jameis Winston      1     1  ...      2.0   Jameis Winston           TB\n",
       "31        Nick Foles      3    88  ...      8.0       Nick Foles          PHI\n",
       "13        Cam Newton      1     1  ...      7.0       Cam Newton          CAR\n",
       "62    Marcus Mariota      1     2  ...      3.0   Marcus Mariota          TEN\n",
       "112      Ben DiNucci      7   231  ...      2.0      Ben DiNucci          DAL\n",
       "\n",
       "[81 rows x 21 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>Cmp</th>\n",
       "      <th>Att</th>\n",
       "      <th>Cmp%</th>\n",
       "      <th>Yds</th>\n",
       "      <th>TD</th>\n",
       "      <th>TD%</th>\n",
       "      <th>Int</th>\n",
       "      <th>Int%</th>\n",
       "      <th>Y/A</th>\n",
       "      <th>AY/A</th>\n",
       "      <th>Y/C</th>\n",
       "      <th>Y/G</th>\n",
       "      <th>Rate</th>\n",
       "      <th>seasons</th>\n",
       "      <th>seasons_with_draft_team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>67.6</td>\n",
       "      <td>8403.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>10.57</td>\n",
       "      <td>13.9</td>\n",
       "      <td>271.1</td>\n",
       "      <td>175.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>995.0</td>\n",
       "      <td>66.4</td>\n",
       "      <td>9285.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>9.3</td>\n",
       "      <td>10.38</td>\n",
       "      <td>14.0</td>\n",
       "      <td>168.8</td>\n",
       "      <td>170.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.0</td>\n",
       "      <td>695.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>62.6</td>\n",
       "      <td>8148.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.33</td>\n",
       "      <td>11.7</td>\n",
       "      <td>232.8</td>\n",
       "      <td>137.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>1645.0</td>\n",
       "      <td>70.3</td>\n",
       "      <td>13253.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>8.19</td>\n",
       "      <td>11.5</td>\n",
       "      <td>250.1</td>\n",
       "      <td>155.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>64.1</td>\n",
       "      <td>4265.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.88</td>\n",
       "      <td>10.5</td>\n",
       "      <td>142.2</td>\n",
       "      <td>123.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>37.0</td>\n",
       "      <td>614.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>61.5</td>\n",
       "      <td>7138.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>6.90</td>\n",
       "      <td>11.6</td>\n",
       "      <td>192.9</td>\n",
       "      <td>133.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>46.5</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.7</td>\n",
       "      <td>9.4</td>\n",
       "      <td>55.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>38.0</td>\n",
       "      <td>721.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>60.6</td>\n",
       "      <td>9972.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8.64</td>\n",
       "      <td>13.8</td>\n",
       "      <td>262.4</td>\n",
       "      <td>146.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>22.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.7</td>\n",
       "      <td>8.72</td>\n",
       "      <td>13.5</td>\n",
       "      <td>309.1</td>\n",
       "      <td>152.3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       "145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       "148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "\n",
       "[149 rows x 16 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7525  \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7407407407407407"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1],)))\n",
    "model.add(Dense(units=32, activation='tanh'))\n",
    "model.add(Normalization())\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(units=16, activation='tanh'))\n",
    "model.add(Dense(units=8, activation='tanh'))\n",
    "model.add(Normalization())\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='poisson')\n",
    "model.fit(X_train, y_train)\n",
    "y_probs = model.predict(X_test)\n",
    "y_preds = (y_probs >= 5).astype(int)\n",
    "\n",
    "roc_auc_score(y_test, y_probs)\n",
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class MyFirstModel(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "\n",
    "validating = MyFirstModel(first_name=\"marc\", last_name=\"nealer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  'import mfl as mfl',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np',\n",
       "  'df = mfl.api.data_loaders.load_qb_data_cleaned()',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata',\n",
       "  'df = mfldata.load_qb_data_cleaned()',\n",
       "  'df',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.ensemble',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom keras.layers import Dense',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom keras.layers import Dense',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Sigmoid',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  'nfl.import_draft_picks()',\n",
       "  'nfl.import_draft_picks(years=2002)',\n",
       "  'nfl.import_draft_picks(years=[2002])',\n",
       "  'df',\n",
       "  \"df.select_dtypes(include='int')\",\n",
       "  \"df.select_dtypes(include='float')\",\n",
       "  \"numerics_only = df.select_dtypes(include='float')\",\n",
       "  'numerics_only',\n",
       "  \"numerics_only = df.select_dtypes(include='float').dropna()\",\n",
       "  'numerics_only',\n",
       "  'numerics_only.head(5)',\n",
       "  'def map_response(x):\\n    if x >= 4:\\n        return 1\\n    else: \\n        return 0',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = LogisticRegression()\\nmodel.fit(X_train, y_train)',\n",
       "  'model = LogisticRegression()\\nmodel.fit(X_train, y_train)\\nmodel.predict(X_test)',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\nmodel.predict(X_test)',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = RandomForestClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = XGBClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = LogisticRegression(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = LogisticRegression()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_tste)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,0]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = RandomForestClassifier\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'y_probss',\n",
       "  'y_probs',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_preds)',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC(probability=1)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:5]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:4]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:4]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:8]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:8]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:9]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:9]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:10]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = CatBoostClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = CatBoostClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = CatBoostClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  'X_train',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'X_train',\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  'df = df.dropna()',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(iterations=300, n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional\\nfrom keras.models import Sequential',\n",
       "  'model = Sequential()',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense()\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(units=df.shape[0])\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(units=df.shape[1])\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=df.shape[1])\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=df.shape[1], units=64)\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=tuple(df.shape[1]), units=64)\\n)',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization\\nfrom keras.models import Sequential',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout\\nfrom keras.models import Sequential',\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)=\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'X_train',\n",
       "  'y_train',\n",
       "  'y_train.isna().sum(0)',\n",
       "  'y_train.isna().sum()',\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(ReLU(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'y_train.shape',\n",
       "  'X_train.shape',\n",
       "  'model',\n",
       "  'X_train',\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'X_train.shape[1]',\n",
       "  \"model = Sequential()\\nmodel.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\\nfrom keras.models import Sequential',\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=X_train.shape[1]))\\nmodel.add(Dense(units=32activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=X_train.shape[1]))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(,X_train.shape[1])))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_preds, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\ny_probs(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='roc-auc')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='roc_auc')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='SparseCategoricalCrossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\\ny_preds_NN = model.predict(X_test)\\ny_preds_NN = (y_preds_NN >= .5).astype(int)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\n\\n\\nmodel.fit(X_train, y_train)\\ny_preds_NN = model.predict(X_test)\\ny_preds_NN = (y_preds_NN >= .5).astype(int)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\\nfrom keras.models import Sequential',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        \",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n        model = model(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n        model = model(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        self.meric_dict = metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        self.model_results = pd.DataFrame(self.metric_dict)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        self.meric_dict = metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        self.model_results = pd.DataFrame(self.metric_dict)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='lr')\",\n",
       "  \"NFL = FranchiseQB(model='svc')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='svm')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='xgb')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='rf')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  'df',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        if self.model == 'catboost':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=X.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        if self.model == 'catboost':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=X.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  'X_train',\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat'\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=3):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=1):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(X.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       X['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend((model.predict_proba(X_test)[:,1]))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  'import pydantic',\n",
       "  'from pydantic import BaseModel',\n",
       "  'from pydantic import BaseModel\\n\\nclass MyFirstModel(BaseModel):\\n    first_name: str\\n    last_name: str\\n\\nvalidating = MyFirstModel(first_name=\"marc\", last_name=\"nealer\")',\n",
       "  \"MyFirstModel(first_name='Ben', last_name=0)\",\n",
       "  'from Exception import RuntimeError',\n",
       "  'Exception',\n",
       "  'ValueError',\n",
       "  'compile',\n",
       "  'globals()',\n",
       "  'globals().__len__()',\n",
       "  'globals()[9]',\n",
       "  'locals()',\n",
       "  'vars()'],\n",
       " '_oh': {6:      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       "  0       Sam Bradford      1  ...           LA                      4.0\n",
       "  1          Tim Tebow      1  ...          DEN                      2.0\n",
       "  2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       "  3         Colt McCoy      3  ...          CLE                      3.0\n",
       "  4         Mike Kafka      4  ...          PHI                      1.0\n",
       "  ..               ...    ...  ...          ...                      ...\n",
       "  144    C.J. Beathard      3  ...          NaN                      NaN\n",
       "  145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       "  146  Nathan Peterman      5  ...          BUF                      2.0\n",
       "  147       Brad Kaaya      6  ...          NaN                      NaN\n",
       "  148       Chad Kelly      7  ...          DEN                      1.0\n",
       "  \n",
       "  [149 rows x 22 columns],\n",
       "  18:       season  round  pick team  ... rec_tds def_solo_tackles def_ints def_sacks\n",
       "  6526    2002      1     1  HOU  ...     0.0              NaN      NaN       NaN\n",
       "  6527    2002      1     2  CAR  ...     0.0            557.0     11.0     159.5\n",
       "  6528    2002      1     3  DET  ...     0.0              NaN      NaN       NaN\n",
       "  6529    2002      1     4  BUF  ...     0.0              NaN      NaN       NaN\n",
       "  6530    2002      1     5  SDG  ...     0.0            631.0     21.0       NaN\n",
       "  ...      ...    ...   ...  ...  ...     ...              ...      ...       ...\n",
       "  6782    2002      7   257  WAS  ...     2.0             74.0      NaN       NaN\n",
       "  6783    2002      7   258  CAR  ...     0.0              3.0      NaN       NaN\n",
       "  6784    2002      7   259  DET  ...     0.0              NaN      NaN       NaN\n",
       "  6785    2002      7   260  BUF  ...     0.0             20.0      NaN       NaN\n",
       "  6786    2002      7   261  HOU  ...     NaN              NaN      NaN       NaN\n",
       "  \n",
       "  [261 rows x 36 columns],\n",
       "  19:      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       "  0       Sam Bradford      1  ...           LA                      4.0\n",
       "  1          Tim Tebow      1  ...          DEN                      2.0\n",
       "  2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       "  3         Colt McCoy      3  ...          CLE                      3.0\n",
       "  4         Mike Kafka      4  ...          PHI                      1.0\n",
       "  ..               ...    ...  ...          ...                      ...\n",
       "  144    C.J. Beathard      3  ...          NaN                      NaN\n",
       "  145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       "  146  Nathan Peterman      5  ...          BUF                      2.0\n",
       "  147       Brad Kaaya      6  ...          NaN                      NaN\n",
       "  148       Chad Kelly      7  ...          DEN                      1.0\n",
       "  \n",
       "  [149 rows x 22 columns],\n",
       "  20:      round  pick  season\n",
       "  0        1     1    2010\n",
       "  1        1    25    2010\n",
       "  2        2    48    2010\n",
       "  3        3    85    2010\n",
       "  4        4   122    2010\n",
       "  ..     ...   ...     ...\n",
       "  144      3   104    2017\n",
       "  145      4   135    2017\n",
       "  146      5   171    2017\n",
       "  147      6   215    2017\n",
       "  148      7   253    2017\n",
       "  \n",
       "  [149 rows x 3 columns],\n",
       "  21:         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "  144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       "  145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "  146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "  147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       "  148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "  \n",
       "  [149 rows x 16 columns],\n",
       "  23:         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "  144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       "  145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "  146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "  147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       "  148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "  \n",
       "  [149 rows x 16 columns],\n",
       "  25:         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "  142  25.0   423.0   696.0  60.8  ...  232.4  147.7      2.0                      1.0\n",
       "  143  23.0   459.0   747.0  61.4  ...  241.6  138.4      7.0                      1.0\n",
       "  145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "  146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "  148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "  \n",
       "  [108 rows x 16 columns],\n",
       "  26:       G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0  31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1  55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2  35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3  53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4  30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  \n",
       "  [5 rows x 16 columns],\n",
       "  31: LogisticRegression(),\n",
       "  32: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0]),\n",
       "  34: array([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0]),\n",
       "  35: 0.6666666666666666,\n",
       "  36: 0.6666666666666666,\n",
       "  37: 0.5185185185185185,\n",
       "  39: 0.7037037037037037,\n",
       "  40: 0.7407407407407407,\n",
       "  42: 0.5,\n",
       "  45: 0.41428571428571426,\n",
       "  46: 0.5857142857142856,\n",
       "  47: 0.41428571428571426,\n",
       "  49: 0.4392857142857143,\n",
       "  51: array([0.32, 0.23, 0.21, 0.05, 0.1 , 0.48, 0.16, 0.19, 0.45, 0.08, 0.37,\n",
       "         0.58, 0.36, 0.22, 0.37, 0.32, 0.2 , 0.21, 0.57, 0.59, 0.42, 0.17,\n",
       "         0.29, 0.07, 0.41, 0.12, 0.1 ]),\n",
       "  52: 0.6296296296296297,\n",
       "  54: 0.7407407407407407,\n",
       "  55: <function sklearn.metrics._ranking.roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)>,\n",
       "  56: 0.375,\n",
       "  57: 0.3571428571428571,\n",
       "  58: 0.5555555555555556,\n",
       "  61: 0.7407407407407407,\n",
       "  62: 0.5857142857142856,\n",
       "  64: 0.5357142857142857,\n",
       "  67: 0.4357142857142857,\n",
       "  70: 0.6000000000000001,\n",
       "  71: 0.7407407407407407,\n",
       "  74: 0.40714285714285714,\n",
       "  76: 0.5357142857142857,\n",
       "  77: 0.5785714285714285,\n",
       "  78: 0.7407407407407407,\n",
       "  79: 0.7407407407407407,\n",
       "  82:         G    Cmp     Att  Cmp%      Yds    TD   TD%   Int  Int%   Y/A\n",
       "  31    1.0    5.0     8.0  62.5     57.0   0.0   0.0   0.0   0.0   7.1\n",
       "  53   18.0  192.0   368.0  52.2   2732.0  16.0   4.3  10.0   2.7   7.4\n",
       "  97   29.0  552.0   813.0  67.9   7386.0  69.0   8.5  15.0   1.8   9.1\n",
       "  138  44.0  759.0  1166.0  65.1  10514.0  93.0   8.0  25.0   2.1   9.0\n",
       "  30   14.0  225.0   309.0  72.8   3175.0  33.0  10.7   4.0   1.3  10.3\n",
       "  ..    ...    ...     ...   ...      ...   ...   ...   ...   ...   ...\n",
       "  61   27.0  562.0   851.0  66.0   7964.0  65.0   7.6  28.0   3.3   9.4\n",
       "  126   8.0  218.0   340.0  64.1   2315.0  12.0   3.5   9.0   2.6   6.8\n",
       "  95   22.0  396.0   579.0  68.4   5373.0  63.0  10.9   9.0   1.6   9.3\n",
       "  146  10.0   20.0    43.0  46.5     94.0   0.0   0.0   2.0   4.7   2.2\n",
       "  81   13.0  281.0   450.0  62.4   4033.0  27.0   6.0   8.0   1.8   9.0\n",
       "  \n",
       "  [81 rows x 10 columns],\n",
       "  87:     pfr_player_name  round  pick  ...  seasons            name  recent_team\n",
       "  120     Ryan Finley      4   104  ...      8.0     Ryan Finley          CIN\n",
       "  28   Brandon Weeden      1    22  ...      4.0  Brandon Weeden          CLE\n",
       "  99      Davis Mills      3    67  ...      3.0     Davis Mills          HOU\n",
       "  69     Carson Wentz      1     2  ...      NaN             NaN          NaN\n",
       "  131   Mason Rudolph      3    76  ...      4.0   Mason Rudolph          PIT\n",
       "  ..              ...    ...   ...  ...      ...             ...          ...\n",
       "  129      Josh Rosen      1    10  ...      3.0      Josh Rosen          ARI\n",
       "  136    Danny Etling      7   219  ...      8.0    Danny Etling          NaN\n",
       "  1         Tim Tebow      1    25  ...      4.0       Tim Tebow          DEN\n",
       "  3        Colt McCoy      3    85  ...      4.0      Colt McCoy          CLE\n",
       "  96        Mac Jones      1    15  ...      3.0       Mac Jones           NE\n",
       "  \n",
       "  [111 rows x 21 columns],\n",
       "  92: 0.7777777777777778,\n",
       "  93: 0.7285714285714285,\n",
       "  95: 0.7714285714285714,\n",
       "  96: 0.7777777777777778,\n",
       "  97: 0.7714285714285714,\n",
       "  98: 0.7777777777777778,\n",
       "  116:         G    Cmp     Att  Cmp%      Yds  ...   AY/A   Y/C    Y/G   Rate  seasons\n",
       "  2    35.0  695.0  1110.0  62.6   8148.0  ...   7.33  11.7  232.8  137.2      3.0\n",
       "  31    1.0    5.0     8.0  62.5     57.0  ...   7.13  11.4   57.0  122.4      8.0\n",
       "  17   50.0  812.0  1317.0  61.7  10314.0  ...   7.88  12.7  206.3  140.7      4.0\n",
       "  100  48.0  728.0  1141.0  63.8   8948.0  ...   8.32  12.3  186.4  147.0      5.0\n",
       "  9    27.0  421.0   682.0  61.7   5018.0  ...   7.48  11.9  185.9  141.4      3.0\n",
       "  ..    ...    ...     ...   ...      ...  ...    ...   ...    ...    ...      ...\n",
       "  14   40.0  619.0  1147.0  54.0   7639.0  ...   6.21  12.3  191.0  119.1      4.0\n",
       "  138  44.0  759.0  1166.0  65.1  10514.0  ...   9.65  13.9  239.0  162.9      5.0\n",
       "  4    30.0  408.0   637.0  64.1   4265.0  ...   5.88  10.5  142.2  123.9      4.0\n",
       "  103  33.0  474.0   684.0  69.3   7442.0  ...  12.70  15.7  225.5  199.4      3.0\n",
       "  70   38.0  758.0  1205.0  62.9   8863.0  ...   7.48  11.7  233.2  137.0      3.0\n",
       "  \n",
       "  [81 rows x 15 columns],\n",
       "  117: 2      0\n",
       "  31     1\n",
       "  17     1\n",
       "  100    0\n",
       "  9      0\n",
       "        ..\n",
       "  14     1\n",
       "  138    0\n",
       "  4      0\n",
       "  103    0\n",
       "  70     0\n",
       "  Name: seasons_with_draft_team, Length: 81, dtype: int64,\n",
       "  118: 0,\n",
       "  119: 0,\n",
       "  122: (81,),\n",
       "  123: (81, 15),\n",
       "  124: <Sequential name=sequential_6, built=True>,\n",
       "  125:         G    Cmp     Att  Cmp%      Yds  ...   AY/A   Y/C    Y/G   Rate  seasons\n",
       "  2    35.0  695.0  1110.0  62.6   8148.0  ...   7.33  11.7  232.8  137.2      3.0\n",
       "  31    1.0    5.0     8.0  62.5     57.0  ...   7.13  11.4   57.0  122.4      8.0\n",
       "  17   50.0  812.0  1317.0  61.7  10314.0  ...   7.88  12.7  206.3  140.7      4.0\n",
       "  100  48.0  728.0  1141.0  63.8   8948.0  ...   8.32  12.3  186.4  147.0      5.0\n",
       "  9    27.0  421.0   682.0  61.7   5018.0  ...   7.48  11.9  185.9  141.4      3.0\n",
       "  ..    ...    ...     ...   ...      ...  ...    ...   ...    ...    ...      ...\n",
       "  14   40.0  619.0  1147.0  54.0   7639.0  ...   6.21  12.3  191.0  119.1      4.0\n",
       "  138  44.0  759.0  1166.0  65.1  10514.0  ...   9.65  13.9  239.0  162.9      5.0\n",
       "  4    30.0  408.0   637.0  64.1   4265.0  ...   5.88  10.5  142.2  123.9      4.0\n",
       "  103  33.0  474.0   684.0  69.3   7442.0  ...  12.70  15.7  225.5  199.4      3.0\n",
       "  70   38.0  758.0  1205.0  62.9   8863.0  ...   7.48  11.7  233.2  137.0      3.0\n",
       "  \n",
       "  [81 rows x 15 columns],\n",
       "  128: 15,\n",
       "  129: <keras.src.callbacks.history.History at 0x313944250>,\n",
       "  136: 0.6714285714285714,\n",
       "  137: 0.5571428571428572,\n",
       "  138: 0.5142857142857143,\n",
       "  139: 0.6178571428571428,\n",
       "  141: 1.0,\n",
       "  142: 0.5,\n",
       "  144: 0.55,\n",
       "  146: 0.7407407407407407,\n",
       "  154: 0.7407407407407407,\n",
       "  155: 0.42142857142857143,\n",
       "  156: 0.7407407407407407,\n",
       "  157: 0.7407407407407407,\n",
       "  158: 0.49999999999999994,\n",
       "  159: 0.7407407407407407,\n",
       "  162: 0.8148148148148148,\n",
       "  163: 0.8428571428571429,\n",
       "  201:    accuracy        f1   roc_auc\n",
       "  0  0.685185  0.105263  0.427324,\n",
       "  205:    accuracy   f1   roc_auc\n",
       "  0  0.685185  0.0  0.390223,\n",
       "  211:    accuracy   f1   roc_auc\n",
       "  0  0.583333  0.0  0.326931,\n",
       "  214:    accuracy   f1   roc_auc\n",
       "  0  0.666667  0.1  0.404409,\n",
       "  215:      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       "  0       Sam Bradford      1  ...           LA                      4.0\n",
       "  1          Tim Tebow      1  ...          DEN                      2.0\n",
       "  2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       "  3         Colt McCoy      3  ...          CLE                      3.0\n",
       "  4         Mike Kafka      4  ...          PHI                      1.0\n",
       "  ..               ...    ...  ...          ...                      ...\n",
       "  142    DeShone Kizer      2  ...          CLE                      1.0\n",
       "  143       Davis Webb      3  ...          BUF                      1.0\n",
       "  145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       "  146  Nathan Peterman      5  ...          BUF                      2.0\n",
       "  148       Chad Kelly      7  ...          DEN                      1.0\n",
       "  \n",
       "  [108 rows x 22 columns],\n",
       "  219:    accuracy        f1   roc_auc\n",
       "  0  0.694444  0.108108  0.428634,\n",
       "  221: 0.7928571428571428,\n",
       "  225:    accuracy        f1   roc_auc\n",
       "  0  0.694444  0.108108  0.428634,\n",
       "  226:      pfr_player_name  round  pick  ...  seasons             name  recent_team\n",
       "  102       Joe Burrow      1     1  ...      7.0       Joe Burrow          CIN\n",
       "  97        Kyle Trask      2    64  ...      5.0       Kyle Trask           TB\n",
       "  105      Jordan Love      1    26  ...      3.0      Jordan Love           GB\n",
       "  146  Nathan Peterman      5   171  ...      7.0  Nathan Peterman          BUF\n",
       "  119       Will Grier      3   100  ...      7.0       Will Grier          CAR\n",
       "  ..               ...    ...   ...  ...      ...              ...          ...\n",
       "  61    Jameis Winston      1     1  ...      2.0   Jameis Winston           TB\n",
       "  31        Nick Foles      3    88  ...      8.0       Nick Foles          PHI\n",
       "  13        Cam Newton      1     1  ...      7.0       Cam Newton          CAR\n",
       "  62    Marcus Mariota      1     2  ...      3.0   Marcus Mariota          TEN\n",
       "  112      Ben DiNucci      7   231  ...      2.0      Ben DiNucci          DAL\n",
       "  \n",
       "  [81 rows x 21 columns],\n",
       "  238:    accuracy        f1  roc_auc\n",
       "  0  0.703704  0.428571   0.7189,\n",
       "  242:    accuracy        f1  roc_auc\n",
       "  0  0.657407  0.350877  0.68529,\n",
       "  249:    accuracy        f1   roc_auc\n",
       "  0   0.62037  0.349206  0.666085,\n",
       "  259:    accuracy        f1   roc_auc\n",
       "  0  0.851852  0.666667  0.921429,\n",
       "  265: Exception,\n",
       "  266: ValueError,\n",
       "  267: <function compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1, *, _feature_version=-1)>,\n",
       "  268: {...},\n",
       "  269: 427,\n",
       "  271: {...}},\n",
       " '_dh': [PosixPath('/Users/benstager/Desktop/mfl_project/mfl/model_dev')],\n",
       " 'In': ['',\n",
       "  'import mfl as mfl',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np',\n",
       "  'df = mfl.api.data_loaders.load_qb_data_cleaned()',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata',\n",
       "  'df = mfldata.load_qb_data_cleaned()',\n",
       "  'df',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.ensemble',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom keras.layers import Dense',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom keras.layers import Dense',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Sigmoid',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  'nfl.import_draft_picks()',\n",
       "  'nfl.import_draft_picks(years=2002)',\n",
       "  'nfl.import_draft_picks(years=[2002])',\n",
       "  'df',\n",
       "  \"df.select_dtypes(include='int')\",\n",
       "  \"df.select_dtypes(include='float')\",\n",
       "  \"numerics_only = df.select_dtypes(include='float')\",\n",
       "  'numerics_only',\n",
       "  \"numerics_only = df.select_dtypes(include='float').dropna()\",\n",
       "  'numerics_only',\n",
       "  'numerics_only.head(5)',\n",
       "  'def map_response(x):\\n    if x >= 4:\\n        return 1\\n    else: \\n        return 0',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = LogisticRegression()\\nmodel.fit(X_train, y_train)',\n",
       "  'model = LogisticRegression()\\nmodel.fit(X_train, y_train)\\nmodel.predict(X_test)',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\nmodel.predict(X_test)',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = RandomForestClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = XGBClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = LogisticRegression(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = LogisticRegression()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_tste)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,0]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = RandomForestClassifier\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'y_probss',\n",
       "  'y_probs',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_preds)',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC(probability=1)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:5]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:4]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:4]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:8]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:8]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:9]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:9]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:10]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = CatBoostClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = CatBoostClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = CatBoostClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  'X_train',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'X_train',\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  'df = df.dropna()',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(iterations=300, n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional\\nfrom keras.models import Sequential',\n",
       "  'model = Sequential()',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense()\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(units=df.shape[0])\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(units=df.shape[1])\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=df.shape[1])\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=df.shape[1], units=64)\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=tuple(df.shape[1]), units=64)\\n)',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization\\nfrom keras.models import Sequential',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout\\nfrom keras.models import Sequential',\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)=\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'X_train',\n",
       "  'y_train',\n",
       "  'y_train.isna().sum(0)',\n",
       "  'y_train.isna().sum()',\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(ReLU(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'y_train.shape',\n",
       "  'X_train.shape',\n",
       "  'model',\n",
       "  'X_train',\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'X_train.shape[1]',\n",
       "  \"model = Sequential()\\nmodel.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\\nfrom keras.models import Sequential',\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=X_train.shape[1]))\\nmodel.add(Dense(units=32activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=X_train.shape[1]))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(,X_train.shape[1])))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_preds, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\ny_probs(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='roc-auc')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='roc_auc')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='SparseCategoricalCrossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\\ny_preds_NN = model.predict(X_test)\\ny_preds_NN = (y_preds_NN >= .5).astype(int)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\n\\n\\nmodel.fit(X_train, y_train)\\ny_preds_NN = model.predict(X_test)\\ny_preds_NN = (y_preds_NN >= .5).astype(int)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\\nfrom keras.models import Sequential',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        \",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n        model = model(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n        model = model(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        self.meric_dict = metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        self.model_results = pd.DataFrame(self.metric_dict)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        self.meric_dict = metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        self.model_results = pd.DataFrame(self.metric_dict)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='lr')\",\n",
       "  \"NFL = FranchiseQB(model='svc')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='svm')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='xgb')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='rf')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  'df',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        if self.model == 'catboost':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=X.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        if self.model == 'catboost':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=X.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  'X_train',\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat'\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=3):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=1):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(X.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       X['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend((model.predict_proba(X_test)[:,1]))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  'import pydantic',\n",
       "  'from pydantic import BaseModel',\n",
       "  'from pydantic import BaseModel\\n\\nclass MyFirstModel(BaseModel):\\n    first_name: str\\n    last_name: str\\n\\nvalidating = MyFirstModel(first_name=\"marc\", last_name=\"nealer\")',\n",
       "  \"MyFirstModel(first_name='Ben', last_name=0)\",\n",
       "  'from Exception import RuntimeError',\n",
       "  'Exception',\n",
       "  'ValueError',\n",
       "  'compile',\n",
       "  'globals()',\n",
       "  'globals().__len__()',\n",
       "  'globals()[9]',\n",
       "  'locals()',\n",
       "  'vars()'],\n",
       " 'Out': {6:      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       "  0       Sam Bradford      1  ...           LA                      4.0\n",
       "  1          Tim Tebow      1  ...          DEN                      2.0\n",
       "  2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       "  3         Colt McCoy      3  ...          CLE                      3.0\n",
       "  4         Mike Kafka      4  ...          PHI                      1.0\n",
       "  ..               ...    ...  ...          ...                      ...\n",
       "  144    C.J. Beathard      3  ...          NaN                      NaN\n",
       "  145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       "  146  Nathan Peterman      5  ...          BUF                      2.0\n",
       "  147       Brad Kaaya      6  ...          NaN                      NaN\n",
       "  148       Chad Kelly      7  ...          DEN                      1.0\n",
       "  \n",
       "  [149 rows x 22 columns],\n",
       "  18:       season  round  pick team  ... rec_tds def_solo_tackles def_ints def_sacks\n",
       "  6526    2002      1     1  HOU  ...     0.0              NaN      NaN       NaN\n",
       "  6527    2002      1     2  CAR  ...     0.0            557.0     11.0     159.5\n",
       "  6528    2002      1     3  DET  ...     0.0              NaN      NaN       NaN\n",
       "  6529    2002      1     4  BUF  ...     0.0              NaN      NaN       NaN\n",
       "  6530    2002      1     5  SDG  ...     0.0            631.0     21.0       NaN\n",
       "  ...      ...    ...   ...  ...  ...     ...              ...      ...       ...\n",
       "  6782    2002      7   257  WAS  ...     2.0             74.0      NaN       NaN\n",
       "  6783    2002      7   258  CAR  ...     0.0              3.0      NaN       NaN\n",
       "  6784    2002      7   259  DET  ...     0.0              NaN      NaN       NaN\n",
       "  6785    2002      7   260  BUF  ...     0.0             20.0      NaN       NaN\n",
       "  6786    2002      7   261  HOU  ...     NaN              NaN      NaN       NaN\n",
       "  \n",
       "  [261 rows x 36 columns],\n",
       "  19:      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       "  0       Sam Bradford      1  ...           LA                      4.0\n",
       "  1          Tim Tebow      1  ...          DEN                      2.0\n",
       "  2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       "  3         Colt McCoy      3  ...          CLE                      3.0\n",
       "  4         Mike Kafka      4  ...          PHI                      1.0\n",
       "  ..               ...    ...  ...          ...                      ...\n",
       "  144    C.J. Beathard      3  ...          NaN                      NaN\n",
       "  145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       "  146  Nathan Peterman      5  ...          BUF                      2.0\n",
       "  147       Brad Kaaya      6  ...          NaN                      NaN\n",
       "  148       Chad Kelly      7  ...          DEN                      1.0\n",
       "  \n",
       "  [149 rows x 22 columns],\n",
       "  20:      round  pick  season\n",
       "  0        1     1    2010\n",
       "  1        1    25    2010\n",
       "  2        2    48    2010\n",
       "  3        3    85    2010\n",
       "  4        4   122    2010\n",
       "  ..     ...   ...     ...\n",
       "  144      3   104    2017\n",
       "  145      4   135    2017\n",
       "  146      5   171    2017\n",
       "  147      6   215    2017\n",
       "  148      7   253    2017\n",
       "  \n",
       "  [149 rows x 3 columns],\n",
       "  21:         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "  144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       "  145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "  146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "  147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       "  148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "  \n",
       "  [149 rows x 16 columns],\n",
       "  23:         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "  144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       "  145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "  146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "  147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       "  148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "  \n",
       "  [149 rows x 16 columns],\n",
       "  25:         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "  142  25.0   423.0   696.0  60.8  ...  232.4  147.7      2.0                      1.0\n",
       "  143  23.0   459.0   747.0  61.4  ...  241.6  138.4      7.0                      1.0\n",
       "  145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "  146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "  148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "  \n",
       "  [108 rows x 16 columns],\n",
       "  26:       G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0  31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1  55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2  35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3  53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4  30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  \n",
       "  [5 rows x 16 columns],\n",
       "  31: LogisticRegression(),\n",
       "  32: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0]),\n",
       "  34: array([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0]),\n",
       "  35: 0.6666666666666666,\n",
       "  36: 0.6666666666666666,\n",
       "  37: 0.5185185185185185,\n",
       "  39: 0.7037037037037037,\n",
       "  40: 0.7407407407407407,\n",
       "  42: 0.5,\n",
       "  45: 0.41428571428571426,\n",
       "  46: 0.5857142857142856,\n",
       "  47: 0.41428571428571426,\n",
       "  49: 0.4392857142857143,\n",
       "  51: array([0.32, 0.23, 0.21, 0.05, 0.1 , 0.48, 0.16, 0.19, 0.45, 0.08, 0.37,\n",
       "         0.58, 0.36, 0.22, 0.37, 0.32, 0.2 , 0.21, 0.57, 0.59, 0.42, 0.17,\n",
       "         0.29, 0.07, 0.41, 0.12, 0.1 ]),\n",
       "  52: 0.6296296296296297,\n",
       "  54: 0.7407407407407407,\n",
       "  55: <function sklearn.metrics._ranking.roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)>,\n",
       "  56: 0.375,\n",
       "  57: 0.3571428571428571,\n",
       "  58: 0.5555555555555556,\n",
       "  61: 0.7407407407407407,\n",
       "  62: 0.5857142857142856,\n",
       "  64: 0.5357142857142857,\n",
       "  67: 0.4357142857142857,\n",
       "  70: 0.6000000000000001,\n",
       "  71: 0.7407407407407407,\n",
       "  74: 0.40714285714285714,\n",
       "  76: 0.5357142857142857,\n",
       "  77: 0.5785714285714285,\n",
       "  78: 0.7407407407407407,\n",
       "  79: 0.7407407407407407,\n",
       "  82:         G    Cmp     Att  Cmp%      Yds    TD   TD%   Int  Int%   Y/A\n",
       "  31    1.0    5.0     8.0  62.5     57.0   0.0   0.0   0.0   0.0   7.1\n",
       "  53   18.0  192.0   368.0  52.2   2732.0  16.0   4.3  10.0   2.7   7.4\n",
       "  97   29.0  552.0   813.0  67.9   7386.0  69.0   8.5  15.0   1.8   9.1\n",
       "  138  44.0  759.0  1166.0  65.1  10514.0  93.0   8.0  25.0   2.1   9.0\n",
       "  30   14.0  225.0   309.0  72.8   3175.0  33.0  10.7   4.0   1.3  10.3\n",
       "  ..    ...    ...     ...   ...      ...   ...   ...   ...   ...   ...\n",
       "  61   27.0  562.0   851.0  66.0   7964.0  65.0   7.6  28.0   3.3   9.4\n",
       "  126   8.0  218.0   340.0  64.1   2315.0  12.0   3.5   9.0   2.6   6.8\n",
       "  95   22.0  396.0   579.0  68.4   5373.0  63.0  10.9   9.0   1.6   9.3\n",
       "  146  10.0   20.0    43.0  46.5     94.0   0.0   0.0   2.0   4.7   2.2\n",
       "  81   13.0  281.0   450.0  62.4   4033.0  27.0   6.0   8.0   1.8   9.0\n",
       "  \n",
       "  [81 rows x 10 columns],\n",
       "  87:     pfr_player_name  round  pick  ...  seasons            name  recent_team\n",
       "  120     Ryan Finley      4   104  ...      8.0     Ryan Finley          CIN\n",
       "  28   Brandon Weeden      1    22  ...      4.0  Brandon Weeden          CLE\n",
       "  99      Davis Mills      3    67  ...      3.0     Davis Mills          HOU\n",
       "  69     Carson Wentz      1     2  ...      NaN             NaN          NaN\n",
       "  131   Mason Rudolph      3    76  ...      4.0   Mason Rudolph          PIT\n",
       "  ..              ...    ...   ...  ...      ...             ...          ...\n",
       "  129      Josh Rosen      1    10  ...      3.0      Josh Rosen          ARI\n",
       "  136    Danny Etling      7   219  ...      8.0    Danny Etling          NaN\n",
       "  1         Tim Tebow      1    25  ...      4.0       Tim Tebow          DEN\n",
       "  3        Colt McCoy      3    85  ...      4.0      Colt McCoy          CLE\n",
       "  96        Mac Jones      1    15  ...      3.0       Mac Jones           NE\n",
       "  \n",
       "  [111 rows x 21 columns],\n",
       "  92: 0.7777777777777778,\n",
       "  93: 0.7285714285714285,\n",
       "  95: 0.7714285714285714,\n",
       "  96: 0.7777777777777778,\n",
       "  97: 0.7714285714285714,\n",
       "  98: 0.7777777777777778,\n",
       "  116:         G    Cmp     Att  Cmp%      Yds  ...   AY/A   Y/C    Y/G   Rate  seasons\n",
       "  2    35.0  695.0  1110.0  62.6   8148.0  ...   7.33  11.7  232.8  137.2      3.0\n",
       "  31    1.0    5.0     8.0  62.5     57.0  ...   7.13  11.4   57.0  122.4      8.0\n",
       "  17   50.0  812.0  1317.0  61.7  10314.0  ...   7.88  12.7  206.3  140.7      4.0\n",
       "  100  48.0  728.0  1141.0  63.8   8948.0  ...   8.32  12.3  186.4  147.0      5.0\n",
       "  9    27.0  421.0   682.0  61.7   5018.0  ...   7.48  11.9  185.9  141.4      3.0\n",
       "  ..    ...    ...     ...   ...      ...  ...    ...   ...    ...    ...      ...\n",
       "  14   40.0  619.0  1147.0  54.0   7639.0  ...   6.21  12.3  191.0  119.1      4.0\n",
       "  138  44.0  759.0  1166.0  65.1  10514.0  ...   9.65  13.9  239.0  162.9      5.0\n",
       "  4    30.0  408.0   637.0  64.1   4265.0  ...   5.88  10.5  142.2  123.9      4.0\n",
       "  103  33.0  474.0   684.0  69.3   7442.0  ...  12.70  15.7  225.5  199.4      3.0\n",
       "  70   38.0  758.0  1205.0  62.9   8863.0  ...   7.48  11.7  233.2  137.0      3.0\n",
       "  \n",
       "  [81 rows x 15 columns],\n",
       "  117: 2      0\n",
       "  31     1\n",
       "  17     1\n",
       "  100    0\n",
       "  9      0\n",
       "        ..\n",
       "  14     1\n",
       "  138    0\n",
       "  4      0\n",
       "  103    0\n",
       "  70     0\n",
       "  Name: seasons_with_draft_team, Length: 81, dtype: int64,\n",
       "  118: 0,\n",
       "  119: 0,\n",
       "  122: (81,),\n",
       "  123: (81, 15),\n",
       "  124: <Sequential name=sequential_6, built=True>,\n",
       "  125:         G    Cmp     Att  Cmp%      Yds  ...   AY/A   Y/C    Y/G   Rate  seasons\n",
       "  2    35.0  695.0  1110.0  62.6   8148.0  ...   7.33  11.7  232.8  137.2      3.0\n",
       "  31    1.0    5.0     8.0  62.5     57.0  ...   7.13  11.4   57.0  122.4      8.0\n",
       "  17   50.0  812.0  1317.0  61.7  10314.0  ...   7.88  12.7  206.3  140.7      4.0\n",
       "  100  48.0  728.0  1141.0  63.8   8948.0  ...   8.32  12.3  186.4  147.0      5.0\n",
       "  9    27.0  421.0   682.0  61.7   5018.0  ...   7.48  11.9  185.9  141.4      3.0\n",
       "  ..    ...    ...     ...   ...      ...  ...    ...   ...    ...    ...      ...\n",
       "  14   40.0  619.0  1147.0  54.0   7639.0  ...   6.21  12.3  191.0  119.1      4.0\n",
       "  138  44.0  759.0  1166.0  65.1  10514.0  ...   9.65  13.9  239.0  162.9      5.0\n",
       "  4    30.0  408.0   637.0  64.1   4265.0  ...   5.88  10.5  142.2  123.9      4.0\n",
       "  103  33.0  474.0   684.0  69.3   7442.0  ...  12.70  15.7  225.5  199.4      3.0\n",
       "  70   38.0  758.0  1205.0  62.9   8863.0  ...   7.48  11.7  233.2  137.0      3.0\n",
       "  \n",
       "  [81 rows x 15 columns],\n",
       "  128: 15,\n",
       "  129: <keras.src.callbacks.history.History at 0x313944250>,\n",
       "  136: 0.6714285714285714,\n",
       "  137: 0.5571428571428572,\n",
       "  138: 0.5142857142857143,\n",
       "  139: 0.6178571428571428,\n",
       "  141: 1.0,\n",
       "  142: 0.5,\n",
       "  144: 0.55,\n",
       "  146: 0.7407407407407407,\n",
       "  154: 0.7407407407407407,\n",
       "  155: 0.42142857142857143,\n",
       "  156: 0.7407407407407407,\n",
       "  157: 0.7407407407407407,\n",
       "  158: 0.49999999999999994,\n",
       "  159: 0.7407407407407407,\n",
       "  162: 0.8148148148148148,\n",
       "  163: 0.8428571428571429,\n",
       "  201:    accuracy        f1   roc_auc\n",
       "  0  0.685185  0.105263  0.427324,\n",
       "  205:    accuracy   f1   roc_auc\n",
       "  0  0.685185  0.0  0.390223,\n",
       "  211:    accuracy   f1   roc_auc\n",
       "  0  0.583333  0.0  0.326931,\n",
       "  214:    accuracy   f1   roc_auc\n",
       "  0  0.666667  0.1  0.404409,\n",
       "  215:      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       "  0       Sam Bradford      1  ...           LA                      4.0\n",
       "  1          Tim Tebow      1  ...          DEN                      2.0\n",
       "  2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       "  3         Colt McCoy      3  ...          CLE                      3.0\n",
       "  4         Mike Kafka      4  ...          PHI                      1.0\n",
       "  ..               ...    ...  ...          ...                      ...\n",
       "  142    DeShone Kizer      2  ...          CLE                      1.0\n",
       "  143       Davis Webb      3  ...          BUF                      1.0\n",
       "  145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       "  146  Nathan Peterman      5  ...          BUF                      2.0\n",
       "  148       Chad Kelly      7  ...          DEN                      1.0\n",
       "  \n",
       "  [108 rows x 22 columns],\n",
       "  219:    accuracy        f1   roc_auc\n",
       "  0  0.694444  0.108108  0.428634,\n",
       "  221: 0.7928571428571428,\n",
       "  225:    accuracy        f1   roc_auc\n",
       "  0  0.694444  0.108108  0.428634,\n",
       "  226:      pfr_player_name  round  pick  ...  seasons             name  recent_team\n",
       "  102       Joe Burrow      1     1  ...      7.0       Joe Burrow          CIN\n",
       "  97        Kyle Trask      2    64  ...      5.0       Kyle Trask           TB\n",
       "  105      Jordan Love      1    26  ...      3.0      Jordan Love           GB\n",
       "  146  Nathan Peterman      5   171  ...      7.0  Nathan Peterman          BUF\n",
       "  119       Will Grier      3   100  ...      7.0       Will Grier          CAR\n",
       "  ..               ...    ...   ...  ...      ...              ...          ...\n",
       "  61    Jameis Winston      1     1  ...      2.0   Jameis Winston           TB\n",
       "  31        Nick Foles      3    88  ...      8.0       Nick Foles          PHI\n",
       "  13        Cam Newton      1     1  ...      7.0       Cam Newton          CAR\n",
       "  62    Marcus Mariota      1     2  ...      3.0   Marcus Mariota          TEN\n",
       "  112      Ben DiNucci      7   231  ...      2.0      Ben DiNucci          DAL\n",
       "  \n",
       "  [81 rows x 21 columns],\n",
       "  238:    accuracy        f1  roc_auc\n",
       "  0  0.703704  0.428571   0.7189,\n",
       "  242:    accuracy        f1  roc_auc\n",
       "  0  0.657407  0.350877  0.68529,\n",
       "  249:    accuracy        f1   roc_auc\n",
       "  0   0.62037  0.349206  0.666085,\n",
       "  259:    accuracy        f1   roc_auc\n",
       "  0  0.851852  0.666667  0.921429,\n",
       "  265: Exception,\n",
       "  266: ValueError,\n",
       "  267: <function compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1, *, _feature_version=-1)>,\n",
       "  268: {...},\n",
       "  269: 427,\n",
       "  271: {...}},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x103f96890>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x103fa2cd0>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x103fa2cd0>,\n",
       " 'open': <function io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)>,\n",
       " '_': {...},\n",
       " '__': 427,\n",
       " '___': {...},\n",
       " '__vsc_ipynb_file__': '/Users/benstager/Desktop/mfl_project/mfl/model_dev/model_dev_1_2.ipynb',\n",
       " '_i': 'locals()',\n",
       " '_ii': 'globals()[9]',\n",
       " '_iii': 'globals().__len__()',\n",
       " '_i1': 'import mfl as mfl',\n",
       " 'mfl': <module 'mfl' from '/Users/benstager/Desktop/mfl_project/mfl/__init__.py'>,\n",
       " '_i2': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np',\n",
       " 'pd': <module 'pandas' from '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/__init__.py'>,\n",
       " 'np': <module 'numpy' from '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/__init__.py'>,\n",
       " '_i3': 'df = mfl.api.data_loaders.load_qb_data_cleaned()',\n",
       " '_i4': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata',\n",
       " 'mfldata': <module 'mfl.api.data_loaders' from '/Users/benstager/Desktop/mfl_project/mfl/api/data_loaders.py'>,\n",
       " '_i5': 'df = mfldata.load_qb_data_cleaned()',\n",
       " 'df':      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       " 0       Sam Bradford      1  ...           LA                      4.0\n",
       " 1          Tim Tebow      1  ...          DEN                      2.0\n",
       " 2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       " 3         Colt McCoy      3  ...          CLE                      3.0\n",
       " 4         Mike Kafka      4  ...          PHI                      1.0\n",
       " ..               ...    ...  ...          ...                      ...\n",
       " 142    DeShone Kizer      2  ...          CLE                      1.0\n",
       " 143       Davis Webb      3  ...          BUF                      1.0\n",
       " 145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       " 146  Nathan Peterman      5  ...          BUF                      2.0\n",
       " 148       Chad Kelly      7  ...          DEN                      1.0\n",
       " \n",
       " [108 rows x 22 columns],\n",
       " '_i6': 'df',\n",
       " '_6':      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       " 0       Sam Bradford      1  ...           LA                      4.0\n",
       " 1          Tim Tebow      1  ...          DEN                      2.0\n",
       " 2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       " 3         Colt McCoy      3  ...          CLE                      3.0\n",
       " 4         Mike Kafka      4  ...          PHI                      1.0\n",
       " ..               ...    ...  ...          ...                      ...\n",
       " 144    C.J. Beathard      3  ...          NaN                      NaN\n",
       " 145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       " 146  Nathan Peterman      5  ...          BUF                      2.0\n",
       " 147       Brad Kaaya      6  ...          NaN                      NaN\n",
       " 148       Chad Kelly      7  ...          DEN                      1.0\n",
       " \n",
       " [149 rows x 22 columns],\n",
       " '_i7': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.ensemble',\n",
       " '_i8': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier',\n",
       " 'nfl': <module 'nfl_data_py' from '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nfl_data_py/__init__.py'>,\n",
       " 'LogisticRegression': sklearn.linear_model._logistic.LogisticRegression,\n",
       " 'HistGradientBoostingClassifier': sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
       " 'RandomForestClassifier': sklearn.ensemble._forest.RandomForestClassifier,\n",
       " 'SVC': sklearn.svm._classes.SVC,\n",
       " 'XGBClassifier': xgboost.sklearn.XGBClassifier,\n",
       " 'XGBRFClassifier': xgboost.sklearn.XGBRFClassifier,\n",
       " '_i9': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom keras.layers import Dense',\n",
       " '_i10': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras',\n",
       " 'keras': <module 'keras' from '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/__init__.py'>,\n",
       " '_i11': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom keras.layers import Dense',\n",
       " 'Dense': keras.src.layers.core.dense.Dense,\n",
       " '_i12': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU',\n",
       " 'ReLU': keras.src.layers.activations.relu.ReLU,\n",
       " '_i13': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Sigmoid',\n",
       " '_i14': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       " 'Bidirectional': keras.src.layers.rnn.bidirectional.Bidirectional,\n",
       " '_i15': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       " 'CatBoostClassifier': catboost.core.CatBoostClassifier,\n",
       " '_i16': 'nfl.import_draft_picks()',\n",
       " '_i17': 'nfl.import_draft_picks(years=2002)',\n",
       " '_i18': 'nfl.import_draft_picks(years=[2002])',\n",
       " '_18':       season  round  pick team  ... rec_tds def_solo_tackles def_ints def_sacks\n",
       " 6526    2002      1     1  HOU  ...     0.0              NaN      NaN       NaN\n",
       " 6527    2002      1     2  CAR  ...     0.0            557.0     11.0     159.5\n",
       " 6528    2002      1     3  DET  ...     0.0              NaN      NaN       NaN\n",
       " 6529    2002      1     4  BUF  ...     0.0              NaN      NaN       NaN\n",
       " 6530    2002      1     5  SDG  ...     0.0            631.0     21.0       NaN\n",
       " ...      ...    ...   ...  ...  ...     ...              ...      ...       ...\n",
       " 6782    2002      7   257  WAS  ...     2.0             74.0      NaN       NaN\n",
       " 6783    2002      7   258  CAR  ...     0.0              3.0      NaN       NaN\n",
       " 6784    2002      7   259  DET  ...     0.0              NaN      NaN       NaN\n",
       " 6785    2002      7   260  BUF  ...     0.0             20.0      NaN       NaN\n",
       " 6786    2002      7   261  HOU  ...     NaN              NaN      NaN       NaN\n",
       " \n",
       " [261 rows x 36 columns],\n",
       " '_i19': 'df',\n",
       " '_19':      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       " 0       Sam Bradford      1  ...           LA                      4.0\n",
       " 1          Tim Tebow      1  ...          DEN                      2.0\n",
       " 2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       " 3         Colt McCoy      3  ...          CLE                      3.0\n",
       " 4         Mike Kafka      4  ...          PHI                      1.0\n",
       " ..               ...    ...  ...          ...                      ...\n",
       " 144    C.J. Beathard      3  ...          NaN                      NaN\n",
       " 145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       " 146  Nathan Peterman      5  ...          BUF                      2.0\n",
       " 147       Brad Kaaya      6  ...          NaN                      NaN\n",
       " 148       Chad Kelly      7  ...          DEN                      1.0\n",
       " \n",
       " [149 rows x 22 columns],\n",
       " '_i20': \"df.select_dtypes(include='int')\",\n",
       " '_20':      round  pick  season\n",
       " 0        1     1    2010\n",
       " 1        1    25    2010\n",
       " 2        2    48    2010\n",
       " 3        3    85    2010\n",
       " 4        4   122    2010\n",
       " ..     ...   ...     ...\n",
       " 144      3   104    2017\n",
       " 145      4   135    2017\n",
       " 146      5   171    2017\n",
       " 147      6   215    2017\n",
       " 148      7   253    2017\n",
       " \n",
       " [149 rows x 3 columns],\n",
       " '_i21': \"df.select_dtypes(include='float')\",\n",
       " '_21':         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       " 0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       " 1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       " 2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       " 3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       " 4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       " ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       " 144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       " 145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       " 146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       " 147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       " 148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       " \n",
       " [149 rows x 16 columns],\n",
       " '_i22': \"numerics_only = df.select_dtypes(include='float')\",\n",
       " 'numerics_only':         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       " 0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       " 1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       " 2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       " 3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       " 4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       " ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       " 142  25.0   423.0   696.0  60.8  ...  232.4  147.7      2.0                      1.0\n",
       " 143  23.0   459.0   747.0  61.4  ...  241.6  138.4      7.0                      1.0\n",
       " 145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       " 146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       " 148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       " \n",
       " [108 rows x 16 columns],\n",
       " '_i23': 'numerics_only',\n",
       " '_23':         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       " 0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       " 1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       " 2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       " 3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       " 4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       " ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       " 144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       " 145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       " 146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       " 147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       " 148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       " \n",
       " [149 rows x 16 columns],\n",
       " '_i24': \"numerics_only = df.select_dtypes(include='float').dropna()\",\n",
       " '_i25': 'numerics_only',\n",
       " '_25':         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       " 0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       " 1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       " 2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       " 3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       " 4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       " ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       " 142  25.0   423.0   696.0  60.8  ...  232.4  147.7      2.0                      1.0\n",
       " 143  23.0   459.0   747.0  61.4  ...  241.6  138.4      7.0                      1.0\n",
       " 145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       " 146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       " 148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       " \n",
       " [108 rows x 16 columns],\n",
       " '_i26': 'numerics_only.head(5)',\n",
       " '_26':       G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       " 0  31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       " 1  55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       " 2  35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       " 3  53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       " 4  30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       " \n",
       " [5 rows x 16 columns],\n",
       " '_i27': 'def map_response(x):\\n    if x >= 4:\\n        return 1\\n    else: \\n        return 0',\n",
       " 'map_response': <function __main__.map_response(x)>,\n",
       " '_i28': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i29': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       " 'train_test_split': <function sklearn.model_selection._split.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)>,\n",
       " '_i30': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " 'X_train':      pfr_player_name  round  pick  ...  seasons             name  recent_team\n",
       " 102       Joe Burrow      1     1  ...      7.0       Joe Burrow          CIN\n",
       " 97        Kyle Trask      2    64  ...      5.0       Kyle Trask           TB\n",
       " 105      Jordan Love      1    26  ...      3.0      Jordan Love           GB\n",
       " 146  Nathan Peterman      5   171  ...      7.0  Nathan Peterman          BUF\n",
       " 119       Will Grier      3   100  ...      7.0       Will Grier          CAR\n",
       " ..               ...    ...   ...  ...      ...              ...          ...\n",
       " 61    Jameis Winston      1     1  ...      2.0   Jameis Winston           TB\n",
       " 31        Nick Foles      3    88  ...      8.0       Nick Foles          PHI\n",
       " 13        Cam Newton      1     1  ...      7.0       Cam Newton          CAR\n",
       " 62    Marcus Mariota      1     2  ...      3.0   Marcus Mariota          TEN\n",
       " 112      Ben DiNucci      7   231  ...      2.0      Ben DiNucci          DAL\n",
       " \n",
       " [81 rows x 21 columns],\n",
       " 'X_test':        pfr_player_name  round  pick  ...  seasons               name  recent_team\n",
       " 9            Tony Pike      6   204  ...      3.0          Tony Pike          CAR\n",
       " 29      Brock Osweiler      2    57  ...      3.0     Brock Osweiler          DEN\n",
       " 128         Josh Allen      1     7  ...      5.0         Josh Allen          BUF\n",
       " 106        Jalen Hurts      2    53  ...      7.0        Jalen Hurts          PHI\n",
       " 33        Ryan Lindley      6   185  ...      4.0       Ryan Lindley          ARI\n",
       " 2        Jimmy Clausen      2    48  ...      3.0      Jimmy Clausen          CAR\n",
       " 56   Zach Mettenberger      6   178  ...      3.0  Zach Mettenberger          TEN\n",
       " 46        Sean Renfree      7   249  ...      4.0       Sean Renfree          ATL\n",
       " 49   Teddy Bridgewater      1    32  ...      3.0  Teddy Bridgewater          MIN\n",
       " 77         Kevin Hogan      5   162  ...      4.0        Kevin Hogan          CLE\n",
       " 109         Jake Fromm      5   167  ...      3.0         Jake Fromm          NYG\n",
       " 103     Tua Tagovailoa      1     5  ...      3.0     Tua Tagovailoa          MIA\n",
       " 73        Cody Kessler      3    93  ...      4.0       Cody Kessler          CLE\n",
       " 0         Sam Bradford      1     1  ...      3.0       Sam Bradford           LA\n",
       " 7          Rusty Smith      6   176  ...      4.0        Rusty Smith          TEN\n",
       " 116       Daniel Jones      1     6  ...      3.0       Daniel Jones          NYG\n",
       " 15      Blaine Gabbert      1    10  ...      3.0     Blaine Gabbert          JAX\n",
       " 95       Justin Fields      1    11  ...      6.0      Justin Fields          CHI\n",
       " 70        Paxton Lynch      1    26  ...      3.0       Paxton Lynch          DEN\n",
       " 50          Derek Carr      2    36  ...      5.0         Derek Carr           LV\n",
       " 52        Logan Thomas      4   120  ...      4.0       Logan Thomas          ARI\n",
       " 93         Zach Wilson      1     2  ...      3.0        Zach Wilson          NYJ\n",
       " 138     Logan Woodside      7   249  ...      5.0     Logan Woodside          TEN\n",
       " 134          Luke Falk      6   199  ...      4.0          Luke Falk          NYJ\n",
       " 84      Desmond Ridder      3    74  ...      4.0     Desmond Ridder          ATL\n",
       " 30      Russell Wilson      3    75  ...      7.0     Russell Wilson          SEA\n",
       " 18    Colin Kaepernick      2    36  ...      4.0   Colin Kaepernick           SF\n",
       " \n",
       " [27 rows x 21 columns],\n",
       " 'y_train': 102    0\n",
       " 97     0\n",
       " 105    0\n",
       " 146    0\n",
       " 119    0\n",
       "       ..\n",
       " 61     1\n",
       " 31     1\n",
       " 13     1\n",
       " 62     1\n",
       " 112    0\n",
       " Name: seasons_with_draft_team, Length: 81, dtype: int64,\n",
       " 'y_test': 9      0\n",
       " 29     1\n",
       " 128    1\n",
       " 106    0\n",
       " 33     0\n",
       " 2      0\n",
       " 56     0\n",
       " 46     0\n",
       " 49     0\n",
       " 77     0\n",
       " 109    0\n",
       " 103    0\n",
       " 73     0\n",
       " 0      1\n",
       " 7      0\n",
       " 116    1\n",
       " 15     0\n",
       " 95     0\n",
       " 70     0\n",
       " 50     1\n",
       " 52     0\n",
       " 93     0\n",
       " 138    0\n",
       " 134    0\n",
       " 84     0\n",
       " 30     1\n",
       " 18     1\n",
       " Name: seasons_with_draft_team, dtype: int64,\n",
       " '_i31': 'model = LogisticRegression()\\nmodel.fit(X_train, y_train)',\n",
       " 'model': <catboost.core.CatBoostClassifier at 0x32068ced0>,\n",
       " '_31': LogisticRegression(),\n",
       " '_i32': 'model = LogisticRegression()\\nmodel.fit(X_train, y_train)\\nmodel.predict(X_test)',\n",
       " '_32': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0]),\n",
       " '_i33': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       " 'accuracy_score': <function sklearn.metrics._classification.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)>,\n",
       " 'f1_score': <function sklearn.metrics._classification.f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'roc_auc_score': <function sklearn.metrics._ranking.roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)>,\n",
       " 'recall_score': <function sklearn.metrics._classification.recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'precision_score': <function sklearn.metrics._classification.precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'precision_recall_curve': <function sklearn.metrics._ranking.precision_recall_curve(y_true, y_score=None, *, pos_label=None, sample_weight=None, drop_intermediate=False, probas_pred='deprecated')>,\n",
       " '_i34': 'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\nmodel.predict(X_test)',\n",
       " '_34': array([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0]),\n",
       " '_i35': 'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       " 'y_preds': array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1]),\n",
       " '_35': 0.6666666666666666,\n",
       " '_i36': 'model = RandomForestClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       " '_36': 0.6666666666666666,\n",
       " '_i37': 'model = XGBClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       " '_37': 0.5185185185185185,\n",
       " '_i38': 'model = LogisticRegression(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       " '_i39': 'model = LogisticRegression()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       " '_39': 0.7037037037037037,\n",
       " '_i40': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       " '_40': 0.7407407407407407,\n",
       " '_i41': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_tste)',\n",
       " 'y_probs': array([0.07915182, 0.24816314, 0.27551177, 0.06560755, 0.3334303 ,\n",
       "        0.36275897, 0.12708844, 0.24322529, 0.5979279 , 0.19634122,\n",
       "        0.07040088, 0.12356025, 0.19607455, 0.70742475, 0.35561972,\n",
       "        0.45213291, 0.7535539 , 0.0901011 , 0.40037947, 0.6902618 ,\n",
       "        0.33112739, 0.11957425, 0.11106133, 0.14325322, 0.09657287,\n",
       "        0.1470086 , 0.66689199]),\n",
       " '_i42': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_42': 0.5,\n",
       " '_i43': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_i44': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_i45': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_45': 0.41428571428571426,\n",
       " '_i46': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,0]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_46': 0.5857142857142856,\n",
       " '_i47': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_47': 0.41428571428571426,\n",
       " '_i48': 'model = RandomForestClassifier\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_i49': 'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_49': 0.4392857142857143,\n",
       " '_i50': 'y_probss',\n",
       " '_i51': 'y_probs',\n",
       " '_51': array([0.32, 0.23, 0.21, 0.05, 0.1 , 0.48, 0.16, 0.19, 0.45, 0.08, 0.37,\n",
       "        0.58, 0.36, 0.22, 0.37, 0.32, 0.2 , 0.21, 0.57, 0.59, 0.42, 0.17,\n",
       "        0.29, 0.07, 0.41, 0.12, 0.1 ]),\n",
       " '_i52': 'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_52': 0.6296296296296297,\n",
       " '_i53': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_i54': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_54': 0.7407407407407407,\n",
       " '_i55': 'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score',\n",
       " '_55': <function sklearn.metrics._ranking.roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)>,\n",
       " '_i56': 'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_preds)',\n",
       " '_56': 0.375,\n",
       " '_i57': 'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_57': 0.3571428571428571,\n",
       " '_i58': 'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_58': 0.5555555555555556,\n",
       " '_i59': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_i60': 'model = SVC(probability=1)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_i61': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_61': 0.7407407407407407,\n",
       " '_i62': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_62': 0.5857142857142856,\n",
       " '_i63': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:5]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i64': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_64': 0.5357142857142857,\n",
       " '_i65': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:4]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i66': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:4]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i67': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_67': 0.4357142857142857,\n",
       " '_i68': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:8]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i69': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:8]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i70': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_70': 0.6000000000000001,\n",
       " '_i71': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_71': 0.7407407407407407,\n",
       " '_i72': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:9]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i73': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:9]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i74': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_74': 0.40714285714285714,\n",
       " '_i75': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:10]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i76': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_76': 0.5357142857142857,\n",
       " '_i77': 'model = CatBoostClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_77': 0.5785714285714285,\n",
       " '_i78': 'model = CatBoostClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_78': 0.7407407407407407,\n",
       " '_i79': 'model = CatBoostClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_79': 0.7407407407407407,\n",
       " '_i80': \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i81': \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i82': 'X_train',\n",
       " '_82':         G    Cmp     Att  Cmp%      Yds    TD   TD%   Int  Int%   Y/A\n",
       " 31    1.0    5.0     8.0  62.5     57.0   0.0   0.0   0.0   0.0   7.1\n",
       " 53   18.0  192.0   368.0  52.2   2732.0  16.0   4.3  10.0   2.7   7.4\n",
       " 97   29.0  552.0   813.0  67.9   7386.0  69.0   8.5  15.0   1.8   9.1\n",
       " 138  44.0  759.0  1166.0  65.1  10514.0  93.0   8.0  25.0   2.1   9.0\n",
       " 30   14.0  225.0   309.0  72.8   3175.0  33.0  10.7   4.0   1.3  10.3\n",
       " ..    ...    ...     ...   ...      ...   ...   ...   ...   ...   ...\n",
       " 61   27.0  562.0   851.0  66.0   7964.0  65.0   7.6  28.0   3.3   9.4\n",
       " 126   8.0  218.0   340.0  64.1   2315.0  12.0   3.5   9.0   2.6   6.8\n",
       " 95   22.0  396.0   579.0  68.4   5373.0  63.0  10.9   9.0   1.6   9.3\n",
       " 146  10.0   20.0    43.0  46.5     94.0   0.0   0.0   2.0   4.7   2.2\n",
       " 81   13.0  281.0   450.0  62.4   4033.0  27.0   6.0   8.0   1.8   9.0\n",
       " \n",
       " [81 rows x 10 columns],\n",
       " '_i83': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i84': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i85': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i86': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i87': 'X_train',\n",
       " '_87':     pfr_player_name  round  pick  ...  seasons            name  recent_team\n",
       " 120     Ryan Finley      4   104  ...      8.0     Ryan Finley          CIN\n",
       " 28   Brandon Weeden      1    22  ...      4.0  Brandon Weeden          CLE\n",
       " 99      Davis Mills      3    67  ...      3.0     Davis Mills          HOU\n",
       " 69     Carson Wentz      1     2  ...      NaN             NaN          NaN\n",
       " 131   Mason Rudolph      3    76  ...      4.0   Mason Rudolph          PIT\n",
       " ..              ...    ...   ...  ...      ...             ...          ...\n",
       " 129      Josh Rosen      1    10  ...      3.0      Josh Rosen          ARI\n",
       " 136    Danny Etling      7   219  ...      8.0    Danny Etling          NaN\n",
       " 1         Tim Tebow      1    25  ...      4.0       Tim Tebow          DEN\n",
       " 3        Colt McCoy      3    85  ...      4.0      Colt McCoy          CLE\n",
       " 96        Mac Jones      1    15  ...      3.0       Mac Jones           NE\n",
       " \n",
       " [111 rows x 21 columns],\n",
       " '_i88': \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i89': 'df = df.dropna()',\n",
       " '_i90': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i91': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i92': \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_92': 0.7777777777777778,\n",
       " '_i93': \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_93': 0.7285714285714285,\n",
       " '_i94': \"model = CatBoostClassifier(iterations=300, n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_i95': \"model = CatBoostClassifier(iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_95': 0.7714285714285714,\n",
       " '_i96': \"model = CatBoostClassifier(iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_96': 0.7777777777777778,\n",
       " '_i97': \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_97': 0.7714285714285714,\n",
       " '_i98': \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_98': 0.7777777777777778,\n",
       " '_i99': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional\\nfrom keras.models import Sequential',\n",
       " 'Sequential': keras.src.models.sequential.Sequential,\n",
       " '_i100': 'model = Sequential()',\n",
       " '_i101': 'model = Sequential()\\nmodel.add(\\n    Dense()\\n)',\n",
       " '_i102': 'model = Sequential()\\nmodel.add(\\n    Dense(units=df.shape[0])\\n)',\n",
       " '_i103': 'model = Sequential()\\nmodel.add(\\n    Dense(units=df.shape[1])\\n)',\n",
       " '_i104': 'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=df.shape[1])\\n)',\n",
       " '_i105': 'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=df.shape[1], units=64)\\n)',\n",
       " '_i106': 'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=tuple(df.shape[1]), units=64)\\n)',\n",
       " '_i107': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization\\nfrom keras.models import Sequential',\n",
       " 'Normalization': keras.src.layers.preprocessing.normalization.Normalization,\n",
       " '_i108': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout\\nfrom keras.models import Sequential',\n",
       " 'Dropout': keras.src.layers.regularization.dropout.Dropout,\n",
       " '_i109': \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)=\",\n",
       " '_i110': \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       " '_i111': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i112': \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       " '_i113': \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       " '_i114': \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\",\n",
       " '_i115': \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       " '_i116': 'X_train',\n",
       " '_116':         G    Cmp     Att  Cmp%      Yds  ...   AY/A   Y/C    Y/G   Rate  seasons\n",
       " 2    35.0  695.0  1110.0  62.6   8148.0  ...   7.33  11.7  232.8  137.2      3.0\n",
       " 31    1.0    5.0     8.0  62.5     57.0  ...   7.13  11.4   57.0  122.4      8.0\n",
       " 17   50.0  812.0  1317.0  61.7  10314.0  ...   7.88  12.7  206.3  140.7      4.0\n",
       " 100  48.0  728.0  1141.0  63.8   8948.0  ...   8.32  12.3  186.4  147.0      5.0\n",
       " 9    27.0  421.0   682.0  61.7   5018.0  ...   7.48  11.9  185.9  141.4      3.0\n",
       " ..    ...    ...     ...   ...      ...  ...    ...   ...    ...    ...      ...\n",
       " 14   40.0  619.0  1147.0  54.0   7639.0  ...   6.21  12.3  191.0  119.1      4.0\n",
       " 138  44.0  759.0  1166.0  65.1  10514.0  ...   9.65  13.9  239.0  162.9      5.0\n",
       " 4    30.0  408.0   637.0  64.1   4265.0  ...   5.88  10.5  142.2  123.9      4.0\n",
       " 103  33.0  474.0   684.0  69.3   7442.0  ...  12.70  15.7  225.5  199.4      3.0\n",
       " 70   38.0  758.0  1205.0  62.9   8863.0  ...   7.48  11.7  233.2  137.0      3.0\n",
       " \n",
       " [81 rows x 15 columns],\n",
       " '_i117': 'y_train',\n",
       " '_117': 2      0\n",
       " 31     1\n",
       " 17     1\n",
       " 100    0\n",
       " 9      0\n",
       "       ..\n",
       " 14     1\n",
       " 138    0\n",
       " 4      0\n",
       " 103    0\n",
       " 70     0\n",
       " Name: seasons_with_draft_team, Length: 81, dtype: int64,\n",
       " '_i118': 'y_train.isna().sum(0)',\n",
       " '_118': 0,\n",
       " '_i119': 'y_train.isna().sum()',\n",
       " '_119': 0,\n",
       " '_i120': \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(ReLU(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       " '_i121': \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       " '_i122': 'y_train.shape',\n",
       " '_122': (81,),\n",
       " '_i123': 'X_train.shape',\n",
       " '_123': (81, 15),\n",
       " '_i124': 'model',\n",
       " '_124': <Sequential name=sequential_6, built=True>,\n",
       " '_i125': 'X_train',\n",
       " '_125':         G    Cmp     Att  Cmp%      Yds  ...   AY/A   Y/C    Y/G   Rate  seasons\n",
       " 2    35.0  695.0  1110.0  62.6   8148.0  ...   7.33  11.7  232.8  137.2      3.0\n",
       " 31    1.0    5.0     8.0  62.5     57.0  ...   7.13  11.4   57.0  122.4      8.0\n",
       " 17   50.0  812.0  1317.0  61.7  10314.0  ...   7.88  12.7  206.3  140.7      4.0\n",
       " 100  48.0  728.0  1141.0  63.8   8948.0  ...   8.32  12.3  186.4  147.0      5.0\n",
       " 9    27.0  421.0   682.0  61.7   5018.0  ...   7.48  11.9  185.9  141.4      3.0\n",
       " ..    ...    ...     ...   ...      ...  ...    ...   ...    ...    ...      ...\n",
       " 14   40.0  619.0  1147.0  54.0   7639.0  ...   6.21  12.3  191.0  119.1      4.0\n",
       " 138  44.0  759.0  1166.0  65.1  10514.0  ...   9.65  13.9  239.0  162.9      5.0\n",
       " 4    30.0  408.0   637.0  64.1   4265.0  ...   5.88  10.5  142.2  123.9      4.0\n",
       " 103  33.0  474.0   684.0  69.3   7442.0  ...  12.70  15.7  225.5  199.4      3.0\n",
       " 70   38.0  758.0  1205.0  62.9   8863.0  ...   7.48  11.7  233.2  137.0      3.0\n",
       " \n",
       " [81 rows x 15 columns],\n",
       " '_i126': \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       " '_i127': \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       " '_i128': 'X_train.shape[1]',\n",
       " '_128': 15,\n",
       " '_i129': \"model = Sequential()\\nmodel.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       " '_129': <keras.src.callbacks.history.History at 0x313944250>,\n",
       " '_i130': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\\nfrom keras.models import Sequential',\n",
       " 'Input': <function keras.src.layers.core.input_layer.Input(shape=None, batch_size=None, dtype=None, sparse=None, batch_shape=None, name=None, tensor=None, optional=False)>,\n",
       " '_i131': \"model = Sequential()\\nmodel.add(Input(shape=X_train.shape[1]))\\nmodel.add(Dense(units=32activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       " '_i132': \"model = Sequential()\\nmodel.add(Input(shape=X_train.shape[1]))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       " '_i133': \"model = Sequential()\\nmodel.add(Input(shape=(,X_train.shape[1])))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       " '_i134': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       " '_i135': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\n\\nroc_auc_score()\",\n",
       " '_i136': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_136': 0.6714285714285714,\n",
       " '_i137': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_137': 0.5571428571428572,\n",
       " '_i138': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_138': 0.5142857142857143,\n",
       " '_i139': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_139': 0.6178571428571428,\n",
       " '_i140': \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_preds)\",\n",
       " '_i141': \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_preds, y_preds)\",\n",
       " '_141': 1.0,\n",
       " '_i142': \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_142': 0.5,\n",
       " '_i143': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\",\n",
       " '_i144': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_144': 0.55,\n",
       " '_i145': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\ny_probs(y_test, y_probs)\",\n",
       " '_i146': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_146': 0.7407407407407407,\n",
       " '_i147': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='roc-auc')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i148': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='roc_auc')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i149': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i150': \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='SparseCategoricalCrossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\\ny_preds_NN = model.predict(X_test)\\ny_preds_NN = (y_preds_NN >= .5).astype(int)\",\n",
       " '_i151': \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\n\\n\\nmodel.fit(X_train, y_train)\\ny_preds_NN = model.predict(X_test)\\ny_preds_NN = (y_preds_NN >= .5).astype(int)\",\n",
       " 'y_preds_NN': array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]),\n",
       " '_i152': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i153': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i154': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_154': 0.7407407407407407,\n",
       " '_i155': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_155': 0.42142857142857143,\n",
       " '_i156': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_156': 0.7407407407407407,\n",
       " '_i157': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_157': 0.7407407407407407,\n",
       " '_i158': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_158': 0.49999999999999994,\n",
       " '_i159': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_159': 0.7407407407407407,\n",
       " '_i160': \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i161': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i162': \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_162': 0.8148148148148148,\n",
       " '_i163': \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_163': 0.8428571428571429,\n",
       " '_i164': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\\nfrom keras.models import Sequential',\n",
       " 'StratifiedKFold': sklearn.model_selection._split.StratifiedKFold,\n",
       " '_i165': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        \",\n",
       " 'FranchiseQB': __main__.FranchiseQB,\n",
       " '_i166': 'NFL = FranchiseQB()',\n",
       " 'NFL': <__main__.FranchiseQB at 0x322dc2450>,\n",
       " '_i167': 'NFL.run()',\n",
       " '_i168': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n        model = model(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       " '_i169': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n        model = model(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       " '_i170': 'NFL = FranchiseQB()',\n",
       " '_i171': 'NFL.run()',\n",
       " '_i172': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       " '_i173': 'NFL = FranchiseQB()',\n",
       " '_i174': 'NFL.run()',\n",
       " '_i175': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       " '_i176': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       " '_i177': 'NFL = FranchiseQB()',\n",
       " '_i178': 'NFL.run()',\n",
       " '_i179': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       " '_i180': 'NFL = FranchiseQB()',\n",
       " '_i181': 'NFL.run()',\n",
       " '_i182': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        self.meric_dict = metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        self.model_results = pd.DataFrame(self.metric_dict)\",\n",
       " '_i183': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        self.meric_dict = metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        self.model_results = pd.DataFrame(self.metric_dict)\",\n",
       " '_i184': 'NFL = FranchiseQB()',\n",
       " '_i185': 'NFL.run()',\n",
       " '_i186': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       " '_i187': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       " '_i188': 'NFL = FranchiseQB()',\n",
       " '_i189': 'NFL.run()',\n",
       " '_i190': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       " '_i191': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       " '_i192': 'NFL = FranchiseQB()',\n",
       " '_i193': 'NFL.run()',\n",
       " '_i194': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       " '_i195': 'NFL = FranchiseQB()',\n",
       " '_i196': 'NFL.run()',\n",
       " '_i197': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i198': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i199': 'NFL = FranchiseQB()',\n",
       " '_i200': 'NFL.run()',\n",
       " '_i201': 'NFL.model_results',\n",
       " '_201':    accuracy        f1   roc_auc\n",
       " 0  0.685185  0.105263  0.427324,\n",
       " '_i202': \"NFL = FranchiseQB(model='lr')\",\n",
       " '_i203': \"NFL = FranchiseQB(model='svc')\",\n",
       " '_i204': 'NFL.run()',\n",
       " '_i205': 'NFL.model_results',\n",
       " '_205':    accuracy   f1   roc_auc\n",
       " 0  0.685185  0.0  0.390223,\n",
       " '_i206': \"NFL = FranchiseQB(model='svm')\",\n",
       " '_i207': 'NFL.run()',\n",
       " '_i208': 'NFL.model_results',\n",
       " '_i209': \"NFL = FranchiseQB(model='xgb')\",\n",
       " '_i210': 'NFL.run()',\n",
       " '_i211': 'NFL.model_results',\n",
       " '_211':    accuracy   f1   roc_auc\n",
       " 0  0.583333  0.0  0.326931,\n",
       " '_i212': \"NFL = FranchiseQB(model='rf')\",\n",
       " '_i213': 'NFL.run()',\n",
       " '_i214': 'NFL.model_results',\n",
       " '_214':    accuracy   f1   roc_auc\n",
       " 0  0.666667  0.1  0.404409,\n",
       " '_i215': 'df',\n",
       " '_215':      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       " 0       Sam Bradford      1  ...           LA                      4.0\n",
       " 1          Tim Tebow      1  ...          DEN                      2.0\n",
       " 2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       " 3         Colt McCoy      3  ...          CLE                      3.0\n",
       " 4         Mike Kafka      4  ...          PHI                      1.0\n",
       " ..               ...    ...  ...          ...                      ...\n",
       " 142    DeShone Kizer      2  ...          CLE                      1.0\n",
       " 143       Davis Webb      3  ...          BUF                      1.0\n",
       " 145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       " 146  Nathan Peterman      5  ...          BUF                      2.0\n",
       " 148       Chad Kelly      7  ...          DEN                      1.0\n",
       " \n",
       " [108 rows x 22 columns],\n",
       " '_i216': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        if self.model == 'catboost':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=X.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i217': \"NFL = FranchiseQB(model='catboost')\",\n",
       " '_i218': 'NFL.run()',\n",
       " '_i219': 'NFL.model_results',\n",
       " '_219':    accuracy        f1   roc_auc\n",
       " 0  0.694444  0.108108  0.428634,\n",
       " '_i220': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i221': \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_221': 0.7928571428571428,\n",
       " '_i222': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        if self.model == 'catboost':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=X.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i223': \"NFL = FranchiseQB(model='catboost')\",\n",
       " '_i224': 'NFL.run()',\n",
       " '_i225': 'NFL.model_results',\n",
       " '_225':    accuracy        f1   roc_auc\n",
       " 0  0.694444  0.108108  0.428634,\n",
       " '_i226': 'X_train',\n",
       " '_226':      pfr_player_name  round  pick  ...  seasons             name  recent_team\n",
       " 102       Joe Burrow      1     1  ...      7.0       Joe Burrow          CIN\n",
       " 97        Kyle Trask      2    64  ...      5.0       Kyle Trask           TB\n",
       " 105      Jordan Love      1    26  ...      3.0      Jordan Love           GB\n",
       " 146  Nathan Peterman      5   171  ...      7.0  Nathan Peterman          BUF\n",
       " 119       Will Grier      3   100  ...      7.0       Will Grier          CAR\n",
       " ..               ...    ...   ...  ...      ...              ...          ...\n",
       " 61    Jameis Winston      1     1  ...      2.0   Jameis Winston           TB\n",
       " 31        Nick Foles      3    88  ...      8.0       Nick Foles          PHI\n",
       " 13        Cam Newton      1     1  ...      7.0       Cam Newton          CAR\n",
       " 62    Marcus Mariota      1     2  ...      3.0   Marcus Mariota          TEN\n",
       " 112      Ben DiNucci      7   231  ...      2.0      Ben DiNucci          DAL\n",
       " \n",
       " [81 rows x 21 columns],\n",
       " '_i227': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i228': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i229': 'NFL.run()',\n",
       " '_i230': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat'\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i231': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i232': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i233': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i234': 'NFL.run()',\n",
       " '_i235': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i236': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i237': 'NFL.run()',\n",
       " '_i238': 'NFL.model_results',\n",
       " '_238':    accuracy        f1  roc_auc\n",
       " 0  0.703704  0.428571   0.7189,\n",
       " '_i239': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=3):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i240': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i241': 'NFL.run()',\n",
       " '_i242': 'NFL.model_results',\n",
       " '_242':    accuracy        f1  roc_auc\n",
       " 0  0.657407  0.350877  0.68529,\n",
       " '_i243': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=1):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i244': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i245': 'NFL.run()',\n",
       " '_i246': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i247': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i248': 'NFL.run()',\n",
       " '_i249': 'NFL.model_results',\n",
       " '_249':    accuracy        f1   roc_auc\n",
       " 0   0.62037  0.349206  0.666085,\n",
       " '_i250': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(X.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       X['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i251': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i252': 'NFL.run()',\n",
       " '_i253': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i254': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i255': 'NFL.run()',\n",
       " '_i256': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend((model.predict_proba(X_test)[:,1]))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i257': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i258': 'NFL.run()',\n",
       " '_i259': 'NFL.model_results',\n",
       " '_259':    accuracy        f1   roc_auc\n",
       " 0  0.851852  0.666667  0.921429,\n",
       " '_i260': 'import pydantic',\n",
       " 'pydantic': <module 'pydantic' from '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/__init__.py'>,\n",
       " '_i261': 'from pydantic import BaseModel',\n",
       " 'BaseModel': pydantic.main.BaseModel,\n",
       " '_i262': 'from pydantic import BaseModel\\n\\nclass MyFirstModel(BaseModel):\\n    first_name: str\\n    last_name: str\\n\\nvalidating = MyFirstModel(first_name=\"marc\", last_name=\"nealer\")',\n",
       " 'MyFirstModel': __main__.MyFirstModel,\n",
       " 'validating': MyFirstModel(first_name='marc', last_name='nealer'),\n",
       " '_i263': \"MyFirstModel(first_name='Ben', last_name=0)\",\n",
       " '_i264': 'from Exception import RuntimeError',\n",
       " '_i265': 'Exception',\n",
       " '_265': Exception,\n",
       " '_i266': 'ValueError',\n",
       " '_266': ValueError,\n",
       " '_i267': 'compile',\n",
       " '_267': <function compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1, *, _feature_version=-1)>,\n",
       " '_i268': 'globals()',\n",
       " '_268': {...},\n",
       " '_i269': 'globals().__len__()',\n",
       " '_269': 427,\n",
       " '_i270': 'globals()[9]',\n",
       " '_i271': 'locals()',\n",
       " '_271': {...},\n",
       " '_i272': 'vars()'}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for MyFirstModel\nlast_name\n  Input should be a valid string [type=string_type, input_value=0, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[263], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mMyFirstModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for MyFirstModel\nlast_name\n  Input should be a valid string [type=string_type, input_value=0, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type"
     ]
    }
   ],
   "source": [
    "MyFirstModel(first_name='Ben', last_name=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
