{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mfl as mfl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mfl.api.data_loaders as mfldata\n",
    "import nfl_data_py as nfl\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FranchiseQB:\n",
    "    def __init__(self, \n",
    "                 feature_set='numeric', \n",
    "                 model='catboost', \n",
    "                 dataset='../data/for_modeling.csv', \n",
    "                 **kwargs):\n",
    "        \n",
    "        self.feature_set = feature_set\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.kwargs = kwargs\n",
    "        self.model_map = {\n",
    "            'catboost' : CatBoostClassifier(),\n",
    "            'xgb' : XGBClassifier(), \n",
    "            'rf' : RandomForestClassifier(),\n",
    "            'lr' : LogisticRegression(),\n",
    "            'hgb' : HistGradientBoostingClassifier(),\n",
    "            'svm' : SVC(),\n",
    "            'nn_basic': ...\n",
    "        }\n",
    "        self.available_models = list(self.model_map.keys())\n",
    "        self.model_func = self.model_map[model]\n",
    "        self.full_dataset = pd.read_csv(dataset)\n",
    "\n",
    "    def create_training_data(self, n_splits=4, stratify=True):\n",
    "        pass\n",
    "\n",
    "    def map_response(self, x):\n",
    "        if x >= 4:\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "        \n",
    "    def score(self, y_test, y_probs, y_preds):\n",
    "        accuracy = accuracy_score(y_test, y_preds)\n",
    "        f1 = f1_score(y_test, y_preds)\n",
    "        roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "        metric_dict = {\n",
    "            'accuracy' : accuracy,\n",
    "            'f1' : f1,\n",
    "            'roc_auc': roc_auc\n",
    "        }\n",
    "\n",
    "        return metric_dict\n",
    "\n",
    "    def run(self, kfold=False, folds=2):\n",
    "        df = self.full_dataset.dropna()\n",
    "        self.all_players = df['pfr_player_name']\n",
    "        model = self.model_func\n",
    "\n",
    "        if self.feature_set == 'numeric':\n",
    "            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\n",
    "        elif self.feature_set == 'cat':\n",
    "            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\n",
    "            X = df.drop('seasons_with_draft_team', axis=1)\n",
    "        \n",
    "        y = df['seasons_with_draft_team'].apply(self.map_response)\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\n",
    "        folds = k_strat.split(X, y)\n",
    "        y_preds = []\n",
    "        y_probs = []\n",
    "        y_tests = []\n",
    "        \n",
    "        if kfold:\n",
    "            for train, test in folds:\n",
    "                \n",
    "                X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "                y_train, y_test = y.values[train], y.values[test]    \n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                y_tests.extend(y_test)\n",
    "                y_preds.extend(model.predict(X_test))\n",
    "                y_probs.extend(model.predict_proba(X_test)[:,1])\n",
    "        \n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\n",
    "                                                                       axis=1),\n",
    "                                                                       df['seasons_with_draft_team'].apply(self.map_response),\n",
    "                                                                       test_size=.25,\n",
    "                                                                       shuffle=True,\n",
    "                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\n",
    "            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_tests.extend(y_test)\n",
    "            y_preds.extend(model.predict(X_test))\n",
    "            y_probs.extend((model.predict_proba(X_test)[:,1]))\n",
    "        \n",
    "        self.y_preds = y_preds\n",
    "        self.y_probs = y_probs\n",
    "\n",
    "        metrics = self.score(y_tests, y_probs, y_preds)\n",
    "        self.model_results = pd.DataFrame(metrics, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFL = FranchiseQB(model='catboost', feature_set='cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pfr_player_name</th>\n",
       "      <th>round</th>\n",
       "      <th>pick</th>\n",
       "      <th>season</th>\n",
       "      <th>G</th>\n",
       "      <th>Cmp</th>\n",
       "      <th>Att</th>\n",
       "      <th>Cmp%</th>\n",
       "      <th>Yds</th>\n",
       "      <th>TD</th>\n",
       "      <th>...</th>\n",
       "      <th>Int%</th>\n",
       "      <th>Y/A</th>\n",
       "      <th>AY/A</th>\n",
       "      <th>Y/C</th>\n",
       "      <th>Y/G</th>\n",
       "      <th>Rate</th>\n",
       "      <th>seasons</th>\n",
       "      <th>name</th>\n",
       "      <th>recent_team</th>\n",
       "      <th>seasons_with_draft_team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sam Bradford</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>31.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>67.6</td>\n",
       "      <td>8403.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>10.57</td>\n",
       "      <td>13.9</td>\n",
       "      <td>271.1</td>\n",
       "      <td>175.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sam Bradford</td>\n",
       "      <td>LA</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tim Tebow</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2010</td>\n",
       "      <td>55.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>995.0</td>\n",
       "      <td>66.4</td>\n",
       "      <td>9285.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6</td>\n",
       "      <td>9.3</td>\n",
       "      <td>10.38</td>\n",
       "      <td>14.0</td>\n",
       "      <td>168.8</td>\n",
       "      <td>170.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Tim Tebow</td>\n",
       "      <td>DEN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jimmy Clausen</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>2010</td>\n",
       "      <td>35.0</td>\n",
       "      <td>695.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>62.6</td>\n",
       "      <td>8148.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.33</td>\n",
       "      <td>11.7</td>\n",
       "      <td>232.8</td>\n",
       "      <td>137.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Jimmy Clausen</td>\n",
       "      <td>CAR</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Colt McCoy</td>\n",
       "      <td>3</td>\n",
       "      <td>85</td>\n",
       "      <td>2010</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>1645.0</td>\n",
       "      <td>70.3</td>\n",
       "      <td>13253.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>8.19</td>\n",
       "      <td>11.5</td>\n",
       "      <td>250.1</td>\n",
       "      <td>155.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Colt McCoy</td>\n",
       "      <td>CLE</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mike Kafka</td>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>2010</td>\n",
       "      <td>30.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>64.1</td>\n",
       "      <td>4265.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.88</td>\n",
       "      <td>10.5</td>\n",
       "      <td>142.2</td>\n",
       "      <td>123.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Mike Kafka</td>\n",
       "      <td>PHI</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>C.J. Beathard</td>\n",
       "      <td>3</td>\n",
       "      <td>104</td>\n",
       "      <td>2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Joshua Dobbs</td>\n",
       "      <td>4</td>\n",
       "      <td>135</td>\n",
       "      <td>2017</td>\n",
       "      <td>37.0</td>\n",
       "      <td>614.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>61.5</td>\n",
       "      <td>7138.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>6.90</td>\n",
       "      <td>11.6</td>\n",
       "      <td>192.9</td>\n",
       "      <td>133.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Joshua Dobbs</td>\n",
       "      <td>PIT</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Nathan Peterman</td>\n",
       "      <td>5</td>\n",
       "      <td>171</td>\n",
       "      <td>2017</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>46.5</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.7</td>\n",
       "      <td>9.4</td>\n",
       "      <td>55.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Nathan Peterman</td>\n",
       "      <td>BUF</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Brad Kaaya</td>\n",
       "      <td>6</td>\n",
       "      <td>215</td>\n",
       "      <td>2017</td>\n",
       "      <td>38.0</td>\n",
       "      <td>721.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>60.6</td>\n",
       "      <td>9972.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8.64</td>\n",
       "      <td>13.8</td>\n",
       "      <td>262.4</td>\n",
       "      <td>146.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Brad Kaaya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Chad Kelly</td>\n",
       "      <td>7</td>\n",
       "      <td>253</td>\n",
       "      <td>2017</td>\n",
       "      <td>22.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.7</td>\n",
       "      <td>8.72</td>\n",
       "      <td>13.5</td>\n",
       "      <td>309.1</td>\n",
       "      <td>152.3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Chad Kelly</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pfr_player_name  round  pick  season     G     Cmp     Att  Cmp%  \\\n",
       "0       Sam Bradford      1     1    2010  31.0   604.0   893.0  67.6   \n",
       "1          Tim Tebow      1    25    2010  55.0   661.0   995.0  66.4   \n",
       "2      Jimmy Clausen      2    48    2010  35.0   695.0  1110.0  62.6   \n",
       "3         Colt McCoy      3    85    2010  53.0  1157.0  1645.0  70.3   \n",
       "4         Mike Kafka      4   122    2010  30.0   408.0   637.0  64.1   \n",
       "..               ...    ...   ...     ...   ...     ...     ...   ...   \n",
       "144    C.J. Beathard      3   104    2017   NaN     NaN     NaN   NaN   \n",
       "145     Joshua Dobbs      4   135    2017  37.0   614.0   999.0  61.5   \n",
       "146  Nathan Peterman      5   171    2017  10.0    20.0    43.0  46.5   \n",
       "147       Brad Kaaya      6   215    2017  38.0   721.0  1189.0  60.6   \n",
       "148       Chad Kelly      7   253    2017  22.0   503.0   786.0  64.0   \n",
       "\n",
       "         Yds     TD  ...  Int%  Y/A   AY/A   Y/C    Y/G   Rate  seasons  \\\n",
       "0     8403.0   88.0  ...   1.8  9.4  10.57  13.9  271.1  175.6      3.0   \n",
       "1     9285.0   88.0  ...   1.6  9.3  10.38  14.0  168.8  170.8      4.0   \n",
       "2     8148.0   60.0  ...   2.4  7.3   7.33  11.7  232.8  137.2      3.0   \n",
       "3    13253.0  112.0  ...   2.7  8.1   8.19  11.5  250.1  155.0      4.0   \n",
       "4     4265.0   19.0  ...   3.1  6.7   5.88  10.5  142.2  123.9      4.0   \n",
       "..       ...    ...  ...   ...  ...    ...   ...    ...    ...      ...   \n",
       "144      NaN    NaN  ...   NaN  NaN    NaN   NaN    NaN    NaN      NaN   \n",
       "145   7138.0   53.0  ...   2.9  7.1   6.90  11.6  192.9  133.2      4.0   \n",
       "146     94.0    0.0  ...   4.7  2.2   0.09   4.7    9.4   55.6      7.0   \n",
       "147   9972.0   69.0  ...   2.0  8.4   8.64  13.8  262.4  146.2      3.0   \n",
       "148   6800.0   50.0  ...   2.7  8.7   8.72  13.5  309.1  152.3      7.0   \n",
       "\n",
       "                name  recent_team seasons_with_draft_team  \n",
       "0       Sam Bradford           LA                     4.0  \n",
       "1          Tim Tebow          DEN                     2.0  \n",
       "2      Jimmy Clausen          CAR                     1.0  \n",
       "3         Colt McCoy          CLE                     3.0  \n",
       "4         Mike Kafka          PHI                     1.0  \n",
       "..               ...          ...                     ...  \n",
       "144              NaN          NaN                     NaN  \n",
       "145     Joshua Dobbs          PIT                     2.0  \n",
       "146  Nathan Peterman          BUF                     2.0  \n",
       "147       Brad Kaaya          NaN                     NaN  \n",
       "148       Chad Kelly          DEN                     1.0  \n",
       "\n",
       "[149 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NFL.full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.010625\n",
      "0:\tlearn: 0.6852129\ttotal: 60.9ms\tremaining: 18.2s\n",
      "1:\tlearn: 0.6775699\ttotal: 62.4ms\tremaining: 9.3s\n",
      "2:\tlearn: 0.6707844\ttotal: 64.5ms\tremaining: 6.39s\n",
      "3:\tlearn: 0.6628242\ttotal: 65.8ms\tremaining: 4.87s\n",
      "4:\tlearn: 0.6586915\ttotal: 67.8ms\tremaining: 4s\n",
      "5:\tlearn: 0.6521197\ttotal: 69.9ms\tremaining: 3.42s\n",
      "6:\tlearn: 0.6454259\ttotal: 71.1ms\tremaining: 2.98s\n",
      "7:\tlearn: 0.6401258\ttotal: 72.1ms\tremaining: 2.63s\n",
      "8:\tlearn: 0.6352136\ttotal: 73.3ms\tremaining: 2.37s\n",
      "9:\tlearn: 0.6282357\ttotal: 74.5ms\tremaining: 2.16s\n",
      "10:\tlearn: 0.6239145\ttotal: 75.4ms\tremaining: 1.98s\n",
      "11:\tlearn: 0.6194098\ttotal: 75.9ms\tremaining: 1.82s\n",
      "12:\tlearn: 0.6154147\ttotal: 76.5ms\tremaining: 1.69s\n",
      "13:\tlearn: 0.6099335\ttotal: 77.7ms\tremaining: 1.59s\n",
      "14:\tlearn: 0.6042443\ttotal: 78.9ms\tremaining: 1.5s\n",
      "15:\tlearn: 0.5979067\ttotal: 79.9ms\tremaining: 1.42s\n",
      "16:\tlearn: 0.5922089\ttotal: 80.9ms\tremaining: 1.35s\n",
      "17:\tlearn: 0.5870287\ttotal: 81.8ms\tremaining: 1.28s\n",
      "18:\tlearn: 0.5815895\ttotal: 82.6ms\tremaining: 1.22s\n",
      "19:\tlearn: 0.5772209\ttotal: 83.4ms\tremaining: 1.17s\n",
      "20:\tlearn: 0.5734577\ttotal: 84.2ms\tremaining: 1.12s\n",
      "21:\tlearn: 0.5681447\ttotal: 85.1ms\tremaining: 1.07s\n",
      "22:\tlearn: 0.5612580\ttotal: 86.6ms\tremaining: 1.04s\n",
      "23:\tlearn: 0.5565171\ttotal: 87.5ms\tremaining: 1.01s\n",
      "24:\tlearn: 0.5529483\ttotal: 88.4ms\tremaining: 972ms\n",
      "25:\tlearn: 0.5474920\ttotal: 89.3ms\tremaining: 941ms\n",
      "26:\tlearn: 0.5449934\ttotal: 90.2ms\tremaining: 912ms\n",
      "27:\tlearn: 0.5397018\ttotal: 91.3ms\tremaining: 887ms\n",
      "28:\tlearn: 0.5339462\ttotal: 92.4ms\tremaining: 863ms\n",
      "29:\tlearn: 0.5328347\ttotal: 92.7ms\tremaining: 834ms\n",
      "30:\tlearn: 0.5282740\ttotal: 93.6ms\tremaining: 812ms\n",
      "31:\tlearn: 0.5247046\ttotal: 94.4ms\tremaining: 791ms\n",
      "32:\tlearn: 0.5207727\ttotal: 95.3ms\tremaining: 771ms\n",
      "33:\tlearn: 0.5167016\ttotal: 95.8ms\tremaining: 750ms\n",
      "34:\tlearn: 0.5124685\ttotal: 96.6ms\tremaining: 732ms\n",
      "35:\tlearn: 0.5078874\ttotal: 97.3ms\tremaining: 714ms\n",
      "36:\tlearn: 0.5030360\ttotal: 98.1ms\tremaining: 697ms\n",
      "37:\tlearn: 0.5010005\ttotal: 98.5ms\tremaining: 679ms\n",
      "38:\tlearn: 0.4978862\ttotal: 99.9ms\tremaining: 668ms\n",
      "39:\tlearn: 0.4936634\ttotal: 101ms\tremaining: 654ms\n",
      "40:\tlearn: 0.4895817\ttotal: 101ms\tremaining: 640ms\n",
      "41:\tlearn: 0.4847478\ttotal: 102ms\tremaining: 627ms\n",
      "42:\tlearn: 0.4826953\ttotal: 103ms\tremaining: 614ms\n",
      "43:\tlearn: 0.4784359\ttotal: 104ms\tremaining: 602ms\n",
      "44:\tlearn: 0.4744074\ttotal: 104ms\tremaining: 591ms\n",
      "45:\tlearn: 0.4701132\ttotal: 105ms\tremaining: 580ms\n",
      "46:\tlearn: 0.4670180\ttotal: 106ms\tremaining: 571ms\n",
      "47:\tlearn: 0.4640474\ttotal: 107ms\tremaining: 560ms\n",
      "48:\tlearn: 0.4605014\ttotal: 108ms\tremaining: 554ms\n",
      "49:\tlearn: 0.4571705\ttotal: 109ms\tremaining: 545ms\n",
      "50:\tlearn: 0.4539639\ttotal: 110ms\tremaining: 535ms\n",
      "51:\tlearn: 0.4508636\ttotal: 110ms\tremaining: 527ms\n",
      "52:\tlearn: 0.4475152\ttotal: 111ms\tremaining: 518ms\n",
      "53:\tlearn: 0.4435729\ttotal: 112ms\tremaining: 510ms\n",
      "54:\tlearn: 0.4394994\ttotal: 113ms\tremaining: 501ms\n",
      "55:\tlearn: 0.4357934\ttotal: 113ms\tremaining: 494ms\n",
      "56:\tlearn: 0.4320338\ttotal: 114ms\tremaining: 486ms\n",
      "57:\tlearn: 0.4285896\ttotal: 115ms\tremaining: 478ms\n",
      "58:\tlearn: 0.4265055\ttotal: 115ms\tremaining: 471ms\n",
      "59:\tlearn: 0.4236146\ttotal: 116ms\tremaining: 464ms\n",
      "60:\tlearn: 0.4200178\ttotal: 117ms\tremaining: 457ms\n",
      "61:\tlearn: 0.4166400\ttotal: 117ms\tremaining: 450ms\n",
      "62:\tlearn: 0.4142666\ttotal: 118ms\tremaining: 444ms\n",
      "63:\tlearn: 0.4115368\ttotal: 119ms\tremaining: 438ms\n",
      "64:\tlearn: 0.4095780\ttotal: 120ms\tremaining: 433ms\n",
      "65:\tlearn: 0.4063057\ttotal: 120ms\tremaining: 427ms\n",
      "66:\tlearn: 0.4035314\ttotal: 121ms\tremaining: 422ms\n",
      "67:\tlearn: 0.4018263\ttotal: 122ms\tremaining: 416ms\n",
      "68:\tlearn: 0.3986538\ttotal: 123ms\tremaining: 410ms\n",
      "69:\tlearn: 0.3952129\ttotal: 123ms\tremaining: 405ms\n",
      "70:\tlearn: 0.3928223\ttotal: 124ms\tremaining: 400ms\n",
      "71:\tlearn: 0.3897533\ttotal: 125ms\tremaining: 395ms\n",
      "72:\tlearn: 0.3869911\ttotal: 125ms\tremaining: 390ms\n",
      "73:\tlearn: 0.3851620\ttotal: 126ms\tremaining: 385ms\n",
      "74:\tlearn: 0.3831536\ttotal: 127ms\tremaining: 381ms\n",
      "75:\tlearn: 0.3810990\ttotal: 128ms\tremaining: 376ms\n",
      "76:\tlearn: 0.3782301\ttotal: 128ms\tremaining: 371ms\n",
      "77:\tlearn: 0.3765860\ttotal: 129ms\tremaining: 367ms\n",
      "78:\tlearn: 0.3737041\ttotal: 130ms\tremaining: 363ms\n",
      "79:\tlearn: 0.3717324\ttotal: 130ms\tremaining: 359ms\n",
      "80:\tlearn: 0.3697424\ttotal: 131ms\tremaining: 354ms\n",
      "81:\tlearn: 0.3671961\ttotal: 132ms\tremaining: 350ms\n",
      "82:\tlearn: 0.3649472\ttotal: 132ms\tremaining: 346ms\n",
      "83:\tlearn: 0.3613996\ttotal: 133ms\tremaining: 342ms\n",
      "84:\tlearn: 0.3585493\ttotal: 134ms\tremaining: 338ms\n",
      "85:\tlearn: 0.3571278\ttotal: 134ms\tremaining: 334ms\n",
      "86:\tlearn: 0.3549038\ttotal: 135ms\tremaining: 330ms\n",
      "87:\tlearn: 0.3519606\ttotal: 136ms\tremaining: 327ms\n",
      "88:\tlearn: 0.3490309\ttotal: 136ms\tremaining: 323ms\n",
      "89:\tlearn: 0.3468736\ttotal: 137ms\tremaining: 320ms\n",
      "90:\tlearn: 0.3438070\ttotal: 138ms\tremaining: 316ms\n",
      "91:\tlearn: 0.3415245\ttotal: 138ms\tremaining: 313ms\n",
      "92:\tlearn: 0.3394707\ttotal: 139ms\tremaining: 309ms\n",
      "93:\tlearn: 0.3374091\ttotal: 140ms\tremaining: 308ms\n",
      "94:\tlearn: 0.3346044\ttotal: 141ms\tremaining: 304ms\n",
      "95:\tlearn: 0.3330305\ttotal: 142ms\tremaining: 301ms\n",
      "96:\tlearn: 0.3307224\ttotal: 142ms\tremaining: 298ms\n",
      "97:\tlearn: 0.3284235\ttotal: 143ms\tremaining: 295ms\n",
      "98:\tlearn: 0.3262762\ttotal: 144ms\tremaining: 292ms\n",
      "99:\tlearn: 0.3247432\ttotal: 145ms\tremaining: 289ms\n",
      "100:\tlearn: 0.3220269\ttotal: 145ms\tremaining: 286ms\n",
      "101:\tlearn: 0.3202033\ttotal: 146ms\tremaining: 283ms\n",
      "102:\tlearn: 0.3183692\ttotal: 147ms\tremaining: 281ms\n",
      "103:\tlearn: 0.3155968\ttotal: 148ms\tremaining: 279ms\n",
      "104:\tlearn: 0.3131830\ttotal: 149ms\tremaining: 276ms\n",
      "105:\tlearn: 0.3109495\ttotal: 149ms\tremaining: 273ms\n",
      "106:\tlearn: 0.3093617\ttotal: 150ms\tremaining: 271ms\n",
      "107:\tlearn: 0.3071652\ttotal: 151ms\tremaining: 268ms\n",
      "108:\tlearn: 0.3065078\ttotal: 152ms\tremaining: 266ms\n",
      "109:\tlearn: 0.3042277\ttotal: 152ms\tremaining: 263ms\n",
      "110:\tlearn: 0.3025427\ttotal: 154ms\tremaining: 262ms\n",
      "111:\tlearn: 0.3005915\ttotal: 154ms\tremaining: 259ms\n",
      "112:\tlearn: 0.2991349\ttotal: 155ms\tremaining: 257ms\n",
      "113:\tlearn: 0.2967500\ttotal: 156ms\tremaining: 254ms\n",
      "114:\tlearn: 0.2960848\ttotal: 156ms\tremaining: 251ms\n",
      "115:\tlearn: 0.2935405\ttotal: 157ms\tremaining: 250ms\n",
      "116:\tlearn: 0.2924557\ttotal: 158ms\tremaining: 248ms\n",
      "117:\tlearn: 0.2909870\ttotal: 159ms\tremaining: 245ms\n",
      "118:\tlearn: 0.2893737\ttotal: 160ms\tremaining: 243ms\n",
      "119:\tlearn: 0.2873170\ttotal: 160ms\tremaining: 241ms\n",
      "120:\tlearn: 0.2856616\ttotal: 161ms\tremaining: 238ms\n",
      "121:\tlearn: 0.2841130\ttotal: 162ms\tremaining: 236ms\n",
      "122:\tlearn: 0.2825976\ttotal: 163ms\tremaining: 234ms\n",
      "123:\tlearn: 0.2815259\ttotal: 163ms\tremaining: 231ms\n",
      "124:\tlearn: 0.2796512\ttotal: 164ms\tremaining: 229ms\n",
      "125:\tlearn: 0.2776673\ttotal: 164ms\tremaining: 227ms\n",
      "126:\tlearn: 0.2761645\ttotal: 165ms\tremaining: 225ms\n",
      "127:\tlearn: 0.2739920\ttotal: 166ms\tremaining: 223ms\n",
      "128:\tlearn: 0.2724222\ttotal: 166ms\tremaining: 221ms\n",
      "129:\tlearn: 0.2707469\ttotal: 167ms\tremaining: 219ms\n",
      "130:\tlearn: 0.2689728\ttotal: 168ms\tremaining: 217ms\n",
      "131:\tlearn: 0.2672466\ttotal: 169ms\tremaining: 215ms\n",
      "132:\tlearn: 0.2655209\ttotal: 170ms\tremaining: 213ms\n",
      "133:\tlearn: 0.2636280\ttotal: 171ms\tremaining: 211ms\n",
      "134:\tlearn: 0.2614258\ttotal: 171ms\tremaining: 209ms\n",
      "135:\tlearn: 0.2609400\ttotal: 172ms\tremaining: 207ms\n",
      "136:\tlearn: 0.2591781\ttotal: 172ms\tremaining: 205ms\n",
      "137:\tlearn: 0.2579704\ttotal: 173ms\tremaining: 203ms\n",
      "138:\tlearn: 0.2565690\ttotal: 174ms\tremaining: 201ms\n",
      "139:\tlearn: 0.2548773\ttotal: 174ms\tremaining: 199ms\n",
      "140:\tlearn: 0.2538787\ttotal: 175ms\tremaining: 197ms\n",
      "141:\tlearn: 0.2523724\ttotal: 176ms\tremaining: 196ms\n",
      "142:\tlearn: 0.2511521\ttotal: 176ms\tremaining: 194ms\n",
      "143:\tlearn: 0.2505298\ttotal: 177ms\tremaining: 192ms\n",
      "144:\tlearn: 0.2495994\ttotal: 178ms\tremaining: 190ms\n",
      "145:\tlearn: 0.2492455\ttotal: 178ms\tremaining: 188ms\n",
      "146:\tlearn: 0.2480185\ttotal: 179ms\tremaining: 186ms\n",
      "147:\tlearn: 0.2464651\ttotal: 180ms\tremaining: 185ms\n",
      "148:\tlearn: 0.2459173\ttotal: 180ms\tremaining: 183ms\n",
      "149:\tlearn: 0.2446080\ttotal: 181ms\tremaining: 181ms\n",
      "150:\tlearn: 0.2430202\ttotal: 182ms\tremaining: 179ms\n",
      "151:\tlearn: 0.2416528\ttotal: 182ms\tremaining: 178ms\n",
      "152:\tlearn: 0.2400896\ttotal: 183ms\tremaining: 176ms\n",
      "153:\tlearn: 0.2386576\ttotal: 184ms\tremaining: 174ms\n",
      "154:\tlearn: 0.2372514\ttotal: 185ms\tremaining: 173ms\n",
      "155:\tlearn: 0.2361041\ttotal: 185ms\tremaining: 171ms\n",
      "156:\tlearn: 0.2347943\ttotal: 186ms\tremaining: 169ms\n",
      "157:\tlearn: 0.2339571\ttotal: 187ms\tremaining: 168ms\n",
      "158:\tlearn: 0.2321934\ttotal: 187ms\tremaining: 166ms\n",
      "159:\tlearn: 0.2308757\ttotal: 188ms\tremaining: 165ms\n",
      "160:\tlearn: 0.2297860\ttotal: 189ms\tremaining: 163ms\n",
      "161:\tlearn: 0.2283719\ttotal: 190ms\tremaining: 162ms\n",
      "162:\tlearn: 0.2270632\ttotal: 190ms\tremaining: 160ms\n",
      "163:\tlearn: 0.2259522\ttotal: 191ms\tremaining: 159ms\n",
      "164:\tlearn: 0.2247999\ttotal: 192ms\tremaining: 157ms\n",
      "165:\tlearn: 0.2234065\ttotal: 193ms\tremaining: 155ms\n",
      "166:\tlearn: 0.2219139\ttotal: 193ms\tremaining: 154ms\n",
      "167:\tlearn: 0.2203460\ttotal: 194ms\tremaining: 152ms\n",
      "168:\tlearn: 0.2190989\ttotal: 195ms\tremaining: 151ms\n",
      "169:\tlearn: 0.2177756\ttotal: 195ms\tremaining: 149ms\n",
      "170:\tlearn: 0.2165547\ttotal: 196ms\tremaining: 148ms\n",
      "171:\tlearn: 0.2161599\ttotal: 197ms\tremaining: 147ms\n",
      "172:\tlearn: 0.2154968\ttotal: 198ms\tremaining: 145ms\n",
      "173:\tlearn: 0.2143610\ttotal: 199ms\tremaining: 144ms\n",
      "174:\tlearn: 0.2131530\ttotal: 199ms\tremaining: 142ms\n",
      "175:\tlearn: 0.2116314\ttotal: 200ms\tremaining: 141ms\n",
      "176:\tlearn: 0.2103585\ttotal: 201ms\tremaining: 139ms\n",
      "177:\tlearn: 0.2088010\ttotal: 201ms\tremaining: 138ms\n",
      "178:\tlearn: 0.2076326\ttotal: 203ms\tremaining: 137ms\n",
      "179:\tlearn: 0.2063373\ttotal: 204ms\tremaining: 136ms\n",
      "180:\tlearn: 0.2052967\ttotal: 205ms\tremaining: 135ms\n",
      "181:\tlearn: 0.2040239\ttotal: 205ms\tremaining: 133ms\n",
      "182:\tlearn: 0.2025681\ttotal: 206ms\tremaining: 132ms\n",
      "183:\tlearn: 0.2014780\ttotal: 208ms\tremaining: 131ms\n",
      "184:\tlearn: 0.2006595\ttotal: 208ms\tremaining: 130ms\n",
      "185:\tlearn: 0.1995995\ttotal: 210ms\tremaining: 128ms\n",
      "186:\tlearn: 0.1988188\ttotal: 211ms\tremaining: 127ms\n",
      "187:\tlearn: 0.1976094\ttotal: 212ms\tremaining: 126ms\n",
      "188:\tlearn: 0.1964087\ttotal: 213ms\tremaining: 125ms\n",
      "189:\tlearn: 0.1957069\ttotal: 213ms\tremaining: 123ms\n",
      "190:\tlearn: 0.1947645\ttotal: 214ms\tremaining: 122ms\n",
      "191:\tlearn: 0.1938033\ttotal: 216ms\tremaining: 121ms\n",
      "192:\tlearn: 0.1930228\ttotal: 216ms\tremaining: 120ms\n",
      "193:\tlearn: 0.1918844\ttotal: 217ms\tremaining: 119ms\n",
      "194:\tlearn: 0.1914199\ttotal: 218ms\tremaining: 117ms\n",
      "195:\tlearn: 0.1906849\ttotal: 219ms\tremaining: 116ms\n",
      "196:\tlearn: 0.1899441\ttotal: 220ms\tremaining: 115ms\n",
      "197:\tlearn: 0.1887767\ttotal: 221ms\tremaining: 114ms\n",
      "198:\tlearn: 0.1876327\ttotal: 221ms\tremaining: 112ms\n",
      "199:\tlearn: 0.1869851\ttotal: 222ms\tremaining: 111ms\n",
      "200:\tlearn: 0.1864449\ttotal: 223ms\tremaining: 110ms\n",
      "201:\tlearn: 0.1852464\ttotal: 224ms\tremaining: 108ms\n",
      "202:\tlearn: 0.1840188\ttotal: 224ms\tremaining: 107ms\n",
      "203:\tlearn: 0.1833862\ttotal: 225ms\tremaining: 106ms\n",
      "204:\tlearn: 0.1826895\ttotal: 226ms\tremaining: 105ms\n",
      "205:\tlearn: 0.1820751\ttotal: 226ms\tremaining: 103ms\n",
      "206:\tlearn: 0.1813689\ttotal: 227ms\tremaining: 102ms\n",
      "207:\tlearn: 0.1811789\ttotal: 227ms\tremaining: 101ms\n",
      "208:\tlearn: 0.1808745\ttotal: 228ms\tremaining: 99.3ms\n",
      "209:\tlearn: 0.1799057\ttotal: 229ms\tremaining: 98ms\n",
      "210:\tlearn: 0.1790226\ttotal: 229ms\tremaining: 96.7ms\n",
      "211:\tlearn: 0.1783591\ttotal: 230ms\tremaining: 95.5ms\n",
      "212:\tlearn: 0.1775277\ttotal: 231ms\tremaining: 94.3ms\n",
      "213:\tlearn: 0.1764357\ttotal: 232ms\tremaining: 93ms\n",
      "214:\tlearn: 0.1755010\ttotal: 232ms\tremaining: 91.8ms\n",
      "215:\tlearn: 0.1743628\ttotal: 233ms\tremaining: 90.6ms\n",
      "216:\tlearn: 0.1738172\ttotal: 234ms\tremaining: 89.3ms\n",
      "217:\tlearn: 0.1731290\ttotal: 234ms\tremaining: 88.2ms\n",
      "218:\tlearn: 0.1724432\ttotal: 235ms\tremaining: 87ms\n",
      "219:\tlearn: 0.1714670\ttotal: 236ms\tremaining: 85.7ms\n",
      "220:\tlearn: 0.1708124\ttotal: 236ms\tremaining: 84.5ms\n",
      "221:\tlearn: 0.1701833\ttotal: 238ms\tremaining: 83.6ms\n",
      "222:\tlearn: 0.1695497\ttotal: 239ms\tremaining: 82.4ms\n",
      "223:\tlearn: 0.1689868\ttotal: 239ms\tremaining: 81.1ms\n",
      "224:\tlearn: 0.1681447\ttotal: 240ms\tremaining: 79.9ms\n",
      "225:\tlearn: 0.1673033\ttotal: 241ms\tremaining: 78.8ms\n",
      "226:\tlearn: 0.1664972\ttotal: 241ms\tremaining: 77.6ms\n",
      "227:\tlearn: 0.1655299\ttotal: 242ms\tremaining: 76.4ms\n",
      "228:\tlearn: 0.1646706\ttotal: 243ms\tremaining: 75.2ms\n",
      "229:\tlearn: 0.1640254\ttotal: 243ms\tremaining: 74.1ms\n",
      "230:\tlearn: 0.1634036\ttotal: 244ms\tremaining: 72.9ms\n",
      "231:\tlearn: 0.1629005\ttotal: 245ms\tremaining: 71.7ms\n",
      "232:\tlearn: 0.1623489\ttotal: 245ms\tremaining: 70.6ms\n",
      "233:\tlearn: 0.1616988\ttotal: 246ms\tremaining: 69.4ms\n",
      "234:\tlearn: 0.1607314\ttotal: 247ms\tremaining: 68.2ms\n",
      "235:\tlearn: 0.1598017\ttotal: 247ms\tremaining: 67ms\n",
      "236:\tlearn: 0.1593978\ttotal: 248ms\tremaining: 65.9ms\n",
      "237:\tlearn: 0.1584885\ttotal: 249ms\tremaining: 64.7ms\n",
      "238:\tlearn: 0.1576806\ttotal: 249ms\tremaining: 63.6ms\n",
      "239:\tlearn: 0.1576677\ttotal: 249ms\tremaining: 62.3ms\n",
      "240:\tlearn: 0.1568757\ttotal: 250ms\tremaining: 61.2ms\n",
      "241:\tlearn: 0.1562333\ttotal: 251ms\tremaining: 60.1ms\n",
      "242:\tlearn: 0.1557990\ttotal: 252ms\tremaining: 59ms\n",
      "243:\tlearn: 0.1551743\ttotal: 252ms\tremaining: 57.9ms\n",
      "244:\tlearn: 0.1547226\ttotal: 254ms\tremaining: 57ms\n",
      "245:\tlearn: 0.1535189\ttotal: 255ms\tremaining: 55.9ms\n",
      "246:\tlearn: 0.1528574\ttotal: 255ms\tremaining: 54.8ms\n",
      "247:\tlearn: 0.1519661\ttotal: 256ms\tremaining: 53.7ms\n",
      "248:\tlearn: 0.1512729\ttotal: 257ms\tremaining: 52.6ms\n",
      "249:\tlearn: 0.1505194\ttotal: 257ms\tremaining: 51.5ms\n",
      "250:\tlearn: 0.1497824\ttotal: 258ms\tremaining: 50.4ms\n",
      "251:\tlearn: 0.1494411\ttotal: 259ms\tremaining: 49.3ms\n",
      "252:\tlearn: 0.1488225\ttotal: 259ms\tremaining: 48.2ms\n",
      "253:\tlearn: 0.1482942\ttotal: 260ms\tremaining: 47.1ms\n",
      "254:\tlearn: 0.1477273\ttotal: 261ms\tremaining: 46ms\n",
      "255:\tlearn: 0.1475167\ttotal: 262ms\tremaining: 45ms\n",
      "256:\tlearn: 0.1467434\ttotal: 262ms\tremaining: 43.9ms\n",
      "257:\tlearn: 0.1458800\ttotal: 263ms\tremaining: 42.8ms\n",
      "258:\tlearn: 0.1453954\ttotal: 264ms\tremaining: 41.8ms\n",
      "259:\tlearn: 0.1446846\ttotal: 264ms\tremaining: 40.7ms\n",
      "260:\tlearn: 0.1440251\ttotal: 265ms\tremaining: 39.6ms\n",
      "261:\tlearn: 0.1434507\ttotal: 266ms\tremaining: 38.5ms\n",
      "262:\tlearn: 0.1430232\ttotal: 266ms\tremaining: 37.5ms\n",
      "263:\tlearn: 0.1423145\ttotal: 268ms\tremaining: 36.5ms\n",
      "264:\tlearn: 0.1419163\ttotal: 269ms\tremaining: 35.5ms\n",
      "265:\tlearn: 0.1414291\ttotal: 269ms\tremaining: 34.4ms\n",
      "266:\tlearn: 0.1409492\ttotal: 270ms\tremaining: 33.4ms\n",
      "267:\tlearn: 0.1402795\ttotal: 271ms\tremaining: 32.3ms\n",
      "268:\tlearn: 0.1396154\ttotal: 271ms\tremaining: 31.3ms\n",
      "269:\tlearn: 0.1389521\ttotal: 272ms\tremaining: 30.3ms\n",
      "270:\tlearn: 0.1385790\ttotal: 273ms\tremaining: 29.2ms\n",
      "271:\tlearn: 0.1380386\ttotal: 274ms\tremaining: 28.2ms\n",
      "272:\tlearn: 0.1375065\ttotal: 275ms\tremaining: 27.2ms\n",
      "273:\tlearn: 0.1368125\ttotal: 275ms\tremaining: 26.1ms\n",
      "274:\tlearn: 0.1365292\ttotal: 276ms\tremaining: 25.1ms\n",
      "275:\tlearn: 0.1355452\ttotal: 277ms\tremaining: 24ms\n",
      "276:\tlearn: 0.1350120\ttotal: 277ms\tremaining: 23ms\n",
      "277:\tlearn: 0.1345714\ttotal: 278ms\tremaining: 22ms\n",
      "278:\tlearn: 0.1342170\ttotal: 279ms\tremaining: 21ms\n",
      "279:\tlearn: 0.1337817\ttotal: 280ms\tremaining: 20ms\n",
      "280:\tlearn: 0.1335042\ttotal: 280ms\tremaining: 18.9ms\n",
      "281:\tlearn: 0.1328401\ttotal: 281ms\tremaining: 17.9ms\n",
      "282:\tlearn: 0.1325145\ttotal: 282ms\tremaining: 16.9ms\n",
      "283:\tlearn: 0.1320028\ttotal: 282ms\tremaining: 15.9ms\n",
      "284:\tlearn: 0.1314895\ttotal: 283ms\tremaining: 14.9ms\n",
      "285:\tlearn: 0.1310461\ttotal: 284ms\tremaining: 13.9ms\n",
      "286:\tlearn: 0.1306863\ttotal: 284ms\tremaining: 12.9ms\n",
      "287:\tlearn: 0.1302369\ttotal: 285ms\tremaining: 11.9ms\n",
      "288:\tlearn: 0.1296086\ttotal: 286ms\tremaining: 10.9ms\n",
      "289:\tlearn: 0.1292298\ttotal: 286ms\tremaining: 9.87ms\n",
      "290:\tlearn: 0.1284829\ttotal: 287ms\tremaining: 8.88ms\n",
      "291:\tlearn: 0.1281592\ttotal: 288ms\tremaining: 7.89ms\n",
      "292:\tlearn: 0.1278790\ttotal: 289ms\tremaining: 6.89ms\n",
      "293:\tlearn: 0.1272380\ttotal: 289ms\tremaining: 5.9ms\n",
      "294:\tlearn: 0.1267145\ttotal: 290ms\tremaining: 4.92ms\n",
      "295:\tlearn: 0.1263391\ttotal: 292ms\tremaining: 3.94ms\n",
      "296:\tlearn: 0.1258585\ttotal: 292ms\tremaining: 2.95ms\n",
      "297:\tlearn: 0.1254201\ttotal: 293ms\tremaining: 1.97ms\n",
      "298:\tlearn: 0.1247872\ttotal: 294ms\tremaining: 984us\n",
      "299:\tlearn: 0.1242381\ttotal: 295ms\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "NFL.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy        f1   roc_auc\n",
       "0  0.814815  0.545455  0.885714"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NFL.model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mfldata.load_qb_data_cleaned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics_only = df.select_dtypes(include='float').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>Cmp</th>\n",
       "      <th>Att</th>\n",
       "      <th>Cmp%</th>\n",
       "      <th>Yds</th>\n",
       "      <th>TD</th>\n",
       "      <th>TD%</th>\n",
       "      <th>Int</th>\n",
       "      <th>Int%</th>\n",
       "      <th>Y/A</th>\n",
       "      <th>AY/A</th>\n",
       "      <th>Y/C</th>\n",
       "      <th>Y/G</th>\n",
       "      <th>Rate</th>\n",
       "      <th>seasons</th>\n",
       "      <th>seasons_with_draft_team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>67.6</td>\n",
       "      <td>8403.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>10.57</td>\n",
       "      <td>13.9</td>\n",
       "      <td>271.1</td>\n",
       "      <td>175.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>995.0</td>\n",
       "      <td>66.4</td>\n",
       "      <td>9285.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>9.3</td>\n",
       "      <td>10.38</td>\n",
       "      <td>14.0</td>\n",
       "      <td>168.8</td>\n",
       "      <td>170.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.0</td>\n",
       "      <td>695.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>62.6</td>\n",
       "      <td>8148.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.33</td>\n",
       "      <td>11.7</td>\n",
       "      <td>232.8</td>\n",
       "      <td>137.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>1645.0</td>\n",
       "      <td>70.3</td>\n",
       "      <td>13253.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>8.19</td>\n",
       "      <td>11.5</td>\n",
       "      <td>250.1</td>\n",
       "      <td>155.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>64.1</td>\n",
       "      <td>4265.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.88</td>\n",
       "      <td>10.5</td>\n",
       "      <td>142.2</td>\n",
       "      <td>123.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "0  31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "1  55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "2  35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "3  53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "4  30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "\n",
       "[5 rows x 16 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerics_only.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\n",
    "                                                                       axis=1),\n",
    "                                                                       df['seasons_with_draft_team'].apply(map_response),\n",
    "                                                                       test_size=.25,\n",
    "                                                                       shuffle=True,\n",
    "                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\n",
    "                                                                       axis=1),\n",
    "                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\n",
    "                                                                       test_size=.25,\n",
    "                                                                       shuffle=True,\n",
    "                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.010625\n",
      "0:\tlearn: 0.6860037\ttotal: 2.22ms\tremaining: 664ms\n",
      "1:\tlearn: 0.6777927\ttotal: 4.22ms\tremaining: 629ms\n",
      "2:\tlearn: 0.6695467\ttotal: 5.27ms\tremaining: 522ms\n",
      "3:\tlearn: 0.6632005\ttotal: 7.8ms\tremaining: 578ms\n",
      "4:\tlearn: 0.6562832\ttotal: 9.6ms\tremaining: 567ms\n",
      "5:\tlearn: 0.6509253\ttotal: 10.7ms\tremaining: 524ms\n",
      "6:\tlearn: 0.6448863\ttotal: 11.7ms\tremaining: 488ms\n",
      "7:\tlearn: 0.6415797\ttotal: 12.2ms\tremaining: 446ms\n",
      "8:\tlearn: 0.6364113\ttotal: 13.4ms\tremaining: 433ms\n",
      "9:\tlearn: 0.6290033\ttotal: 14.4ms\tremaining: 419ms\n",
      "10:\tlearn: 0.6220800\ttotal: 15.3ms\tremaining: 403ms\n",
      "11:\tlearn: 0.6148267\ttotal: 16.5ms\tremaining: 397ms\n",
      "12:\tlearn: 0.6101433\ttotal: 18.4ms\tremaining: 406ms\n",
      "13:\tlearn: 0.6037479\ttotal: 19.6ms\tremaining: 400ms\n",
      "14:\tlearn: 0.5977267\ttotal: 20.6ms\tremaining: 391ms\n",
      "15:\tlearn: 0.5923923\ttotal: 21.5ms\tremaining: 381ms\n",
      "16:\tlearn: 0.5867612\ttotal: 22.4ms\tremaining: 373ms\n",
      "17:\tlearn: 0.5827596\ttotal: 23.7ms\tremaining: 372ms\n",
      "18:\tlearn: 0.5771357\ttotal: 24.6ms\tremaining: 364ms\n",
      "19:\tlearn: 0.5694309\ttotal: 25.4ms\tremaining: 356ms\n",
      "20:\tlearn: 0.5658455\ttotal: 25.8ms\tremaining: 343ms\n",
      "21:\tlearn: 0.5610902\ttotal: 26.8ms\tremaining: 338ms\n",
      "22:\tlearn: 0.5597035\ttotal: 27.1ms\tremaining: 326ms\n",
      "23:\tlearn: 0.5533159\ttotal: 28.6ms\tremaining: 329ms\n",
      "24:\tlearn: 0.5480409\ttotal: 29.3ms\tremaining: 323ms\n",
      "25:\tlearn: 0.5427419\ttotal: 30.1ms\tremaining: 317ms\n",
      "26:\tlearn: 0.5362003\ttotal: 30.9ms\tremaining: 312ms\n",
      "27:\tlearn: 0.5318121\ttotal: 31.5ms\tremaining: 306ms\n",
      "28:\tlearn: 0.5277533\ttotal: 32.1ms\tremaining: 300ms\n",
      "29:\tlearn: 0.5216916\ttotal: 32.8ms\tremaining: 295ms\n",
      "30:\tlearn: 0.5164736\ttotal: 33.5ms\tremaining: 290ms\n",
      "31:\tlearn: 0.5120956\ttotal: 34.2ms\tremaining: 287ms\n",
      "32:\tlearn: 0.5082404\ttotal: 34.9ms\tremaining: 283ms\n",
      "33:\tlearn: 0.5029867\ttotal: 35.5ms\tremaining: 278ms\n",
      "34:\tlearn: 0.4965903\ttotal: 36.3ms\tremaining: 275ms\n",
      "35:\tlearn: 0.4911687\ttotal: 37.1ms\tremaining: 272ms\n",
      "36:\tlearn: 0.4863159\ttotal: 38.3ms\tremaining: 272ms\n",
      "37:\tlearn: 0.4831417\ttotal: 39.8ms\tremaining: 274ms\n",
      "38:\tlearn: 0.4787644\ttotal: 40.6ms\tremaining: 272ms\n",
      "39:\tlearn: 0.4760859\ttotal: 41.3ms\tremaining: 268ms\n",
      "40:\tlearn: 0.4746385\ttotal: 41.7ms\tremaining: 263ms\n",
      "41:\tlearn: 0.4687500\ttotal: 42.3ms\tremaining: 260ms\n",
      "42:\tlearn: 0.4660409\ttotal: 43ms\tremaining: 257ms\n",
      "43:\tlearn: 0.4625528\ttotal: 43.8ms\tremaining: 255ms\n",
      "44:\tlearn: 0.4585145\ttotal: 44.5ms\tremaining: 252ms\n",
      "45:\tlearn: 0.4542572\ttotal: 45.3ms\tremaining: 250ms\n",
      "46:\tlearn: 0.4517845\ttotal: 45.9ms\tremaining: 247ms\n",
      "47:\tlearn: 0.4485624\ttotal: 46.5ms\tremaining: 244ms\n",
      "48:\tlearn: 0.4443866\ttotal: 47.1ms\tremaining: 241ms\n",
      "49:\tlearn: 0.4401483\ttotal: 48.4ms\tremaining: 242ms\n",
      "50:\tlearn: 0.4367274\ttotal: 49.1ms\tremaining: 240ms\n",
      "51:\tlearn: 0.4329021\ttotal: 49.8ms\tremaining: 237ms\n",
      "52:\tlearn: 0.4294201\ttotal: 50.4ms\tremaining: 235ms\n",
      "53:\tlearn: 0.4258224\ttotal: 51.2ms\tremaining: 233ms\n",
      "54:\tlearn: 0.4231774\ttotal: 51.8ms\tremaining: 231ms\n",
      "55:\tlearn: 0.4193949\ttotal: 52.5ms\tremaining: 229ms\n",
      "56:\tlearn: 0.4157227\ttotal: 53.1ms\tremaining: 226ms\n",
      "57:\tlearn: 0.4127078\ttotal: 53.9ms\tremaining: 225ms\n",
      "58:\tlearn: 0.4100389\ttotal: 55ms\tremaining: 225ms\n",
      "59:\tlearn: 0.4069064\ttotal: 56.5ms\tremaining: 226ms\n",
      "60:\tlearn: 0.4041301\ttotal: 57.3ms\tremaining: 225ms\n",
      "61:\tlearn: 0.4004144\ttotal: 58.1ms\tremaining: 223ms\n",
      "62:\tlearn: 0.3965323\ttotal: 58.9ms\tremaining: 222ms\n",
      "63:\tlearn: 0.3943053\ttotal: 59.3ms\tremaining: 219ms\n",
      "64:\tlearn: 0.3912832\ttotal: 60ms\tremaining: 217ms\n",
      "65:\tlearn: 0.3880797\ttotal: 60.7ms\tremaining: 215ms\n",
      "66:\tlearn: 0.3851302\ttotal: 61.4ms\tremaining: 214ms\n",
      "67:\tlearn: 0.3834912\ttotal: 62.2ms\tremaining: 212ms\n",
      "68:\tlearn: 0.3806593\ttotal: 62.8ms\tremaining: 210ms\n",
      "69:\tlearn: 0.3778226\ttotal: 63.5ms\tremaining: 209ms\n",
      "70:\tlearn: 0.3747209\ttotal: 64.4ms\tremaining: 208ms\n",
      "71:\tlearn: 0.3715683\ttotal: 65.1ms\tremaining: 206ms\n",
      "72:\tlearn: 0.3684941\ttotal: 65.6ms\tremaining: 204ms\n",
      "73:\tlearn: 0.3657287\ttotal: 66.2ms\tremaining: 202ms\n",
      "74:\tlearn: 0.3644687\ttotal: 66.9ms\tremaining: 201ms\n",
      "75:\tlearn: 0.3616780\ttotal: 67.6ms\tremaining: 199ms\n",
      "76:\tlearn: 0.3588888\ttotal: 68.2ms\tremaining: 198ms\n",
      "77:\tlearn: 0.3560348\ttotal: 68.9ms\tremaining: 196ms\n",
      "78:\tlearn: 0.3534702\ttotal: 69.6ms\tremaining: 195ms\n",
      "79:\tlearn: 0.3512633\ttotal: 70.5ms\tremaining: 194ms\n",
      "80:\tlearn: 0.3483155\ttotal: 71.3ms\tremaining: 193ms\n",
      "81:\tlearn: 0.3451383\ttotal: 72.2ms\tremaining: 192ms\n",
      "82:\tlearn: 0.3430613\ttotal: 73.5ms\tremaining: 192ms\n",
      "83:\tlearn: 0.3405574\ttotal: 74.3ms\tremaining: 191ms\n",
      "84:\tlearn: 0.3387700\ttotal: 75.2ms\tremaining: 190ms\n",
      "85:\tlearn: 0.3359461\ttotal: 75.9ms\tremaining: 189ms\n",
      "86:\tlearn: 0.3332326\ttotal: 76.7ms\tremaining: 188ms\n",
      "87:\tlearn: 0.3312779\ttotal: 77.5ms\tremaining: 187ms\n",
      "88:\tlearn: 0.3289833\ttotal: 78.2ms\tremaining: 185ms\n",
      "89:\tlearn: 0.3276879\ttotal: 78.9ms\tremaining: 184ms\n",
      "90:\tlearn: 0.3245574\ttotal: 79.5ms\tremaining: 183ms\n",
      "91:\tlearn: 0.3227922\ttotal: 80.1ms\tremaining: 181ms\n",
      "92:\tlearn: 0.3207355\ttotal: 80.8ms\tremaining: 180ms\n",
      "93:\tlearn: 0.3185965\ttotal: 81.5ms\tremaining: 179ms\n",
      "94:\tlearn: 0.3168189\ttotal: 82.2ms\tremaining: 177ms\n",
      "95:\tlearn: 0.3153263\ttotal: 82.9ms\tremaining: 176ms\n",
      "96:\tlearn: 0.3130134\ttotal: 84.4ms\tremaining: 177ms\n",
      "97:\tlearn: 0.3118534\ttotal: 85.1ms\tremaining: 175ms\n",
      "98:\tlearn: 0.3102042\ttotal: 85.8ms\tremaining: 174ms\n",
      "99:\tlearn: 0.3086383\ttotal: 86.7ms\tremaining: 173ms\n",
      "100:\tlearn: 0.3059442\ttotal: 87.7ms\tremaining: 173ms\n",
      "101:\tlearn: 0.3044225\ttotal: 88.6ms\tremaining: 172ms\n",
      "102:\tlearn: 0.3027919\ttotal: 89.5ms\tremaining: 171ms\n",
      "103:\tlearn: 0.3007658\ttotal: 90.2ms\tremaining: 170ms\n",
      "104:\tlearn: 0.2980790\ttotal: 90.9ms\tremaining: 169ms\n",
      "105:\tlearn: 0.2959221\ttotal: 91.7ms\tremaining: 168ms\n",
      "106:\tlearn: 0.2942563\ttotal: 92.4ms\tremaining: 167ms\n",
      "107:\tlearn: 0.2935584\ttotal: 92.9ms\tremaining: 165ms\n",
      "108:\tlearn: 0.2920227\ttotal: 93.7ms\tremaining: 164ms\n",
      "109:\tlearn: 0.2902555\ttotal: 94.4ms\tremaining: 163ms\n",
      "110:\tlearn: 0.2900891\ttotal: 94.6ms\tremaining: 161ms\n",
      "111:\tlearn: 0.2883565\ttotal: 95.2ms\tremaining: 160ms\n",
      "112:\tlearn: 0.2864017\ttotal: 96ms\tremaining: 159ms\n",
      "113:\tlearn: 0.2842692\ttotal: 96.8ms\tremaining: 158ms\n",
      "114:\tlearn: 0.2830815\ttotal: 97.3ms\tremaining: 157ms\n",
      "115:\tlearn: 0.2817191\ttotal: 97.9ms\tremaining: 155ms\n",
      "116:\tlearn: 0.2795842\ttotal: 98.6ms\tremaining: 154ms\n",
      "117:\tlearn: 0.2773834\ttotal: 99.3ms\tremaining: 153ms\n",
      "118:\tlearn: 0.2754737\ttotal: 101ms\tremaining: 153ms\n",
      "119:\tlearn: 0.2742263\ttotal: 101ms\tremaining: 152ms\n",
      "120:\tlearn: 0.2727494\ttotal: 102ms\tremaining: 151ms\n",
      "121:\tlearn: 0.2714519\ttotal: 103ms\tremaining: 150ms\n",
      "122:\tlearn: 0.2703914\ttotal: 104ms\tremaining: 150ms\n",
      "123:\tlearn: 0.2683544\ttotal: 105ms\tremaining: 149ms\n",
      "124:\tlearn: 0.2670571\ttotal: 105ms\tremaining: 148ms\n",
      "125:\tlearn: 0.2656571\ttotal: 106ms\tremaining: 147ms\n",
      "126:\tlearn: 0.2637953\ttotal: 107ms\tremaining: 146ms\n",
      "127:\tlearn: 0.2623808\ttotal: 108ms\tremaining: 145ms\n",
      "128:\tlearn: 0.2605900\ttotal: 108ms\tremaining: 143ms\n",
      "129:\tlearn: 0.2589565\ttotal: 109ms\tremaining: 142ms\n",
      "130:\tlearn: 0.2577577\ttotal: 110ms\tremaining: 141ms\n",
      "131:\tlearn: 0.2565566\ttotal: 111ms\tremaining: 141ms\n",
      "132:\tlearn: 0.2551464\ttotal: 112ms\tremaining: 140ms\n",
      "133:\tlearn: 0.2531911\ttotal: 112ms\tremaining: 139ms\n",
      "134:\tlearn: 0.2513381\ttotal: 113ms\tremaining: 138ms\n",
      "135:\tlearn: 0.2502180\ttotal: 114ms\tremaining: 137ms\n",
      "136:\tlearn: 0.2484547\ttotal: 115ms\tremaining: 136ms\n",
      "137:\tlearn: 0.2478077\ttotal: 115ms\tremaining: 136ms\n",
      "138:\tlearn: 0.2467110\ttotal: 116ms\tremaining: 135ms\n",
      "139:\tlearn: 0.2457368\ttotal: 117ms\tremaining: 134ms\n",
      "140:\tlearn: 0.2455714\ttotal: 117ms\tremaining: 132ms\n",
      "141:\tlearn: 0.2442697\ttotal: 118ms\tremaining: 131ms\n",
      "142:\tlearn: 0.2430402\ttotal: 119ms\tremaining: 130ms\n",
      "143:\tlearn: 0.2412092\ttotal: 120ms\tremaining: 130ms\n",
      "144:\tlearn: 0.2398108\ttotal: 120ms\tremaining: 129ms\n",
      "145:\tlearn: 0.2382954\ttotal: 123ms\tremaining: 130ms\n",
      "146:\tlearn: 0.2369953\ttotal: 124ms\tremaining: 129ms\n",
      "147:\tlearn: 0.2362595\ttotal: 125ms\tremaining: 128ms\n",
      "148:\tlearn: 0.2351744\ttotal: 126ms\tremaining: 127ms\n",
      "149:\tlearn: 0.2339612\ttotal: 126ms\tremaining: 126ms\n",
      "150:\tlearn: 0.2338796\ttotal: 127ms\tremaining: 125ms\n",
      "151:\tlearn: 0.2325673\ttotal: 127ms\tremaining: 124ms\n",
      "152:\tlearn: 0.2310790\ttotal: 128ms\tremaining: 123ms\n",
      "153:\tlearn: 0.2302860\ttotal: 129ms\tremaining: 122ms\n",
      "154:\tlearn: 0.2284060\ttotal: 129ms\tremaining: 121ms\n",
      "155:\tlearn: 0.2272278\ttotal: 130ms\tremaining: 120ms\n",
      "156:\tlearn: 0.2258567\ttotal: 131ms\tremaining: 119ms\n",
      "157:\tlearn: 0.2250139\ttotal: 131ms\tremaining: 118ms\n",
      "158:\tlearn: 0.2240219\ttotal: 132ms\tremaining: 117ms\n",
      "159:\tlearn: 0.2225017\ttotal: 133ms\tremaining: 116ms\n",
      "160:\tlearn: 0.2213832\ttotal: 133ms\tremaining: 115ms\n",
      "161:\tlearn: 0.2203321\ttotal: 134ms\tremaining: 114ms\n",
      "162:\tlearn: 0.2186189\ttotal: 135ms\tremaining: 113ms\n",
      "163:\tlearn: 0.2178334\ttotal: 135ms\tremaining: 112ms\n",
      "164:\tlearn: 0.2167397\ttotal: 136ms\tremaining: 111ms\n",
      "165:\tlearn: 0.2155833\ttotal: 137ms\tremaining: 110ms\n",
      "166:\tlearn: 0.2147377\ttotal: 138ms\tremaining: 110ms\n",
      "167:\tlearn: 0.2138558\ttotal: 139ms\tremaining: 109ms\n",
      "168:\tlearn: 0.2121823\ttotal: 139ms\tremaining: 108ms\n",
      "169:\tlearn: 0.2114470\ttotal: 140ms\tremaining: 107ms\n",
      "170:\tlearn: 0.2103092\ttotal: 140ms\tremaining: 106ms\n",
      "171:\tlearn: 0.2102554\ttotal: 141ms\tremaining: 105ms\n",
      "172:\tlearn: 0.2095748\ttotal: 141ms\tremaining: 104ms\n",
      "173:\tlearn: 0.2084638\ttotal: 142ms\tremaining: 103ms\n",
      "174:\tlearn: 0.2070404\ttotal: 143ms\tremaining: 102ms\n",
      "175:\tlearn: 0.2057900\ttotal: 143ms\tremaining: 101ms\n",
      "176:\tlearn: 0.2057418\ttotal: 143ms\tremaining: 99.6ms\n",
      "177:\tlearn: 0.2045917\ttotal: 144ms\tremaining: 98.7ms\n",
      "178:\tlearn: 0.2031833\ttotal: 145ms\tremaining: 98ms\n",
      "179:\tlearn: 0.2025743\ttotal: 145ms\tremaining: 96.8ms\n",
      "180:\tlearn: 0.2017798\ttotal: 146ms\tremaining: 95.9ms\n",
      "181:\tlearn: 0.2002762\ttotal: 147ms\tremaining: 95.1ms\n",
      "182:\tlearn: 0.1991014\ttotal: 147ms\tremaining: 94.3ms\n",
      "183:\tlearn: 0.1981233\ttotal: 148ms\tremaining: 93.4ms\n",
      "184:\tlearn: 0.1969890\ttotal: 149ms\tremaining: 92.6ms\n",
      "185:\tlearn: 0.1956314\ttotal: 150ms\tremaining: 91.7ms\n",
      "186:\tlearn: 0.1944889\ttotal: 151ms\tremaining: 91.4ms\n",
      "187:\tlearn: 0.1936778\ttotal: 152ms\tremaining: 90.6ms\n",
      "188:\tlearn: 0.1925399\ttotal: 153ms\tremaining: 89.6ms\n",
      "189:\tlearn: 0.1918157\ttotal: 153ms\tremaining: 88.8ms\n",
      "190:\tlearn: 0.1902695\ttotal: 154ms\tremaining: 87.9ms\n",
      "191:\tlearn: 0.1890875\ttotal: 155ms\tremaining: 87ms\n",
      "192:\tlearn: 0.1881684\ttotal: 155ms\tremaining: 86.2ms\n",
      "193:\tlearn: 0.1875198\ttotal: 156ms\tremaining: 85.3ms\n",
      "194:\tlearn: 0.1862718\ttotal: 157ms\tremaining: 84.5ms\n",
      "195:\tlearn: 0.1855233\ttotal: 158ms\tremaining: 83.6ms\n",
      "196:\tlearn: 0.1843243\ttotal: 159ms\tremaining: 83.2ms\n",
      "197:\tlearn: 0.1835263\ttotal: 160ms\tremaining: 82.3ms\n",
      "198:\tlearn: 0.1825861\ttotal: 161ms\tremaining: 81.5ms\n",
      "199:\tlearn: 0.1820098\ttotal: 161ms\tremaining: 80.6ms\n",
      "200:\tlearn: 0.1815209\ttotal: 162ms\tremaining: 79.8ms\n",
      "201:\tlearn: 0.1804775\ttotal: 163ms\tremaining: 78.9ms\n",
      "202:\tlearn: 0.1796041\ttotal: 163ms\tremaining: 78.1ms\n",
      "203:\tlearn: 0.1785366\ttotal: 164ms\tremaining: 77.2ms\n",
      "204:\tlearn: 0.1775796\ttotal: 165ms\tremaining: 76.3ms\n",
      "205:\tlearn: 0.1768129\ttotal: 165ms\tremaining: 75.5ms\n",
      "206:\tlearn: 0.1760751\ttotal: 166ms\tremaining: 74.6ms\n",
      "207:\tlearn: 0.1760439\ttotal: 166ms\tremaining: 73.5ms\n",
      "208:\tlearn: 0.1749725\ttotal: 167ms\tremaining: 72.7ms\n",
      "209:\tlearn: 0.1740533\ttotal: 168ms\tremaining: 71.9ms\n",
      "210:\tlearn: 0.1731028\ttotal: 169ms\tremaining: 71.4ms\n",
      "211:\tlearn: 0.1725858\ttotal: 170ms\tremaining: 70.5ms\n",
      "212:\tlearn: 0.1716198\ttotal: 171ms\tremaining: 69.7ms\n",
      "213:\tlearn: 0.1715622\ttotal: 171ms\tremaining: 68.7ms\n",
      "214:\tlearn: 0.1705225\ttotal: 172ms\tremaining: 67.9ms\n",
      "215:\tlearn: 0.1694816\ttotal: 172ms\tremaining: 67.1ms\n",
      "216:\tlearn: 0.1690974\ttotal: 173ms\tremaining: 66.1ms\n",
      "217:\tlearn: 0.1679222\ttotal: 173ms\tremaining: 65.3ms\n",
      "218:\tlearn: 0.1673266\ttotal: 174ms\tremaining: 64.4ms\n",
      "219:\tlearn: 0.1667782\ttotal: 175ms\tremaining: 63.5ms\n",
      "220:\tlearn: 0.1660783\ttotal: 175ms\tremaining: 62.7ms\n",
      "221:\tlearn: 0.1652618\ttotal: 176ms\tremaining: 61.8ms\n",
      "222:\tlearn: 0.1644115\ttotal: 177ms\tremaining: 61.2ms\n",
      "223:\tlearn: 0.1639197\ttotal: 179ms\tremaining: 60.6ms\n",
      "224:\tlearn: 0.1632194\ttotal: 179ms\tremaining: 59.8ms\n",
      "225:\tlearn: 0.1625594\ttotal: 180ms\tremaining: 59ms\n",
      "226:\tlearn: 0.1625347\ttotal: 180ms\tremaining: 58ms\n",
      "227:\tlearn: 0.1618872\ttotal: 181ms\tremaining: 57.2ms\n",
      "228:\tlearn: 0.1612063\ttotal: 182ms\tremaining: 56.4ms\n",
      "229:\tlearn: 0.1610155\ttotal: 182ms\tremaining: 55.5ms\n",
      "230:\tlearn: 0.1607183\ttotal: 183ms\tremaining: 54.6ms\n",
      "231:\tlearn: 0.1605100\ttotal: 183ms\tremaining: 53.6ms\n",
      "232:\tlearn: 0.1592134\ttotal: 184ms\tremaining: 52.8ms\n",
      "233:\tlearn: 0.1582837\ttotal: 185ms\tremaining: 52.1ms\n",
      "234:\tlearn: 0.1575475\ttotal: 185ms\tremaining: 51.3ms\n",
      "235:\tlearn: 0.1569659\ttotal: 186ms\tremaining: 50.5ms\n",
      "236:\tlearn: 0.1566963\ttotal: 187ms\tremaining: 49.7ms\n",
      "237:\tlearn: 0.1555236\ttotal: 188ms\tremaining: 49ms\n",
      "238:\tlearn: 0.1552997\ttotal: 189ms\tremaining: 48.1ms\n",
      "239:\tlearn: 0.1545407\ttotal: 190ms\tremaining: 47.5ms\n",
      "240:\tlearn: 0.1540820\ttotal: 191ms\tremaining: 46.6ms\n",
      "241:\tlearn: 0.1534282\ttotal: 191ms\tremaining: 45.9ms\n",
      "242:\tlearn: 0.1527701\ttotal: 192ms\tremaining: 45.1ms\n",
      "243:\tlearn: 0.1518453\ttotal: 194ms\tremaining: 44.5ms\n",
      "244:\tlearn: 0.1513219\ttotal: 194ms\tremaining: 43.7ms\n",
      "245:\tlearn: 0.1511386\ttotal: 195ms\tremaining: 42.7ms\n",
      "246:\tlearn: 0.1503077\ttotal: 196ms\tremaining: 42ms\n",
      "247:\tlearn: 0.1496699\ttotal: 196ms\tremaining: 41.2ms\n",
      "248:\tlearn: 0.1487662\ttotal: 197ms\tremaining: 40.4ms\n",
      "249:\tlearn: 0.1480262\ttotal: 198ms\tremaining: 39.6ms\n",
      "250:\tlearn: 0.1477591\ttotal: 198ms\tremaining: 38.7ms\n",
      "251:\tlearn: 0.1470499\ttotal: 199ms\tremaining: 37.9ms\n",
      "252:\tlearn: 0.1461304\ttotal: 200ms\tremaining: 37.1ms\n",
      "253:\tlearn: 0.1455850\ttotal: 200ms\tremaining: 36.3ms\n",
      "254:\tlearn: 0.1454419\ttotal: 201ms\tremaining: 35.4ms\n",
      "255:\tlearn: 0.1449629\ttotal: 202ms\tremaining: 34.6ms\n",
      "256:\tlearn: 0.1441504\ttotal: 202ms\tremaining: 33.9ms\n",
      "257:\tlearn: 0.1437663\ttotal: 203ms\tremaining: 33ms\n",
      "258:\tlearn: 0.1431426\ttotal: 204ms\tremaining: 32.2ms\n",
      "259:\tlearn: 0.1428482\ttotal: 204ms\tremaining: 31.4ms\n",
      "260:\tlearn: 0.1426916\ttotal: 204ms\tremaining: 30.5ms\n",
      "261:\tlearn: 0.1418239\ttotal: 205ms\tremaining: 29.7ms\n",
      "262:\tlearn: 0.1412539\ttotal: 206ms\tremaining: 28.9ms\n",
      "263:\tlearn: 0.1408203\ttotal: 207ms\tremaining: 28.2ms\n",
      "264:\tlearn: 0.1400394\ttotal: 208ms\tremaining: 27.5ms\n",
      "265:\tlearn: 0.1395601\ttotal: 209ms\tremaining: 26.7ms\n",
      "266:\tlearn: 0.1390306\ttotal: 210ms\tremaining: 25.9ms\n",
      "267:\tlearn: 0.1387121\ttotal: 210ms\tremaining: 25.1ms\n",
      "268:\tlearn: 0.1381401\ttotal: 211ms\tremaining: 24.3ms\n",
      "269:\tlearn: 0.1374756\ttotal: 211ms\tremaining: 23.5ms\n",
      "270:\tlearn: 0.1371511\ttotal: 213ms\tremaining: 22.8ms\n",
      "271:\tlearn: 0.1367263\ttotal: 213ms\tremaining: 22ms\n",
      "272:\tlearn: 0.1360371\ttotal: 214ms\tremaining: 21.2ms\n",
      "273:\tlearn: 0.1353967\ttotal: 215ms\tremaining: 20.4ms\n",
      "274:\tlearn: 0.1345073\ttotal: 216ms\tremaining: 19.6ms\n",
      "275:\tlearn: 0.1339782\ttotal: 217ms\tremaining: 18.9ms\n",
      "276:\tlearn: 0.1335254\ttotal: 218ms\tremaining: 18.1ms\n",
      "277:\tlearn: 0.1330796\ttotal: 219ms\tremaining: 17.3ms\n",
      "278:\tlearn: 0.1326663\ttotal: 220ms\tremaining: 16.5ms\n",
      "279:\tlearn: 0.1324111\ttotal: 221ms\tremaining: 15.8ms\n",
      "280:\tlearn: 0.1318211\ttotal: 221ms\tremaining: 15ms\n",
      "281:\tlearn: 0.1310334\ttotal: 222ms\tremaining: 14.2ms\n",
      "282:\tlearn: 0.1305866\ttotal: 223ms\tremaining: 13.4ms\n",
      "283:\tlearn: 0.1302007\ttotal: 223ms\tremaining: 12.6ms\n",
      "284:\tlearn: 0.1297652\ttotal: 224ms\tremaining: 11.8ms\n",
      "285:\tlearn: 0.1290486\ttotal: 224ms\tremaining: 11ms\n",
      "286:\tlearn: 0.1290219\ttotal: 224ms\tremaining: 10.2ms\n",
      "287:\tlearn: 0.1282893\ttotal: 225ms\tremaining: 9.38ms\n",
      "288:\tlearn: 0.1277381\ttotal: 226ms\tremaining: 8.6ms\n",
      "289:\tlearn: 0.1273478\ttotal: 227ms\tremaining: 7.81ms\n",
      "290:\tlearn: 0.1271250\ttotal: 227ms\tremaining: 7.02ms\n",
      "291:\tlearn: 0.1265971\ttotal: 228ms\tremaining: 6.24ms\n",
      "292:\tlearn: 0.1264147\ttotal: 228ms\tremaining: 5.45ms\n",
      "293:\tlearn: 0.1259385\ttotal: 229ms\tremaining: 4.67ms\n",
      "294:\tlearn: 0.1256171\ttotal: 229ms\tremaining: 3.88ms\n",
      "295:\tlearn: 0.1252767\ttotal: 230ms\tremaining: 3.11ms\n",
      "296:\tlearn: 0.1249303\ttotal: 231ms\tremaining: 2.33ms\n",
      "297:\tlearn: 0.1246539\ttotal: 231ms\tremaining: 1.55ms\n",
      "298:\tlearn: 0.1239238\ttotal: 232ms\tremaining: 776us\n",
      "299:\tlearn: 0.1232792\ttotal: 233ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7928571428571428"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\n",
    "model.fit(X_train, y_train)\n",
    "y_preds = model.predict(X_test)\n",
    "y_probs = model.predict_proba(X_test)[:,1]\n",
    "accuracy_score(y_test, y_preds)\n",
    "roc_auc_score(y_test, y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pfr_player_name</th>\n",
       "      <th>round</th>\n",
       "      <th>pick</th>\n",
       "      <th>season</th>\n",
       "      <th>G</th>\n",
       "      <th>Cmp</th>\n",
       "      <th>Att</th>\n",
       "      <th>Cmp%</th>\n",
       "      <th>Yds</th>\n",
       "      <th>TD</th>\n",
       "      <th>TD%</th>\n",
       "      <th>Int</th>\n",
       "      <th>Int%</th>\n",
       "      <th>Y/A</th>\n",
       "      <th>AY/A</th>\n",
       "      <th>Y/C</th>\n",
       "      <th>Y/G</th>\n",
       "      <th>Rate</th>\n",
       "      <th>seasons</th>\n",
       "      <th>name</th>\n",
       "      <th>recent_team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Joe Burrow</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>74.4</td>\n",
       "      <td>287.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>8.38</td>\n",
       "      <td>9.9</td>\n",
       "      <td>26.1</td>\n",
       "      <td>153.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Joe Burrow</td>\n",
       "      <td>CIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Kyle Trask</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>2021</td>\n",
       "      <td>29.0</td>\n",
       "      <td>552.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>67.9</td>\n",
       "      <td>7386.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>9.1</td>\n",
       "      <td>9.95</td>\n",
       "      <td>13.4</td>\n",
       "      <td>254.7</td>\n",
       "      <td>168.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Kyle Trask</td>\n",
       "      <td>TB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Jordan Love</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2020</td>\n",
       "      <td>38.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>61.2</td>\n",
       "      <td>8600.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.55</td>\n",
       "      <td>12.5</td>\n",
       "      <td>226.3</td>\n",
       "      <td>137.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Jordan Love</td>\n",
       "      <td>GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Nathan Peterman</td>\n",
       "      <td>5</td>\n",
       "      <td>171</td>\n",
       "      <td>2017</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>46.5</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.7</td>\n",
       "      <td>9.4</td>\n",
       "      <td>55.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Nathan Peterman</td>\n",
       "      <td>BUF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Will Grier</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>2019</td>\n",
       "      <td>22.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>7354.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>9.4</td>\n",
       "      <td>10.03</td>\n",
       "      <td>14.3</td>\n",
       "      <td>334.3</td>\n",
       "      <td>169.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Will Grier</td>\n",
       "      <td>CAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Jameis Winston</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>27.0</td>\n",
       "      <td>562.0</td>\n",
       "      <td>851.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>7964.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>9.4</td>\n",
       "      <td>9.41</td>\n",
       "      <td>14.2</td>\n",
       "      <td>295.0</td>\n",
       "      <td>163.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jameis Winston</td>\n",
       "      <td>TB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Nick Foles</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>2012</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.13</td>\n",
       "      <td>11.4</td>\n",
       "      <td>57.0</td>\n",
       "      <td>122.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Nick Foles</td>\n",
       "      <td>PHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Cam Newton</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.50</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>87.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Cam Newton</td>\n",
       "      <td>CAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Marcus Mariota</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>41.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>66.8</td>\n",
       "      <td>10796.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>10.51</td>\n",
       "      <td>13.9</td>\n",
       "      <td>263.3</td>\n",
       "      <td>171.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Marcus Mariota</td>\n",
       "      <td>TEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Ben DiNucci</td>\n",
       "      <td>7</td>\n",
       "      <td>231</td>\n",
       "      <td>2020</td>\n",
       "      <td>12.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>54.5</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>6.6</td>\n",
       "      <td>5.46</td>\n",
       "      <td>12.2</td>\n",
       "      <td>92.3</td>\n",
       "      <td>113.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Ben DiNucci</td>\n",
       "      <td>DAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pfr_player_name  round  pick  ...  seasons             name  recent_team\n",
       "102       Joe Burrow      1     1  ...      7.0       Joe Burrow          CIN\n",
       "97        Kyle Trask      2    64  ...      5.0       Kyle Trask           TB\n",
       "105      Jordan Love      1    26  ...      3.0      Jordan Love           GB\n",
       "146  Nathan Peterman      5   171  ...      7.0  Nathan Peterman          BUF\n",
       "119       Will Grier      3   100  ...      7.0       Will Grier          CAR\n",
       "..               ...    ...   ...  ...      ...              ...          ...\n",
       "61    Jameis Winston      1     1  ...      2.0   Jameis Winston           TB\n",
       "31        Nick Foles      3    88  ...      8.0       Nick Foles          PHI\n",
       "13        Cam Newton      1     1  ...      7.0       Cam Newton          CAR\n",
       "62    Marcus Mariota      1     2  ...      3.0   Marcus Mariota          TEN\n",
       "112      Ben DiNucci      7   231  ...      2.0      Ben DiNucci          DAL\n",
       "\n",
       "[81 rows x 21 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>Cmp</th>\n",
       "      <th>Att</th>\n",
       "      <th>Cmp%</th>\n",
       "      <th>Yds</th>\n",
       "      <th>TD</th>\n",
       "      <th>TD%</th>\n",
       "      <th>Int</th>\n",
       "      <th>Int%</th>\n",
       "      <th>Y/A</th>\n",
       "      <th>AY/A</th>\n",
       "      <th>Y/C</th>\n",
       "      <th>Y/G</th>\n",
       "      <th>Rate</th>\n",
       "      <th>seasons</th>\n",
       "      <th>seasons_with_draft_team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>67.6</td>\n",
       "      <td>8403.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>10.57</td>\n",
       "      <td>13.9</td>\n",
       "      <td>271.1</td>\n",
       "      <td>175.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>995.0</td>\n",
       "      <td>66.4</td>\n",
       "      <td>9285.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>9.3</td>\n",
       "      <td>10.38</td>\n",
       "      <td>14.0</td>\n",
       "      <td>168.8</td>\n",
       "      <td>170.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.0</td>\n",
       "      <td>695.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>62.6</td>\n",
       "      <td>8148.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>7.33</td>\n",
       "      <td>11.7</td>\n",
       "      <td>232.8</td>\n",
       "      <td>137.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>1645.0</td>\n",
       "      <td>70.3</td>\n",
       "      <td>13253.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>8.19</td>\n",
       "      <td>11.5</td>\n",
       "      <td>250.1</td>\n",
       "      <td>155.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>64.1</td>\n",
       "      <td>4265.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.88</td>\n",
       "      <td>10.5</td>\n",
       "      <td>142.2</td>\n",
       "      <td>123.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>37.0</td>\n",
       "      <td>614.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>61.5</td>\n",
       "      <td>7138.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>6.90</td>\n",
       "      <td>11.6</td>\n",
       "      <td>192.9</td>\n",
       "      <td>133.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>46.5</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.7</td>\n",
       "      <td>9.4</td>\n",
       "      <td>55.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>38.0</td>\n",
       "      <td>721.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>60.6</td>\n",
       "      <td>9972.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8.64</td>\n",
       "      <td>13.8</td>\n",
       "      <td>262.4</td>\n",
       "      <td>146.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>22.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.7</td>\n",
       "      <td>8.72</td>\n",
       "      <td>13.5</td>\n",
       "      <td>309.1</td>\n",
       "      <td>152.3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       "145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       "148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "\n",
       "[149 rows x 16 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7525  \n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7407407407407407"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1],)))\n",
    "model.add(Dense(units=32, activation='tanh'))\n",
    "model.add(Normalization())\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(units=16, activation='tanh'))\n",
    "model.add(Dense(units=8, activation='tanh'))\n",
    "model.add(Normalization())\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='poisson')\n",
    "model.fit(X_train, y_train)\n",
    "y_probs = model.predict(X_test)\n",
    "y_preds = (y_probs >= 5).astype(int)\n",
    "\n",
    "roc_auc_score(y_test, y_probs)\n",
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class MyFirstModel(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "\n",
    "validating = MyFirstModel(first_name=\"marc\", last_name=\"nealer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  'import mfl as mfl',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np',\n",
       "  'df = mfl.api.data_loaders.load_qb_data_cleaned()',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata',\n",
       "  'df = mfldata.load_qb_data_cleaned()',\n",
       "  'df',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.ensemble',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom keras.layers import Dense',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom keras.layers import Dense',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Sigmoid',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  'nfl.import_draft_picks()',\n",
       "  'nfl.import_draft_picks(years=2002)',\n",
       "  'nfl.import_draft_picks(years=[2002])',\n",
       "  'df',\n",
       "  \"df.select_dtypes(include='int')\",\n",
       "  \"df.select_dtypes(include='float')\",\n",
       "  \"numerics_only = df.select_dtypes(include='float')\",\n",
       "  'numerics_only',\n",
       "  \"numerics_only = df.select_dtypes(include='float').dropna()\",\n",
       "  'numerics_only',\n",
       "  'numerics_only.head(5)',\n",
       "  'def map_response(x):\\n    if x >= 4:\\n        return 1\\n    else: \\n        return 0',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = LogisticRegression()\\nmodel.fit(X_train, y_train)',\n",
       "  'model = LogisticRegression()\\nmodel.fit(X_train, y_train)\\nmodel.predict(X_test)',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\nmodel.predict(X_test)',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = RandomForestClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = XGBClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = LogisticRegression(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = LogisticRegression()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_tste)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,0]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = RandomForestClassifier\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'y_probss',\n",
       "  'y_probs',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_preds)',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC(probability=1)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:5]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:4]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:4]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:8]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:8]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:9]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:9]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:10]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = CatBoostClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = CatBoostClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = CatBoostClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  'X_train',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'X_train',\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  'df = df.dropna()',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(iterations=300, n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional\\nfrom keras.models import Sequential',\n",
       "  'model = Sequential()',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense()\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(units=df.shape[0])\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(units=df.shape[1])\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=df.shape[1])\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=df.shape[1], units=64)\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=tuple(df.shape[1]), units=64)\\n)',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization\\nfrom keras.models import Sequential',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout\\nfrom keras.models import Sequential',\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)=\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'X_train',\n",
       "  'y_train',\n",
       "  'y_train.isna().sum(0)',\n",
       "  'y_train.isna().sum()',\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(ReLU(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'y_train.shape',\n",
       "  'X_train.shape',\n",
       "  'model',\n",
       "  'X_train',\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'X_train.shape[1]',\n",
       "  \"model = Sequential()\\nmodel.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\\nfrom keras.models import Sequential',\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=X_train.shape[1]))\\nmodel.add(Dense(units=32activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=X_train.shape[1]))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(,X_train.shape[1])))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_preds, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\ny_probs(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='roc-auc')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='roc_auc')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='SparseCategoricalCrossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\\ny_preds_NN = model.predict(X_test)\\ny_preds_NN = (y_preds_NN >= .5).astype(int)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\n\\n\\nmodel.fit(X_train, y_train)\\ny_preds_NN = model.predict(X_test)\\ny_preds_NN = (y_preds_NN >= .5).astype(int)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\\nfrom keras.models import Sequential',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        \",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n        model = model(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n        model = model(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        self.meric_dict = metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        self.model_results = pd.DataFrame(self.metric_dict)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        self.meric_dict = metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        self.model_results = pd.DataFrame(self.metric_dict)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='lr')\",\n",
       "  \"NFL = FranchiseQB(model='svc')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='svm')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='xgb')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='rf')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  'df',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        if self.model == 'catboost':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=X.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        if self.model == 'catboost':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=X.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  'X_train',\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat'\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=3):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=1):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(X.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       X['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend((model.predict_proba(X_test)[:,1]))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  'import pydantic',\n",
       "  'from pydantic import BaseModel',\n",
       "  'from pydantic import BaseModel\\n\\nclass MyFirstModel(BaseModel):\\n    first_name: str\\n    last_name: str\\n\\nvalidating = MyFirstModel(first_name=\"marc\", last_name=\"nealer\")',\n",
       "  \"MyFirstModel(first_name='Ben', last_name=0)\",\n",
       "  'from Exception import RuntimeError',\n",
       "  'Exception',\n",
       "  'ValueError',\n",
       "  'compile',\n",
       "  'globals()',\n",
       "  'globals().__len__()',\n",
       "  'globals()[9]',\n",
       "  'locals()',\n",
       "  'vars()'],\n",
       " '_oh': {6:      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       "  0       Sam Bradford      1  ...           LA                      4.0\n",
       "  1          Tim Tebow      1  ...          DEN                      2.0\n",
       "  2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       "  3         Colt McCoy      3  ...          CLE                      3.0\n",
       "  4         Mike Kafka      4  ...          PHI                      1.0\n",
       "  ..               ...    ...  ...          ...                      ...\n",
       "  144    C.J. Beathard      3  ...          NaN                      NaN\n",
       "  145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       "  146  Nathan Peterman      5  ...          BUF                      2.0\n",
       "  147       Brad Kaaya      6  ...          NaN                      NaN\n",
       "  148       Chad Kelly      7  ...          DEN                      1.0\n",
       "  \n",
       "  [149 rows x 22 columns],\n",
       "  18:       season  round  pick team  ... rec_tds def_solo_tackles def_ints def_sacks\n",
       "  6526    2002      1     1  HOU  ...     0.0              NaN      NaN       NaN\n",
       "  6527    2002      1     2  CAR  ...     0.0            557.0     11.0     159.5\n",
       "  6528    2002      1     3  DET  ...     0.0              NaN      NaN       NaN\n",
       "  6529    2002      1     4  BUF  ...     0.0              NaN      NaN       NaN\n",
       "  6530    2002      1     5  SDG  ...     0.0            631.0     21.0       NaN\n",
       "  ...      ...    ...   ...  ...  ...     ...              ...      ...       ...\n",
       "  6782    2002      7   257  WAS  ...     2.0             74.0      NaN       NaN\n",
       "  6783    2002      7   258  CAR  ...     0.0              3.0      NaN       NaN\n",
       "  6784    2002      7   259  DET  ...     0.0              NaN      NaN       NaN\n",
       "  6785    2002      7   260  BUF  ...     0.0             20.0      NaN       NaN\n",
       "  6786    2002      7   261  HOU  ...     NaN              NaN      NaN       NaN\n",
       "  \n",
       "  [261 rows x 36 columns],\n",
       "  19:      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       "  0       Sam Bradford      1  ...           LA                      4.0\n",
       "  1          Tim Tebow      1  ...          DEN                      2.0\n",
       "  2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       "  3         Colt McCoy      3  ...          CLE                      3.0\n",
       "  4         Mike Kafka      4  ...          PHI                      1.0\n",
       "  ..               ...    ...  ...          ...                      ...\n",
       "  144    C.J. Beathard      3  ...          NaN                      NaN\n",
       "  145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       "  146  Nathan Peterman      5  ...          BUF                      2.0\n",
       "  147       Brad Kaaya      6  ...          NaN                      NaN\n",
       "  148       Chad Kelly      7  ...          DEN                      1.0\n",
       "  \n",
       "  [149 rows x 22 columns],\n",
       "  20:      round  pick  season\n",
       "  0        1     1    2010\n",
       "  1        1    25    2010\n",
       "  2        2    48    2010\n",
       "  3        3    85    2010\n",
       "  4        4   122    2010\n",
       "  ..     ...   ...     ...\n",
       "  144      3   104    2017\n",
       "  145      4   135    2017\n",
       "  146      5   171    2017\n",
       "  147      6   215    2017\n",
       "  148      7   253    2017\n",
       "  \n",
       "  [149 rows x 3 columns],\n",
       "  21:         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "  144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       "  145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "  146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "  147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       "  148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "  \n",
       "  [149 rows x 16 columns],\n",
       "  23:         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "  144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       "  145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "  146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "  147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       "  148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "  \n",
       "  [149 rows x 16 columns],\n",
       "  25:         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "  142  25.0   423.0   696.0  60.8  ...  232.4  147.7      2.0                      1.0\n",
       "  143  23.0   459.0   747.0  61.4  ...  241.6  138.4      7.0                      1.0\n",
       "  145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "  146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "  148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "  \n",
       "  [108 rows x 16 columns],\n",
       "  26:       G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0  31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1  55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2  35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3  53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4  30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  \n",
       "  [5 rows x 16 columns],\n",
       "  31: LogisticRegression(),\n",
       "  32: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0]),\n",
       "  34: array([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0]),\n",
       "  35: 0.6666666666666666,\n",
       "  36: 0.6666666666666666,\n",
       "  37: 0.5185185185185185,\n",
       "  39: 0.7037037037037037,\n",
       "  40: 0.7407407407407407,\n",
       "  42: 0.5,\n",
       "  45: 0.41428571428571426,\n",
       "  46: 0.5857142857142856,\n",
       "  47: 0.41428571428571426,\n",
       "  49: 0.4392857142857143,\n",
       "  51: array([0.32, 0.23, 0.21, 0.05, 0.1 , 0.48, 0.16, 0.19, 0.45, 0.08, 0.37,\n",
       "         0.58, 0.36, 0.22, 0.37, 0.32, 0.2 , 0.21, 0.57, 0.59, 0.42, 0.17,\n",
       "         0.29, 0.07, 0.41, 0.12, 0.1 ]),\n",
       "  52: 0.6296296296296297,\n",
       "  54: 0.7407407407407407,\n",
       "  55: <function sklearn.metrics._ranking.roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)>,\n",
       "  56: 0.375,\n",
       "  57: 0.3571428571428571,\n",
       "  58: 0.5555555555555556,\n",
       "  61: 0.7407407407407407,\n",
       "  62: 0.5857142857142856,\n",
       "  64: 0.5357142857142857,\n",
       "  67: 0.4357142857142857,\n",
       "  70: 0.6000000000000001,\n",
       "  71: 0.7407407407407407,\n",
       "  74: 0.40714285714285714,\n",
       "  76: 0.5357142857142857,\n",
       "  77: 0.5785714285714285,\n",
       "  78: 0.7407407407407407,\n",
       "  79: 0.7407407407407407,\n",
       "  82:         G    Cmp     Att  Cmp%      Yds    TD   TD%   Int  Int%   Y/A\n",
       "  31    1.0    5.0     8.0  62.5     57.0   0.0   0.0   0.0   0.0   7.1\n",
       "  53   18.0  192.0   368.0  52.2   2732.0  16.0   4.3  10.0   2.7   7.4\n",
       "  97   29.0  552.0   813.0  67.9   7386.0  69.0   8.5  15.0   1.8   9.1\n",
       "  138  44.0  759.0  1166.0  65.1  10514.0  93.0   8.0  25.0   2.1   9.0\n",
       "  30   14.0  225.0   309.0  72.8   3175.0  33.0  10.7   4.0   1.3  10.3\n",
       "  ..    ...    ...     ...   ...      ...   ...   ...   ...   ...   ...\n",
       "  61   27.0  562.0   851.0  66.0   7964.0  65.0   7.6  28.0   3.3   9.4\n",
       "  126   8.0  218.0   340.0  64.1   2315.0  12.0   3.5   9.0   2.6   6.8\n",
       "  95   22.0  396.0   579.0  68.4   5373.0  63.0  10.9   9.0   1.6   9.3\n",
       "  146  10.0   20.0    43.0  46.5     94.0   0.0   0.0   2.0   4.7   2.2\n",
       "  81   13.0  281.0   450.0  62.4   4033.0  27.0   6.0   8.0   1.8   9.0\n",
       "  \n",
       "  [81 rows x 10 columns],\n",
       "  87:     pfr_player_name  round  pick  ...  seasons            name  recent_team\n",
       "  120     Ryan Finley      4   104  ...      8.0     Ryan Finley          CIN\n",
       "  28   Brandon Weeden      1    22  ...      4.0  Brandon Weeden          CLE\n",
       "  99      Davis Mills      3    67  ...      3.0     Davis Mills          HOU\n",
       "  69     Carson Wentz      1     2  ...      NaN             NaN          NaN\n",
       "  131   Mason Rudolph      3    76  ...      4.0   Mason Rudolph          PIT\n",
       "  ..              ...    ...   ...  ...      ...             ...          ...\n",
       "  129      Josh Rosen      1    10  ...      3.0      Josh Rosen          ARI\n",
       "  136    Danny Etling      7   219  ...      8.0    Danny Etling          NaN\n",
       "  1         Tim Tebow      1    25  ...      4.0       Tim Tebow          DEN\n",
       "  3        Colt McCoy      3    85  ...      4.0      Colt McCoy          CLE\n",
       "  96        Mac Jones      1    15  ...      3.0       Mac Jones           NE\n",
       "  \n",
       "  [111 rows x 21 columns],\n",
       "  92: 0.7777777777777778,\n",
       "  93: 0.7285714285714285,\n",
       "  95: 0.7714285714285714,\n",
       "  96: 0.7777777777777778,\n",
       "  97: 0.7714285714285714,\n",
       "  98: 0.7777777777777778,\n",
       "  116:         G    Cmp     Att  Cmp%      Yds  ...   AY/A   Y/C    Y/G   Rate  seasons\n",
       "  2    35.0  695.0  1110.0  62.6   8148.0  ...   7.33  11.7  232.8  137.2      3.0\n",
       "  31    1.0    5.0     8.0  62.5     57.0  ...   7.13  11.4   57.0  122.4      8.0\n",
       "  17   50.0  812.0  1317.0  61.7  10314.0  ...   7.88  12.7  206.3  140.7      4.0\n",
       "  100  48.0  728.0  1141.0  63.8   8948.0  ...   8.32  12.3  186.4  147.0      5.0\n",
       "  9    27.0  421.0   682.0  61.7   5018.0  ...   7.48  11.9  185.9  141.4      3.0\n",
       "  ..    ...    ...     ...   ...      ...  ...    ...   ...    ...    ...      ...\n",
       "  14   40.0  619.0  1147.0  54.0   7639.0  ...   6.21  12.3  191.0  119.1      4.0\n",
       "  138  44.0  759.0  1166.0  65.1  10514.0  ...   9.65  13.9  239.0  162.9      5.0\n",
       "  4    30.0  408.0   637.0  64.1   4265.0  ...   5.88  10.5  142.2  123.9      4.0\n",
       "  103  33.0  474.0   684.0  69.3   7442.0  ...  12.70  15.7  225.5  199.4      3.0\n",
       "  70   38.0  758.0  1205.0  62.9   8863.0  ...   7.48  11.7  233.2  137.0      3.0\n",
       "  \n",
       "  [81 rows x 15 columns],\n",
       "  117: 2      0\n",
       "  31     1\n",
       "  17     1\n",
       "  100    0\n",
       "  9      0\n",
       "        ..\n",
       "  14     1\n",
       "  138    0\n",
       "  4      0\n",
       "  103    0\n",
       "  70     0\n",
       "  Name: seasons_with_draft_team, Length: 81, dtype: int64,\n",
       "  118: 0,\n",
       "  119: 0,\n",
       "  122: (81,),\n",
       "  123: (81, 15),\n",
       "  124: <Sequential name=sequential_6, built=True>,\n",
       "  125:         G    Cmp     Att  Cmp%      Yds  ...   AY/A   Y/C    Y/G   Rate  seasons\n",
       "  2    35.0  695.0  1110.0  62.6   8148.0  ...   7.33  11.7  232.8  137.2      3.0\n",
       "  31    1.0    5.0     8.0  62.5     57.0  ...   7.13  11.4   57.0  122.4      8.0\n",
       "  17   50.0  812.0  1317.0  61.7  10314.0  ...   7.88  12.7  206.3  140.7      4.0\n",
       "  100  48.0  728.0  1141.0  63.8   8948.0  ...   8.32  12.3  186.4  147.0      5.0\n",
       "  9    27.0  421.0   682.0  61.7   5018.0  ...   7.48  11.9  185.9  141.4      3.0\n",
       "  ..    ...    ...     ...   ...      ...  ...    ...   ...    ...    ...      ...\n",
       "  14   40.0  619.0  1147.0  54.0   7639.0  ...   6.21  12.3  191.0  119.1      4.0\n",
       "  138  44.0  759.0  1166.0  65.1  10514.0  ...   9.65  13.9  239.0  162.9      5.0\n",
       "  4    30.0  408.0   637.0  64.1   4265.0  ...   5.88  10.5  142.2  123.9      4.0\n",
       "  103  33.0  474.0   684.0  69.3   7442.0  ...  12.70  15.7  225.5  199.4      3.0\n",
       "  70   38.0  758.0  1205.0  62.9   8863.0  ...   7.48  11.7  233.2  137.0      3.0\n",
       "  \n",
       "  [81 rows x 15 columns],\n",
       "  128: 15,\n",
       "  129: <keras.src.callbacks.history.History at 0x313944250>,\n",
       "  136: 0.6714285714285714,\n",
       "  137: 0.5571428571428572,\n",
       "  138: 0.5142857142857143,\n",
       "  139: 0.6178571428571428,\n",
       "  141: 1.0,\n",
       "  142: 0.5,\n",
       "  144: 0.55,\n",
       "  146: 0.7407407407407407,\n",
       "  154: 0.7407407407407407,\n",
       "  155: 0.42142857142857143,\n",
       "  156: 0.7407407407407407,\n",
       "  157: 0.7407407407407407,\n",
       "  158: 0.49999999999999994,\n",
       "  159: 0.7407407407407407,\n",
       "  162: 0.8148148148148148,\n",
       "  163: 0.8428571428571429,\n",
       "  201:    accuracy        f1   roc_auc\n",
       "  0  0.685185  0.105263  0.427324,\n",
       "  205:    accuracy   f1   roc_auc\n",
       "  0  0.685185  0.0  0.390223,\n",
       "  211:    accuracy   f1   roc_auc\n",
       "  0  0.583333  0.0  0.326931,\n",
       "  214:    accuracy   f1   roc_auc\n",
       "  0  0.666667  0.1  0.404409,\n",
       "  215:      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       "  0       Sam Bradford      1  ...           LA                      4.0\n",
       "  1          Tim Tebow      1  ...          DEN                      2.0\n",
       "  2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       "  3         Colt McCoy      3  ...          CLE                      3.0\n",
       "  4         Mike Kafka      4  ...          PHI                      1.0\n",
       "  ..               ...    ...  ...          ...                      ...\n",
       "  142    DeShone Kizer      2  ...          CLE                      1.0\n",
       "  143       Davis Webb      3  ...          BUF                      1.0\n",
       "  145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       "  146  Nathan Peterman      5  ...          BUF                      2.0\n",
       "  148       Chad Kelly      7  ...          DEN                      1.0\n",
       "  \n",
       "  [108 rows x 22 columns],\n",
       "  219:    accuracy        f1   roc_auc\n",
       "  0  0.694444  0.108108  0.428634,\n",
       "  221: 0.7928571428571428,\n",
       "  225:    accuracy        f1   roc_auc\n",
       "  0  0.694444  0.108108  0.428634,\n",
       "  226:      pfr_player_name  round  pick  ...  seasons             name  recent_team\n",
       "  102       Joe Burrow      1     1  ...      7.0       Joe Burrow          CIN\n",
       "  97        Kyle Trask      2    64  ...      5.0       Kyle Trask           TB\n",
       "  105      Jordan Love      1    26  ...      3.0      Jordan Love           GB\n",
       "  146  Nathan Peterman      5   171  ...      7.0  Nathan Peterman          BUF\n",
       "  119       Will Grier      3   100  ...      7.0       Will Grier          CAR\n",
       "  ..               ...    ...   ...  ...      ...              ...          ...\n",
       "  61    Jameis Winston      1     1  ...      2.0   Jameis Winston           TB\n",
       "  31        Nick Foles      3    88  ...      8.0       Nick Foles          PHI\n",
       "  13        Cam Newton      1     1  ...      7.0       Cam Newton          CAR\n",
       "  62    Marcus Mariota      1     2  ...      3.0   Marcus Mariota          TEN\n",
       "  112      Ben DiNucci      7   231  ...      2.0      Ben DiNucci          DAL\n",
       "  \n",
       "  [81 rows x 21 columns],\n",
       "  238:    accuracy        f1  roc_auc\n",
       "  0  0.703704  0.428571   0.7189,\n",
       "  242:    accuracy        f1  roc_auc\n",
       "  0  0.657407  0.350877  0.68529,\n",
       "  249:    accuracy        f1   roc_auc\n",
       "  0   0.62037  0.349206  0.666085,\n",
       "  259:    accuracy        f1   roc_auc\n",
       "  0  0.851852  0.666667  0.921429,\n",
       "  265: Exception,\n",
       "  266: ValueError,\n",
       "  267: <function compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1, *, _feature_version=-1)>,\n",
       "  268: {...},\n",
       "  269: 427,\n",
       "  271: {...}},\n",
       " '_dh': [PosixPath('/Users/benstager/Desktop/mfl_project/mfl/model_dev')],\n",
       " 'In': ['',\n",
       "  'import mfl as mfl',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np',\n",
       "  'df = mfl.api.data_loaders.load_qb_data_cleaned()',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata',\n",
       "  'df = mfldata.load_qb_data_cleaned()',\n",
       "  'df',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.ensemble',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom keras.layers import Dense',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom keras.layers import Dense',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Sigmoid',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  'nfl.import_draft_picks()',\n",
       "  'nfl.import_draft_picks(years=2002)',\n",
       "  'nfl.import_draft_picks(years=[2002])',\n",
       "  'df',\n",
       "  \"df.select_dtypes(include='int')\",\n",
       "  \"df.select_dtypes(include='float')\",\n",
       "  \"numerics_only = df.select_dtypes(include='float')\",\n",
       "  'numerics_only',\n",
       "  \"numerics_only = df.select_dtypes(include='float').dropna()\",\n",
       "  'numerics_only',\n",
       "  'numerics_only.head(5)',\n",
       "  'def map_response(x):\\n    if x >= 4:\\n        return 1\\n    else: \\n        return 0',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = LogisticRegression()\\nmodel.fit(X_train, y_train)',\n",
       "  'model = LogisticRegression()\\nmodel.fit(X_train, y_train)\\nmodel.predict(X_test)',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\nmodel.predict(X_test)',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = RandomForestClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = XGBClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = LogisticRegression(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = LogisticRegression()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_tste)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,0]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = RandomForestClassifier\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'y_probss',\n",
       "  'y_probs',\n",
       "  'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_preds)',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC(probability=1)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:5]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:4]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:4]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:8]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:8]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:9]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:9]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:10]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = CatBoostClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       "  'model = CatBoostClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  'model = CatBoostClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  'X_train',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  'X_train',\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  'df = df.dropna()',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(iterations=300, n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional\\nfrom keras.models import Sequential',\n",
       "  'model = Sequential()',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense()\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(units=df.shape[0])\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(units=df.shape[1])\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=df.shape[1])\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=df.shape[1], units=64)\\n)',\n",
       "  'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=tuple(df.shape[1]), units=64)\\n)',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization\\nfrom keras.models import Sequential',\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout\\nfrom keras.models import Sequential',\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)=\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'X_train',\n",
       "  'y_train',\n",
       "  'y_train.isna().sum(0)',\n",
       "  'y_train.isna().sum()',\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(ReLU(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'y_train.shape',\n",
       "  'X_train.shape',\n",
       "  'model',\n",
       "  'X_train',\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'X_train.shape[1]',\n",
       "  \"model = Sequential()\\nmodel.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\\nfrom keras.models import Sequential',\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=X_train.shape[1]))\\nmodel.add(Dense(units=32activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=X_train.shape[1]))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(,X_train.shape[1])))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\n\\nroc_auc_score()\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_preds, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\ny_probs(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='roc-auc')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='roc_auc')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='SparseCategoricalCrossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\\ny_preds_NN = model.predict(X_test)\\ny_preds_NN = (y_preds_NN >= .5).astype(int)\",\n",
       "  \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\n\\n\\nmodel.fit(X_train, y_train)\\ny_preds_NN = model.predict(X_test)\\ny_preds_NN = (y_preds_NN >= .5).astype(int)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\\nfrom keras.models import Sequential',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        \",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n        model = model(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n        model = model(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        self.meric_dict = metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        self.model_results = pd.DataFrame(self.metric_dict)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        self.meric_dict = metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        self.model_results = pd.DataFrame(self.metric_dict)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  'NFL = FranchiseQB()',\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='lr')\",\n",
       "  \"NFL = FranchiseQB(model='svc')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='svm')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='xgb')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"NFL = FranchiseQB(model='rf')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  'df',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        if self.model == 'catboost':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=X.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       "  \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        if self.model == 'catboost':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=X.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  'X_train',\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat'\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=3):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=1):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(X.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       X['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend((model.predict_proba(X_test)[:,1]))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       "  \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       "  'NFL.run()',\n",
       "  'NFL.model_results',\n",
       "  'import pydantic',\n",
       "  'from pydantic import BaseModel',\n",
       "  'from pydantic import BaseModel\\n\\nclass MyFirstModel(BaseModel):\\n    first_name: str\\n    last_name: str\\n\\nvalidating = MyFirstModel(first_name=\"marc\", last_name=\"nealer\")',\n",
       "  \"MyFirstModel(first_name='Ben', last_name=0)\",\n",
       "  'from Exception import RuntimeError',\n",
       "  'Exception',\n",
       "  'ValueError',\n",
       "  'compile',\n",
       "  'globals()',\n",
       "  'globals().__len__()',\n",
       "  'globals()[9]',\n",
       "  'locals()',\n",
       "  'vars()'],\n",
       " 'Out': {6:      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       "  0       Sam Bradford      1  ...           LA                      4.0\n",
       "  1          Tim Tebow      1  ...          DEN                      2.0\n",
       "  2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       "  3         Colt McCoy      3  ...          CLE                      3.0\n",
       "  4         Mike Kafka      4  ...          PHI                      1.0\n",
       "  ..               ...    ...  ...          ...                      ...\n",
       "  144    C.J. Beathard      3  ...          NaN                      NaN\n",
       "  145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       "  146  Nathan Peterman      5  ...          BUF                      2.0\n",
       "  147       Brad Kaaya      6  ...          NaN                      NaN\n",
       "  148       Chad Kelly      7  ...          DEN                      1.0\n",
       "  \n",
       "  [149 rows x 22 columns],\n",
       "  18:       season  round  pick team  ... rec_tds def_solo_tackles def_ints def_sacks\n",
       "  6526    2002      1     1  HOU  ...     0.0              NaN      NaN       NaN\n",
       "  6527    2002      1     2  CAR  ...     0.0            557.0     11.0     159.5\n",
       "  6528    2002      1     3  DET  ...     0.0              NaN      NaN       NaN\n",
       "  6529    2002      1     4  BUF  ...     0.0              NaN      NaN       NaN\n",
       "  6530    2002      1     5  SDG  ...     0.0            631.0     21.0       NaN\n",
       "  ...      ...    ...   ...  ...  ...     ...              ...      ...       ...\n",
       "  6782    2002      7   257  WAS  ...     2.0             74.0      NaN       NaN\n",
       "  6783    2002      7   258  CAR  ...     0.0              3.0      NaN       NaN\n",
       "  6784    2002      7   259  DET  ...     0.0              NaN      NaN       NaN\n",
       "  6785    2002      7   260  BUF  ...     0.0             20.0      NaN       NaN\n",
       "  6786    2002      7   261  HOU  ...     NaN              NaN      NaN       NaN\n",
       "  \n",
       "  [261 rows x 36 columns],\n",
       "  19:      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       "  0       Sam Bradford      1  ...           LA                      4.0\n",
       "  1          Tim Tebow      1  ...          DEN                      2.0\n",
       "  2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       "  3         Colt McCoy      3  ...          CLE                      3.0\n",
       "  4         Mike Kafka      4  ...          PHI                      1.0\n",
       "  ..               ...    ...  ...          ...                      ...\n",
       "  144    C.J. Beathard      3  ...          NaN                      NaN\n",
       "  145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       "  146  Nathan Peterman      5  ...          BUF                      2.0\n",
       "  147       Brad Kaaya      6  ...          NaN                      NaN\n",
       "  148       Chad Kelly      7  ...          DEN                      1.0\n",
       "  \n",
       "  [149 rows x 22 columns],\n",
       "  20:      round  pick  season\n",
       "  0        1     1    2010\n",
       "  1        1    25    2010\n",
       "  2        2    48    2010\n",
       "  3        3    85    2010\n",
       "  4        4   122    2010\n",
       "  ..     ...   ...     ...\n",
       "  144      3   104    2017\n",
       "  145      4   135    2017\n",
       "  146      5   171    2017\n",
       "  147      6   215    2017\n",
       "  148      7   253    2017\n",
       "  \n",
       "  [149 rows x 3 columns],\n",
       "  21:         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "  144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       "  145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "  146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "  147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       "  148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "  \n",
       "  [149 rows x 16 columns],\n",
       "  23:         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "  144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       "  145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "  146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "  147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       "  148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "  \n",
       "  [149 rows x 16 columns],\n",
       "  25:         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       "  142  25.0   423.0   696.0  60.8  ...  232.4  147.7      2.0                      1.0\n",
       "  143  23.0   459.0   747.0  61.4  ...  241.6  138.4      7.0                      1.0\n",
       "  145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       "  146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       "  148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       "  \n",
       "  [108 rows x 16 columns],\n",
       "  26:       G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       "  0  31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       "  1  55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       "  2  35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       "  3  53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       "  4  30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       "  \n",
       "  [5 rows x 16 columns],\n",
       "  31: LogisticRegression(),\n",
       "  32: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0]),\n",
       "  34: array([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0]),\n",
       "  35: 0.6666666666666666,\n",
       "  36: 0.6666666666666666,\n",
       "  37: 0.5185185185185185,\n",
       "  39: 0.7037037037037037,\n",
       "  40: 0.7407407407407407,\n",
       "  42: 0.5,\n",
       "  45: 0.41428571428571426,\n",
       "  46: 0.5857142857142856,\n",
       "  47: 0.41428571428571426,\n",
       "  49: 0.4392857142857143,\n",
       "  51: array([0.32, 0.23, 0.21, 0.05, 0.1 , 0.48, 0.16, 0.19, 0.45, 0.08, 0.37,\n",
       "         0.58, 0.36, 0.22, 0.37, 0.32, 0.2 , 0.21, 0.57, 0.59, 0.42, 0.17,\n",
       "         0.29, 0.07, 0.41, 0.12, 0.1 ]),\n",
       "  52: 0.6296296296296297,\n",
       "  54: 0.7407407407407407,\n",
       "  55: <function sklearn.metrics._ranking.roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)>,\n",
       "  56: 0.375,\n",
       "  57: 0.3571428571428571,\n",
       "  58: 0.5555555555555556,\n",
       "  61: 0.7407407407407407,\n",
       "  62: 0.5857142857142856,\n",
       "  64: 0.5357142857142857,\n",
       "  67: 0.4357142857142857,\n",
       "  70: 0.6000000000000001,\n",
       "  71: 0.7407407407407407,\n",
       "  74: 0.40714285714285714,\n",
       "  76: 0.5357142857142857,\n",
       "  77: 0.5785714285714285,\n",
       "  78: 0.7407407407407407,\n",
       "  79: 0.7407407407407407,\n",
       "  82:         G    Cmp     Att  Cmp%      Yds    TD   TD%   Int  Int%   Y/A\n",
       "  31    1.0    5.0     8.0  62.5     57.0   0.0   0.0   0.0   0.0   7.1\n",
       "  53   18.0  192.0   368.0  52.2   2732.0  16.0   4.3  10.0   2.7   7.4\n",
       "  97   29.0  552.0   813.0  67.9   7386.0  69.0   8.5  15.0   1.8   9.1\n",
       "  138  44.0  759.0  1166.0  65.1  10514.0  93.0   8.0  25.0   2.1   9.0\n",
       "  30   14.0  225.0   309.0  72.8   3175.0  33.0  10.7   4.0   1.3  10.3\n",
       "  ..    ...    ...     ...   ...      ...   ...   ...   ...   ...   ...\n",
       "  61   27.0  562.0   851.0  66.0   7964.0  65.0   7.6  28.0   3.3   9.4\n",
       "  126   8.0  218.0   340.0  64.1   2315.0  12.0   3.5   9.0   2.6   6.8\n",
       "  95   22.0  396.0   579.0  68.4   5373.0  63.0  10.9   9.0   1.6   9.3\n",
       "  146  10.0   20.0    43.0  46.5     94.0   0.0   0.0   2.0   4.7   2.2\n",
       "  81   13.0  281.0   450.0  62.4   4033.0  27.0   6.0   8.0   1.8   9.0\n",
       "  \n",
       "  [81 rows x 10 columns],\n",
       "  87:     pfr_player_name  round  pick  ...  seasons            name  recent_team\n",
       "  120     Ryan Finley      4   104  ...      8.0     Ryan Finley          CIN\n",
       "  28   Brandon Weeden      1    22  ...      4.0  Brandon Weeden          CLE\n",
       "  99      Davis Mills      3    67  ...      3.0     Davis Mills          HOU\n",
       "  69     Carson Wentz      1     2  ...      NaN             NaN          NaN\n",
       "  131   Mason Rudolph      3    76  ...      4.0   Mason Rudolph          PIT\n",
       "  ..              ...    ...   ...  ...      ...             ...          ...\n",
       "  129      Josh Rosen      1    10  ...      3.0      Josh Rosen          ARI\n",
       "  136    Danny Etling      7   219  ...      8.0    Danny Etling          NaN\n",
       "  1         Tim Tebow      1    25  ...      4.0       Tim Tebow          DEN\n",
       "  3        Colt McCoy      3    85  ...      4.0      Colt McCoy          CLE\n",
       "  96        Mac Jones      1    15  ...      3.0       Mac Jones           NE\n",
       "  \n",
       "  [111 rows x 21 columns],\n",
       "  92: 0.7777777777777778,\n",
       "  93: 0.7285714285714285,\n",
       "  95: 0.7714285714285714,\n",
       "  96: 0.7777777777777778,\n",
       "  97: 0.7714285714285714,\n",
       "  98: 0.7777777777777778,\n",
       "  116:         G    Cmp     Att  Cmp%      Yds  ...   AY/A   Y/C    Y/G   Rate  seasons\n",
       "  2    35.0  695.0  1110.0  62.6   8148.0  ...   7.33  11.7  232.8  137.2      3.0\n",
       "  31    1.0    5.0     8.0  62.5     57.0  ...   7.13  11.4   57.0  122.4      8.0\n",
       "  17   50.0  812.0  1317.0  61.7  10314.0  ...   7.88  12.7  206.3  140.7      4.0\n",
       "  100  48.0  728.0  1141.0  63.8   8948.0  ...   8.32  12.3  186.4  147.0      5.0\n",
       "  9    27.0  421.0   682.0  61.7   5018.0  ...   7.48  11.9  185.9  141.4      3.0\n",
       "  ..    ...    ...     ...   ...      ...  ...    ...   ...    ...    ...      ...\n",
       "  14   40.0  619.0  1147.0  54.0   7639.0  ...   6.21  12.3  191.0  119.1      4.0\n",
       "  138  44.0  759.0  1166.0  65.1  10514.0  ...   9.65  13.9  239.0  162.9      5.0\n",
       "  4    30.0  408.0   637.0  64.1   4265.0  ...   5.88  10.5  142.2  123.9      4.0\n",
       "  103  33.0  474.0   684.0  69.3   7442.0  ...  12.70  15.7  225.5  199.4      3.0\n",
       "  70   38.0  758.0  1205.0  62.9   8863.0  ...   7.48  11.7  233.2  137.0      3.0\n",
       "  \n",
       "  [81 rows x 15 columns],\n",
       "  117: 2      0\n",
       "  31     1\n",
       "  17     1\n",
       "  100    0\n",
       "  9      0\n",
       "        ..\n",
       "  14     1\n",
       "  138    0\n",
       "  4      0\n",
       "  103    0\n",
       "  70     0\n",
       "  Name: seasons_with_draft_team, Length: 81, dtype: int64,\n",
       "  118: 0,\n",
       "  119: 0,\n",
       "  122: (81,),\n",
       "  123: (81, 15),\n",
       "  124: <Sequential name=sequential_6, built=True>,\n",
       "  125:         G    Cmp     Att  Cmp%      Yds  ...   AY/A   Y/C    Y/G   Rate  seasons\n",
       "  2    35.0  695.0  1110.0  62.6   8148.0  ...   7.33  11.7  232.8  137.2      3.0\n",
       "  31    1.0    5.0     8.0  62.5     57.0  ...   7.13  11.4   57.0  122.4      8.0\n",
       "  17   50.0  812.0  1317.0  61.7  10314.0  ...   7.88  12.7  206.3  140.7      4.0\n",
       "  100  48.0  728.0  1141.0  63.8   8948.0  ...   8.32  12.3  186.4  147.0      5.0\n",
       "  9    27.0  421.0   682.0  61.7   5018.0  ...   7.48  11.9  185.9  141.4      3.0\n",
       "  ..    ...    ...     ...   ...      ...  ...    ...   ...    ...    ...      ...\n",
       "  14   40.0  619.0  1147.0  54.0   7639.0  ...   6.21  12.3  191.0  119.1      4.0\n",
       "  138  44.0  759.0  1166.0  65.1  10514.0  ...   9.65  13.9  239.0  162.9      5.0\n",
       "  4    30.0  408.0   637.0  64.1   4265.0  ...   5.88  10.5  142.2  123.9      4.0\n",
       "  103  33.0  474.0   684.0  69.3   7442.0  ...  12.70  15.7  225.5  199.4      3.0\n",
       "  70   38.0  758.0  1205.0  62.9   8863.0  ...   7.48  11.7  233.2  137.0      3.0\n",
       "  \n",
       "  [81 rows x 15 columns],\n",
       "  128: 15,\n",
       "  129: <keras.src.callbacks.history.History at 0x313944250>,\n",
       "  136: 0.6714285714285714,\n",
       "  137: 0.5571428571428572,\n",
       "  138: 0.5142857142857143,\n",
       "  139: 0.6178571428571428,\n",
       "  141: 1.0,\n",
       "  142: 0.5,\n",
       "  144: 0.55,\n",
       "  146: 0.7407407407407407,\n",
       "  154: 0.7407407407407407,\n",
       "  155: 0.42142857142857143,\n",
       "  156: 0.7407407407407407,\n",
       "  157: 0.7407407407407407,\n",
       "  158: 0.49999999999999994,\n",
       "  159: 0.7407407407407407,\n",
       "  162: 0.8148148148148148,\n",
       "  163: 0.8428571428571429,\n",
       "  201:    accuracy        f1   roc_auc\n",
       "  0  0.685185  0.105263  0.427324,\n",
       "  205:    accuracy   f1   roc_auc\n",
       "  0  0.685185  0.0  0.390223,\n",
       "  211:    accuracy   f1   roc_auc\n",
       "  0  0.583333  0.0  0.326931,\n",
       "  214:    accuracy   f1   roc_auc\n",
       "  0  0.666667  0.1  0.404409,\n",
       "  215:      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       "  0       Sam Bradford      1  ...           LA                      4.0\n",
       "  1          Tim Tebow      1  ...          DEN                      2.0\n",
       "  2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       "  3         Colt McCoy      3  ...          CLE                      3.0\n",
       "  4         Mike Kafka      4  ...          PHI                      1.0\n",
       "  ..               ...    ...  ...          ...                      ...\n",
       "  142    DeShone Kizer      2  ...          CLE                      1.0\n",
       "  143       Davis Webb      3  ...          BUF                      1.0\n",
       "  145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       "  146  Nathan Peterman      5  ...          BUF                      2.0\n",
       "  148       Chad Kelly      7  ...          DEN                      1.0\n",
       "  \n",
       "  [108 rows x 22 columns],\n",
       "  219:    accuracy        f1   roc_auc\n",
       "  0  0.694444  0.108108  0.428634,\n",
       "  221: 0.7928571428571428,\n",
       "  225:    accuracy        f1   roc_auc\n",
       "  0  0.694444  0.108108  0.428634,\n",
       "  226:      pfr_player_name  round  pick  ...  seasons             name  recent_team\n",
       "  102       Joe Burrow      1     1  ...      7.0       Joe Burrow          CIN\n",
       "  97        Kyle Trask      2    64  ...      5.0       Kyle Trask           TB\n",
       "  105      Jordan Love      1    26  ...      3.0      Jordan Love           GB\n",
       "  146  Nathan Peterman      5   171  ...      7.0  Nathan Peterman          BUF\n",
       "  119       Will Grier      3   100  ...      7.0       Will Grier          CAR\n",
       "  ..               ...    ...   ...  ...      ...              ...          ...\n",
       "  61    Jameis Winston      1     1  ...      2.0   Jameis Winston           TB\n",
       "  31        Nick Foles      3    88  ...      8.0       Nick Foles          PHI\n",
       "  13        Cam Newton      1     1  ...      7.0       Cam Newton          CAR\n",
       "  62    Marcus Mariota      1     2  ...      3.0   Marcus Mariota          TEN\n",
       "  112      Ben DiNucci      7   231  ...      2.0      Ben DiNucci          DAL\n",
       "  \n",
       "  [81 rows x 21 columns],\n",
       "  238:    accuracy        f1  roc_auc\n",
       "  0  0.703704  0.428571   0.7189,\n",
       "  242:    accuracy        f1  roc_auc\n",
       "  0  0.657407  0.350877  0.68529,\n",
       "  249:    accuracy        f1   roc_auc\n",
       "  0   0.62037  0.349206  0.666085,\n",
       "  259:    accuracy        f1   roc_auc\n",
       "  0  0.851852  0.666667  0.921429,\n",
       "  265: Exception,\n",
       "  266: ValueError,\n",
       "  267: <function compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1, *, _feature_version=-1)>,\n",
       "  268: {...},\n",
       "  269: 427,\n",
       "  271: {...}},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x103f96890>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x103fa2cd0>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x103fa2cd0>,\n",
       " 'open': <function io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)>,\n",
       " '_': {...},\n",
       " '__': 427,\n",
       " '___': {...},\n",
       " '__vsc_ipynb_file__': '/Users/benstager/Desktop/mfl_project/mfl/model_dev/model_dev_1_2.ipynb',\n",
       " '_i': 'locals()',\n",
       " '_ii': 'globals()[9]',\n",
       " '_iii': 'globals().__len__()',\n",
       " '_i1': 'import mfl as mfl',\n",
       " 'mfl': <module 'mfl' from '/Users/benstager/Desktop/mfl_project/mfl/__init__.py'>,\n",
       " '_i2': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np',\n",
       " 'pd': <module 'pandas' from '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/__init__.py'>,\n",
       " 'np': <module 'numpy' from '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/__init__.py'>,\n",
       " '_i3': 'df = mfl.api.data_loaders.load_qb_data_cleaned()',\n",
       " '_i4': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata',\n",
       " 'mfldata': <module 'mfl.api.data_loaders' from '/Users/benstager/Desktop/mfl_project/mfl/api/data_loaders.py'>,\n",
       " '_i5': 'df = mfldata.load_qb_data_cleaned()',\n",
       " 'df':      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       " 0       Sam Bradford      1  ...           LA                      4.0\n",
       " 1          Tim Tebow      1  ...          DEN                      2.0\n",
       " 2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       " 3         Colt McCoy      3  ...          CLE                      3.0\n",
       " 4         Mike Kafka      4  ...          PHI                      1.0\n",
       " ..               ...    ...  ...          ...                      ...\n",
       " 142    DeShone Kizer      2  ...          CLE                      1.0\n",
       " 143       Davis Webb      3  ...          BUF                      1.0\n",
       " 145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       " 146  Nathan Peterman      5  ...          BUF                      2.0\n",
       " 148       Chad Kelly      7  ...          DEN                      1.0\n",
       " \n",
       " [108 rows x 22 columns],\n",
       " '_i6': 'df',\n",
       " '_6':      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       " 0       Sam Bradford      1  ...           LA                      4.0\n",
       " 1          Tim Tebow      1  ...          DEN                      2.0\n",
       " 2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       " 3         Colt McCoy      3  ...          CLE                      3.0\n",
       " 4         Mike Kafka      4  ...          PHI                      1.0\n",
       " ..               ...    ...  ...          ...                      ...\n",
       " 144    C.J. Beathard      3  ...          NaN                      NaN\n",
       " 145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       " 146  Nathan Peterman      5  ...          BUF                      2.0\n",
       " 147       Brad Kaaya      6  ...          NaN                      NaN\n",
       " 148       Chad Kelly      7  ...          DEN                      1.0\n",
       " \n",
       " [149 rows x 22 columns],\n",
       " '_i7': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.ensemble',\n",
       " '_i8': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier',\n",
       " 'nfl': <module 'nfl_data_py' from '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nfl_data_py/__init__.py'>,\n",
       " 'LogisticRegression': sklearn.linear_model._logistic.LogisticRegression,\n",
       " 'HistGradientBoostingClassifier': sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n",
       " 'RandomForestClassifier': sklearn.ensemble._forest.RandomForestClassifier,\n",
       " 'SVC': sklearn.svm._classes.SVC,\n",
       " 'XGBClassifier': xgboost.sklearn.XGBClassifier,\n",
       " 'XGBRFClassifier': xgboost.sklearn.XGBRFClassifier,\n",
       " '_i9': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom keras.layers import Dense',\n",
       " '_i10': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras',\n",
       " 'keras': <module 'keras' from '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/__init__.py'>,\n",
       " '_i11': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom keras.layers import Dense',\n",
       " 'Dense': keras.src.layers.core.dense.Dense,\n",
       " '_i12': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU',\n",
       " 'ReLU': keras.src.layers.activations.relu.ReLU,\n",
       " '_i13': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Sigmoid',\n",
       " '_i14': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       " 'Bidirectional': keras.src.layers.rnn.bidirectional.Bidirectional,\n",
       " '_i15': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       " 'CatBoostClassifier': catboost.core.CatBoostClassifier,\n",
       " '_i16': 'nfl.import_draft_picks()',\n",
       " '_i17': 'nfl.import_draft_picks(years=2002)',\n",
       " '_i18': 'nfl.import_draft_picks(years=[2002])',\n",
       " '_18':       season  round  pick team  ... rec_tds def_solo_tackles def_ints def_sacks\n",
       " 6526    2002      1     1  HOU  ...     0.0              NaN      NaN       NaN\n",
       " 6527    2002      1     2  CAR  ...     0.0            557.0     11.0     159.5\n",
       " 6528    2002      1     3  DET  ...     0.0              NaN      NaN       NaN\n",
       " 6529    2002      1     4  BUF  ...     0.0              NaN      NaN       NaN\n",
       " 6530    2002      1     5  SDG  ...     0.0            631.0     21.0       NaN\n",
       " ...      ...    ...   ...  ...  ...     ...              ...      ...       ...\n",
       " 6782    2002      7   257  WAS  ...     2.0             74.0      NaN       NaN\n",
       " 6783    2002      7   258  CAR  ...     0.0              3.0      NaN       NaN\n",
       " 6784    2002      7   259  DET  ...     0.0              NaN      NaN       NaN\n",
       " 6785    2002      7   260  BUF  ...     0.0             20.0      NaN       NaN\n",
       " 6786    2002      7   261  HOU  ...     NaN              NaN      NaN       NaN\n",
       " \n",
       " [261 rows x 36 columns],\n",
       " '_i19': 'df',\n",
       " '_19':      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       " 0       Sam Bradford      1  ...           LA                      4.0\n",
       " 1          Tim Tebow      1  ...          DEN                      2.0\n",
       " 2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       " 3         Colt McCoy      3  ...          CLE                      3.0\n",
       " 4         Mike Kafka      4  ...          PHI                      1.0\n",
       " ..               ...    ...  ...          ...                      ...\n",
       " 144    C.J. Beathard      3  ...          NaN                      NaN\n",
       " 145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       " 146  Nathan Peterman      5  ...          BUF                      2.0\n",
       " 147       Brad Kaaya      6  ...          NaN                      NaN\n",
       " 148       Chad Kelly      7  ...          DEN                      1.0\n",
       " \n",
       " [149 rows x 22 columns],\n",
       " '_i20': \"df.select_dtypes(include='int')\",\n",
       " '_20':      round  pick  season\n",
       " 0        1     1    2010\n",
       " 1        1    25    2010\n",
       " 2        2    48    2010\n",
       " 3        3    85    2010\n",
       " 4        4   122    2010\n",
       " ..     ...   ...     ...\n",
       " 144      3   104    2017\n",
       " 145      4   135    2017\n",
       " 146      5   171    2017\n",
       " 147      6   215    2017\n",
       " 148      7   253    2017\n",
       " \n",
       " [149 rows x 3 columns],\n",
       " '_i21': \"df.select_dtypes(include='float')\",\n",
       " '_21':         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       " 0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       " 1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       " 2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       " 3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       " 4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       " ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       " 144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       " 145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       " 146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       " 147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       " 148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       " \n",
       " [149 rows x 16 columns],\n",
       " '_i22': \"numerics_only = df.select_dtypes(include='float')\",\n",
       " 'numerics_only':         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       " 0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       " 1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       " 2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       " 3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       " 4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       " ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       " 142  25.0   423.0   696.0  60.8  ...  232.4  147.7      2.0                      1.0\n",
       " 143  23.0   459.0   747.0  61.4  ...  241.6  138.4      7.0                      1.0\n",
       " 145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       " 146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       " 148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       " \n",
       " [108 rows x 16 columns],\n",
       " '_i23': 'numerics_only',\n",
       " '_23':         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       " 0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       " 1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       " 2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       " 3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       " 4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       " ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       " 144   NaN     NaN     NaN   NaN  ...    NaN    NaN      NaN                      NaN\n",
       " 145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       " 146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       " 147  38.0   721.0  1189.0  60.6  ...  262.4  146.2      3.0                      NaN\n",
       " 148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       " \n",
       " [149 rows x 16 columns],\n",
       " '_i24': \"numerics_only = df.select_dtypes(include='float').dropna()\",\n",
       " '_i25': 'numerics_only',\n",
       " '_25':         G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       " 0    31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       " 1    55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       " 2    35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       " 3    53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       " 4    30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       " ..    ...     ...     ...   ...  ...    ...    ...      ...                      ...\n",
       " 142  25.0   423.0   696.0  60.8  ...  232.4  147.7      2.0                      1.0\n",
       " 143  23.0   459.0   747.0  61.4  ...  241.6  138.4      7.0                      1.0\n",
       " 145  37.0   614.0   999.0  61.5  ...  192.9  133.2      4.0                      2.0\n",
       " 146  10.0    20.0    43.0  46.5  ...    9.4   55.6      7.0                      2.0\n",
       " 148  22.0   503.0   786.0  64.0  ...  309.1  152.3      7.0                      1.0\n",
       " \n",
       " [108 rows x 16 columns],\n",
       " '_i26': 'numerics_only.head(5)',\n",
       " '_26':       G     Cmp     Att  Cmp%  ...    Y/G   Rate  seasons  seasons_with_draft_team\n",
       " 0  31.0   604.0   893.0  67.6  ...  271.1  175.6      3.0                      4.0\n",
       " 1  55.0   661.0   995.0  66.4  ...  168.8  170.8      4.0                      2.0\n",
       " 2  35.0   695.0  1110.0  62.6  ...  232.8  137.2      3.0                      1.0\n",
       " 3  53.0  1157.0  1645.0  70.3  ...  250.1  155.0      4.0                      3.0\n",
       " 4  30.0   408.0   637.0  64.1  ...  142.2  123.9      4.0                      1.0\n",
       " \n",
       " [5 rows x 16 columns],\n",
       " '_i27': 'def map_response(x):\\n    if x >= 4:\\n        return 1\\n    else: \\n        return 0',\n",
       " 'map_response': <function __main__.map_response(x)>,\n",
       " '_i28': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i29': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       " 'train_test_split': <function sklearn.model_selection._split.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)>,\n",
       " '_i30': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " 'X_train':      pfr_player_name  round  pick  ...  seasons             name  recent_team\n",
       " 102       Joe Burrow      1     1  ...      7.0       Joe Burrow          CIN\n",
       " 97        Kyle Trask      2    64  ...      5.0       Kyle Trask           TB\n",
       " 105      Jordan Love      1    26  ...      3.0      Jordan Love           GB\n",
       " 146  Nathan Peterman      5   171  ...      7.0  Nathan Peterman          BUF\n",
       " 119       Will Grier      3   100  ...      7.0       Will Grier          CAR\n",
       " ..               ...    ...   ...  ...      ...              ...          ...\n",
       " 61    Jameis Winston      1     1  ...      2.0   Jameis Winston           TB\n",
       " 31        Nick Foles      3    88  ...      8.0       Nick Foles          PHI\n",
       " 13        Cam Newton      1     1  ...      7.0       Cam Newton          CAR\n",
       " 62    Marcus Mariota      1     2  ...      3.0   Marcus Mariota          TEN\n",
       " 112      Ben DiNucci      7   231  ...      2.0      Ben DiNucci          DAL\n",
       " \n",
       " [81 rows x 21 columns],\n",
       " 'X_test':        pfr_player_name  round  pick  ...  seasons               name  recent_team\n",
       " 9            Tony Pike      6   204  ...      3.0          Tony Pike          CAR\n",
       " 29      Brock Osweiler      2    57  ...      3.0     Brock Osweiler          DEN\n",
       " 128         Josh Allen      1     7  ...      5.0         Josh Allen          BUF\n",
       " 106        Jalen Hurts      2    53  ...      7.0        Jalen Hurts          PHI\n",
       " 33        Ryan Lindley      6   185  ...      4.0       Ryan Lindley          ARI\n",
       " 2        Jimmy Clausen      2    48  ...      3.0      Jimmy Clausen          CAR\n",
       " 56   Zach Mettenberger      6   178  ...      3.0  Zach Mettenberger          TEN\n",
       " 46        Sean Renfree      7   249  ...      4.0       Sean Renfree          ATL\n",
       " 49   Teddy Bridgewater      1    32  ...      3.0  Teddy Bridgewater          MIN\n",
       " 77         Kevin Hogan      5   162  ...      4.0        Kevin Hogan          CLE\n",
       " 109         Jake Fromm      5   167  ...      3.0         Jake Fromm          NYG\n",
       " 103     Tua Tagovailoa      1     5  ...      3.0     Tua Tagovailoa          MIA\n",
       " 73        Cody Kessler      3    93  ...      4.0       Cody Kessler          CLE\n",
       " 0         Sam Bradford      1     1  ...      3.0       Sam Bradford           LA\n",
       " 7          Rusty Smith      6   176  ...      4.0        Rusty Smith          TEN\n",
       " 116       Daniel Jones      1     6  ...      3.0       Daniel Jones          NYG\n",
       " 15      Blaine Gabbert      1    10  ...      3.0     Blaine Gabbert          JAX\n",
       " 95       Justin Fields      1    11  ...      6.0      Justin Fields          CHI\n",
       " 70        Paxton Lynch      1    26  ...      3.0       Paxton Lynch          DEN\n",
       " 50          Derek Carr      2    36  ...      5.0         Derek Carr           LV\n",
       " 52        Logan Thomas      4   120  ...      4.0       Logan Thomas          ARI\n",
       " 93         Zach Wilson      1     2  ...      3.0        Zach Wilson          NYJ\n",
       " 138     Logan Woodside      7   249  ...      5.0     Logan Woodside          TEN\n",
       " 134          Luke Falk      6   199  ...      4.0          Luke Falk          NYJ\n",
       " 84      Desmond Ridder      3    74  ...      4.0     Desmond Ridder          ATL\n",
       " 30      Russell Wilson      3    75  ...      7.0     Russell Wilson          SEA\n",
       " 18    Colin Kaepernick      2    36  ...      4.0   Colin Kaepernick           SF\n",
       " \n",
       " [27 rows x 21 columns],\n",
       " 'y_train': 102    0\n",
       " 97     0\n",
       " 105    0\n",
       " 146    0\n",
       " 119    0\n",
       "       ..\n",
       " 61     1\n",
       " 31     1\n",
       " 13     1\n",
       " 62     1\n",
       " 112    0\n",
       " Name: seasons_with_draft_team, Length: 81, dtype: int64,\n",
       " 'y_test': 9      0\n",
       " 29     1\n",
       " 128    1\n",
       " 106    0\n",
       " 33     0\n",
       " 2      0\n",
       " 56     0\n",
       " 46     0\n",
       " 49     0\n",
       " 77     0\n",
       " 109    0\n",
       " 103    0\n",
       " 73     0\n",
       " 0      1\n",
       " 7      0\n",
       " 116    1\n",
       " 15     0\n",
       " 95     0\n",
       " 70     0\n",
       " 50     1\n",
       " 52     0\n",
       " 93     0\n",
       " 138    0\n",
       " 134    0\n",
       " 84     0\n",
       " 30     1\n",
       " 18     1\n",
       " Name: seasons_with_draft_team, dtype: int64,\n",
       " '_i31': 'model = LogisticRegression()\\nmodel.fit(X_train, y_train)',\n",
       " 'model': <catboost.core.CatBoostClassifier at 0x32068ced0>,\n",
       " '_31': LogisticRegression(),\n",
       " '_i32': 'model = LogisticRegression()\\nmodel.fit(X_train, y_train)\\nmodel.predict(X_test)',\n",
       " '_32': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0]),\n",
       " '_i33': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional',\n",
       " 'accuracy_score': <function sklearn.metrics._classification.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)>,\n",
       " 'f1_score': <function sklearn.metrics._classification.f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'roc_auc_score': <function sklearn.metrics._ranking.roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)>,\n",
       " 'recall_score': <function sklearn.metrics._classification.recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'precision_score': <function sklearn.metrics._classification.precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>,\n",
       " 'precision_recall_curve': <function sklearn.metrics._ranking.precision_recall_curve(y_true, y_score=None, *, pos_label=None, sample_weight=None, drop_intermediate=False, probas_pred='deprecated')>,\n",
       " '_i34': 'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\nmodel.predict(X_test)',\n",
       " '_34': array([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0]),\n",
       " '_i35': 'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       " 'y_preds': array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1]),\n",
       " '_35': 0.6666666666666666,\n",
       " '_i36': 'model = RandomForestClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       " '_36': 0.6666666666666666,\n",
       " '_i37': 'model = XGBClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       " '_37': 0.5185185185185185,\n",
       " '_i38': 'model = LogisticRegression(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       " '_i39': 'model = LogisticRegression()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       " '_39': 0.7037037037037037,\n",
       " '_i40': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\naccuracy_score(y_test, y_preds)',\n",
       " '_40': 0.7407407407407407,\n",
       " '_i41': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_tste)',\n",
       " 'y_probs': array([0.07915182, 0.24816314, 0.27551177, 0.06560755, 0.3334303 ,\n",
       "        0.36275897, 0.12708844, 0.24322529, 0.5979279 , 0.19634122,\n",
       "        0.07040088, 0.12356025, 0.19607455, 0.70742475, 0.35561972,\n",
       "        0.45213291, 0.7535539 , 0.0901011 , 0.40037947, 0.6902618 ,\n",
       "        0.33112739, 0.11957425, 0.11106133, 0.14325322, 0.09657287,\n",
       "        0.1470086 , 0.66689199]),\n",
       " '_i42': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_42': 0.5,\n",
       " '_i43': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_i44': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_i45': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_45': 0.41428571428571426,\n",
       " '_i46': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,0]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_46': 0.5857142857142856,\n",
       " '_i47': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_47': 0.41428571428571426,\n",
       " '_i48': 'model = RandomForestClassifier\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_i49': 'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_49': 0.4392857142857143,\n",
       " '_i50': 'y_probss',\n",
       " '_i51': 'y_probs',\n",
       " '_51': array([0.32, 0.23, 0.21, 0.05, 0.1 , 0.48, 0.16, 0.19, 0.45, 0.08, 0.37,\n",
       "        0.58, 0.36, 0.22, 0.37, 0.32, 0.2 , 0.21, 0.57, 0.59, 0.42, 0.17,\n",
       "        0.29, 0.07, 0.41, 0.12, 0.1 ]),\n",
       " '_i52': 'model = RandomForestClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_52': 0.6296296296296297,\n",
       " '_i53': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_i54': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_54': 0.7407407407407407,\n",
       " '_i55': 'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score',\n",
       " '_55': <function sklearn.metrics._ranking.roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)>,\n",
       " '_i56': 'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\n# y_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_preds)',\n",
       " '_56': 0.375,\n",
       " '_i57': 'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_57': 0.3571428571428571,\n",
       " '_i58': 'model = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_58': 0.5555555555555556,\n",
       " '_i59': 'model = SVC()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_i60': 'model = SVC(probability=1)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_i61': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_61': 0.7407407407407407,\n",
       " '_i62': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_62': 0.5857142857142856,\n",
       " '_i63': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:5]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i64': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_64': 0.5357142857142857,\n",
       " '_i65': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:4]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i66': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:4]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i67': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_67': 0.4357142857142857,\n",
       " '_i68': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:8]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i69': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:8]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i70': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_70': 0.6000000000000001,\n",
       " '_i71': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_71': 0.7407407407407407,\n",
       " '_i72': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:9]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i73': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:9]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i74': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_74': 0.40714285714285714,\n",
       " '_i75': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1)[numerics_only.columns[:10]],\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i76': 'model = SVC(probability=True)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_76': 0.5357142857142857,\n",
       " '_i77': 'model = CatBoostClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)',\n",
       " '_77': 0.5785714285714285,\n",
       " '_i78': 'model = CatBoostClassifier()\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_78': 0.7407407407407407,\n",
       " '_i79': 'model = CatBoostClassifier(n_estimators=200)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)',\n",
       " '_79': 0.7407407407407407,\n",
       " '_i80': \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns)\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i81': \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i82': 'X_train',\n",
       " '_82':         G    Cmp     Att  Cmp%      Yds    TD   TD%   Int  Int%   Y/A\n",
       " 31    1.0    5.0     8.0  62.5     57.0   0.0   0.0   0.0   0.0   7.1\n",
       " 53   18.0  192.0   368.0  52.2   2732.0  16.0   4.3  10.0   2.7   7.4\n",
       " 97   29.0  552.0   813.0  67.9   7386.0  69.0   8.5  15.0   1.8   9.1\n",
       " 138  44.0  759.0  1166.0  65.1  10514.0  93.0   8.0  25.0   2.1   9.0\n",
       " 30   14.0  225.0   309.0  72.8   3175.0  33.0  10.7   4.0   1.3  10.3\n",
       " ..    ...    ...     ...   ...      ...   ...   ...   ...   ...   ...\n",
       " 61   27.0  562.0   851.0  66.0   7964.0  65.0   7.6  28.0   3.3   9.4\n",
       " 126   8.0  218.0   340.0  64.1   2315.0  12.0   3.5   9.0   2.6   6.8\n",
       " 95   22.0  396.0   579.0  68.4   5373.0  63.0  10.9   9.0   1.6   9.3\n",
       " 146  10.0   20.0    43.0  46.5     94.0   0.0   0.0   2.0   4.7   2.2\n",
       " 81   13.0  281.0   450.0  62.4   4033.0  27.0   6.0   8.0   1.8   9.0\n",
       " \n",
       " [81 rows x 10 columns],\n",
       " '_i83': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i84': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i85': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=numerics_only['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i86': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i87': 'X_train',\n",
       " '_87':     pfr_player_name  round  pick  ...  seasons            name  recent_team\n",
       " 120     Ryan Finley      4   104  ...      8.0     Ryan Finley          CIN\n",
       " 28   Brandon Weeden      1    22  ...      4.0  Brandon Weeden          CLE\n",
       " 99      Davis Mills      3    67  ...      3.0     Davis Mills          HOU\n",
       " 69     Carson Wentz      1     2  ...      NaN             NaN          NaN\n",
       " 131   Mason Rudolph      3    76  ...      4.0   Mason Rudolph          PIT\n",
       " ..              ...    ...   ...  ...      ...             ...          ...\n",
       " 129      Josh Rosen      1    10  ...      3.0      Josh Rosen          ARI\n",
       " 136    Danny Etling      7   219  ...      8.0    Danny Etling          NaN\n",
       " 1         Tim Tebow      1    25  ...      4.0       Tim Tebow          DEN\n",
       " 3        Colt McCoy      3    85  ...      4.0      Colt McCoy          CLE\n",
       " 96        Mac Jones      1    15  ...      3.0       Mac Jones           NE\n",
       " \n",
       " [111 rows x 21 columns],\n",
       " '_i88': \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i89': 'df = df.dropna()',\n",
       " '_i90': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i91': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i92': \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_92': 0.7777777777777778,\n",
       " '_i93': \"model = CatBoostClassifier(n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_93': 0.7285714285714285,\n",
       " '_i94': \"model = CatBoostClassifier(iterations=300, n_estimators=200, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_i95': \"model = CatBoostClassifier(iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_95': 0.7714285714285714,\n",
       " '_i96': \"model = CatBoostClassifier(iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_96': 0.7777777777777778,\n",
       " '_i97': \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_97': 0.7714285714285714,\n",
       " '_i98': \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_98': 0.7777777777777778,\n",
       " '_i99': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional\\nfrom keras.models import Sequential',\n",
       " 'Sequential': keras.src.models.sequential.Sequential,\n",
       " '_i100': 'model = Sequential()',\n",
       " '_i101': 'model = Sequential()\\nmodel.add(\\n    Dense()\\n)',\n",
       " '_i102': 'model = Sequential()\\nmodel.add(\\n    Dense(units=df.shape[0])\\n)',\n",
       " '_i103': 'model = Sequential()\\nmodel.add(\\n    Dense(units=df.shape[1])\\n)',\n",
       " '_i104': 'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=df.shape[1])\\n)',\n",
       " '_i105': 'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=df.shape[1], units=64)\\n)',\n",
       " '_i106': 'model = Sequential()\\nmodel.add(\\n    Dense(input_shape=tuple(df.shape[1]), units=64)\\n)',\n",
       " '_i107': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization\\nfrom keras.models import Sequential',\n",
       " 'Normalization': keras.src.layers.preprocessing.normalization.Normalization,\n",
       " '_i108': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout\\nfrom keras.models import Sequential',\n",
       " 'Dropout': keras.src.layers.regularization.dropout.Dropout,\n",
       " '_i109': \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)=\",\n",
       " '_i110': \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       " '_i111': \"X_train, X_test, y_train, y_test = train_test_split(numerics_only.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       numerics_only['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i112': \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       " '_i113': \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\",\n",
       " '_i114': \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\",\n",
       " '_i115': \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       " '_i116': 'X_train',\n",
       " '_116':         G    Cmp     Att  Cmp%      Yds  ...   AY/A   Y/C    Y/G   Rate  seasons\n",
       " 2    35.0  695.0  1110.0  62.6   8148.0  ...   7.33  11.7  232.8  137.2      3.0\n",
       " 31    1.0    5.0     8.0  62.5     57.0  ...   7.13  11.4   57.0  122.4      8.0\n",
       " 17   50.0  812.0  1317.0  61.7  10314.0  ...   7.88  12.7  206.3  140.7      4.0\n",
       " 100  48.0  728.0  1141.0  63.8   8948.0  ...   8.32  12.3  186.4  147.0      5.0\n",
       " 9    27.0  421.0   682.0  61.7   5018.0  ...   7.48  11.9  185.9  141.4      3.0\n",
       " ..    ...    ...     ...   ...      ...  ...    ...   ...    ...    ...      ...\n",
       " 14   40.0  619.0  1147.0  54.0   7639.0  ...   6.21  12.3  191.0  119.1      4.0\n",
       " 138  44.0  759.0  1166.0  65.1  10514.0  ...   9.65  13.9  239.0  162.9      5.0\n",
       " 4    30.0  408.0   637.0  64.1   4265.0  ...   5.88  10.5  142.2  123.9      4.0\n",
       " 103  33.0  474.0   684.0  69.3   7442.0  ...  12.70  15.7  225.5  199.4      3.0\n",
       " 70   38.0  758.0  1205.0  62.9   8863.0  ...   7.48  11.7  233.2  137.0      3.0\n",
       " \n",
       " [81 rows x 15 columns],\n",
       " '_i117': 'y_train',\n",
       " '_117': 2      0\n",
       " 31     1\n",
       " 17     1\n",
       " 100    0\n",
       " 9      0\n",
       "       ..\n",
       " 14     1\n",
       " 138    0\n",
       " 4      0\n",
       " 103    0\n",
       " 70     0\n",
       " Name: seasons_with_draft_team, Length: 81, dtype: int64,\n",
       " '_i118': 'y_train.isna().sum(0)',\n",
       " '_118': 0,\n",
       " '_i119': 'y_train.isna().sum()',\n",
       " '_119': 0,\n",
       " '_i120': \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(ReLU(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       " '_i121': \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       " '_i122': 'y_train.shape',\n",
       " '_122': (81,),\n",
       " '_i123': 'X_train.shape',\n",
       " '_123': (81, 15),\n",
       " '_i124': 'model',\n",
       " '_124': <Sequential name=sequential_6, built=True>,\n",
       " '_i125': 'X_train',\n",
       " '_125':         G    Cmp     Att  Cmp%      Yds  ...   AY/A   Y/C    Y/G   Rate  seasons\n",
       " 2    35.0  695.0  1110.0  62.6   8148.0  ...   7.33  11.7  232.8  137.2      3.0\n",
       " 31    1.0    5.0     8.0  62.5     57.0  ...   7.13  11.4   57.0  122.4      8.0\n",
       " 17   50.0  812.0  1317.0  61.7  10314.0  ...   7.88  12.7  206.3  140.7      4.0\n",
       " 100  48.0  728.0  1141.0  63.8   8948.0  ...   8.32  12.3  186.4  147.0      5.0\n",
       " 9    27.0  421.0   682.0  61.7   5018.0  ...   7.48  11.9  185.9  141.4      3.0\n",
       " ..    ...    ...     ...   ...      ...  ...    ...   ...    ...    ...      ...\n",
       " 14   40.0  619.0  1147.0  54.0   7639.0  ...   6.21  12.3  191.0  119.1      4.0\n",
       " 138  44.0  759.0  1166.0  65.1  10514.0  ...   9.65  13.9  239.0  162.9      5.0\n",
       " 4    30.0  408.0   637.0  64.1   4265.0  ...   5.88  10.5  142.2  123.9      4.0\n",
       " 103  33.0  474.0   684.0  69.3   7442.0  ...  12.70  15.7  225.5  199.4      3.0\n",
       " 70   38.0  758.0  1205.0  62.9   8863.0  ...   7.48  11.7  233.2  137.0      3.0\n",
       " \n",
       " [81 rows x 15 columns],\n",
       " '_i126': \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       " '_i127': \"model.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       " '_i128': 'X_train.shape[1]',\n",
       " '_128': 15,\n",
       " '_i129': \"model = Sequential()\\nmodel.add(Dense(units=32, input_dim=X_train.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\",\n",
       " '_129': <keras.src.callbacks.history.History at 0x313944250>,\n",
       " '_i130': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\\nfrom keras.models import Sequential',\n",
       " 'Input': <function keras.src.layers.core.input_layer.Input(shape=None, batch_size=None, dtype=None, sparse=None, batch_shape=None, name=None, tensor=None, optional=False)>,\n",
       " '_i131': \"model = Sequential()\\nmodel.add(Input(shape=X_train.shape[1]))\\nmodel.add(Dense(units=32activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       " '_i132': \"model = Sequential()\\nmodel.add(Input(shape=X_train.shape[1]))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       " '_i133': \"model = Sequential()\\nmodel.add(Input(shape=(,X_train.shape[1])))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       " '_i134': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_prdmodel.predict(X_test)\\n\\nroc_auc_score()\",\n",
       " '_i135': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\n\\nroc_auc_score()\",\n",
       " '_i136': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_136': 0.6714285714285714,\n",
       " '_i137': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_137': 0.5571428571428572,\n",
       " '_i138': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_138': 0.5142857142857143,\n",
       " '_i139': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_139': 0.6178571428571428,\n",
       " '_i140': \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_preds)\",\n",
       " '_i141': \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_preds, y_preds)\",\n",
       " '_141': 1.0,\n",
       " '_i142': \"model = Sequential()\\nmodel.reset_metrics()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_142': 0.5,\n",
       " '_i143': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\",\n",
       " '_i144': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_144': 0.55,\n",
       " '_i145': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\ny_probs(y_test, y_probs)\",\n",
       " '_i146': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_146': 0.7407407407407407,\n",
       " '_i147': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='roc-auc')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i148': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='roc_auc')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i149': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i150': \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='SparseCategoricalCrossentropy')\\n\\n\\nmodel.fit(X_train, y_train)\\ny_preds_NN = model.predict(X_test)\\ny_preds_NN = (y_preds_NN >= .5).astype(int)\",\n",
       " '_i151': \"model.add(Dense(units=32, input_dim=X_test.shape[1], activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\n\\n\\nmodel.fit(X_train, y_train)\\ny_preds_NN = model.predict(X_test)\\ny_preds_NN = (y_preds_NN >= .5).astype(int)\",\n",
       " 'y_preds_NN': array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]),\n",
       " '_i152': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i153': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='logloss')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i154': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_154': 0.7407407407407407,\n",
       " '_i155': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_155': 0.42142857142857143,\n",
       " '_i156': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_156': 0.7407407407407407,\n",
       " '_i157': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_157': 0.7407407407407407,\n",
       " '_i158': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_158': 0.49999999999999994,\n",
       " '_i159': \"model = Sequential()\\nmodel.add(Input(shape=(X_train.shape[1],)))\\nmodel.add(Dense(units=32, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dropout(.5))\\nmodel.add(Dense(units=16, activation='tanh'))\\nmodel.add(Dense(units=8, activation='tanh'))\\nmodel.add(Normalization())\\nmodel.add(Dense(units=1, activation='sigmoid'))\\n\\nmodel.compile(optimizer='adam', loss='poisson')\\nmodel.fit(X_train, y_train)\\ny_probs = model.predict(X_test)\\ny_preds = (y_probs >= 5).astype(int)\\n\\nroc_auc_score(y_test, y_probs)\\naccuracy_score(y_test, y_preds)\",\n",
       " '_159': 0.7407407407407407,\n",
       " '_i160': \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_i161': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i162': \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\",\n",
       " '_162': 0.8148148148148148,\n",
       " '_i163': \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_163': 0.8428571428571429,\n",
       " '_i164': 'import mfl as mfl\\nimport pandas as pd\\nimport numpy as np\\nimport mfl.api.data_loaders as mfldata\\nimport nfl_data_py as nfl\\n\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score, precision_recall_curve\\n\\nfrom xgboost import XGBClassifier, XGBRFClassifier\\n\\nfrom catboost import CatBoostClassifier\\n\\nimport keras\\nfrom keras.layers import Dense, ReLU, Bidirectional, Normalization, Dropout, Input\\nfrom keras.models import Sequential',\n",
       " 'StratifiedKFold': sklearn.model_selection._split.StratifiedKFold,\n",
       " '_i165': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        \",\n",
       " 'FranchiseQB': __main__.FranchiseQB,\n",
       " '_i166': 'NFL = FranchiseQB()',\n",
       " 'NFL': <__main__.FranchiseQB at 0x322dc2450>,\n",
       " '_i167': 'NFL.run()',\n",
       " '_i168': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n        model = model(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       " '_i169': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n        model = model(self.kwargs)\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       " '_i170': 'NFL = FranchiseQB()',\n",
       " '_i171': 'NFL.run()',\n",
       " '_i172': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float')].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       " '_i173': 'NFL = FranchiseQB()',\n",
       " '_i174': 'NFL.run()',\n",
       " '_i175': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       " '_i176': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = df.iloc[train], df.iloc[test]\\n            y_train, y_test = df.values[train], df.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       " '_i177': 'NFL = FranchiseQB()',\n",
       " '_i178': 'NFL.run()',\n",
       " '_i179': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\",\n",
       " '_i180': 'NFL = FranchiseQB()',\n",
       " '_i181': 'NFL.run()',\n",
       " '_i182': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        self.meric_dict = metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        self.model_results = pd.DataFrame(self.metric_dict)\",\n",
       " '_i183': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        self.meric_dict = metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n        self.model_results = pd.DataFrame(self.metric_dict)\",\n",
       " '_i184': 'NFL = FranchiseQB()',\n",
       " '_i185': 'NFL.run()',\n",
       " '_i186': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       " '_i187': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       " '_i188': 'NFL = FranchiseQB()',\n",
       " '_i189': 'NFL.run()',\n",
       " '_i190': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       " '_i191': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_test, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       " '_i192': 'NFL = FranchiseQB()',\n",
       " '_i193': 'NFL.run()',\n",
       " '_i194': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics)\",\n",
       " '_i195': 'NFL = FranchiseQB()',\n",
       " '_i196': 'NFL.run()',\n",
       " '_i197': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i198': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i199': 'NFL = FranchiseQB()',\n",
       " '_i200': 'NFL.run()',\n",
       " '_i201': 'NFL.model_results',\n",
       " '_201':    accuracy        f1   roc_auc\n",
       " 0  0.685185  0.105263  0.427324,\n",
       " '_i202': \"NFL = FranchiseQB(model='lr')\",\n",
       " '_i203': \"NFL = FranchiseQB(model='svc')\",\n",
       " '_i204': 'NFL.run()',\n",
       " '_i205': 'NFL.model_results',\n",
       " '_205':    accuracy   f1   roc_auc\n",
       " 0  0.685185  0.0  0.390223,\n",
       " '_i206': \"NFL = FranchiseQB(model='svm')\",\n",
       " '_i207': 'NFL.run()',\n",
       " '_i208': 'NFL.model_results',\n",
       " '_i209': \"NFL = FranchiseQB(model='xgb')\",\n",
       " '_i210': 'NFL.run()',\n",
       " '_i211': 'NFL.model_results',\n",
       " '_211':    accuracy   f1   roc_auc\n",
       " 0  0.583333  0.0  0.326931,\n",
       " '_i212': \"NFL = FranchiseQB(model='rf')\",\n",
       " '_i213': 'NFL.run()',\n",
       " '_i214': 'NFL.model_results',\n",
       " '_214':    accuracy   f1   roc_auc\n",
       " 0  0.666667  0.1  0.404409,\n",
       " '_i215': 'df',\n",
       " '_215':      pfr_player_name  round  ...  recent_team  seasons_with_draft_team\n",
       " 0       Sam Bradford      1  ...           LA                      4.0\n",
       " 1          Tim Tebow      1  ...          DEN                      2.0\n",
       " 2      Jimmy Clausen      2  ...          CAR                      1.0\n",
       " 3         Colt McCoy      3  ...          CLE                      3.0\n",
       " 4         Mike Kafka      4  ...          PHI                      1.0\n",
       " ..               ...    ...  ...          ...                      ...\n",
       " 142    DeShone Kizer      2  ...          CLE                      1.0\n",
       " 143       Davis Webb      3  ...          BUF                      1.0\n",
       " 145     Joshua Dobbs      4  ...          PIT                      2.0\n",
       " 146  Nathan Peterman      5  ...          BUF                      2.0\n",
       " 148       Chad Kelly      7  ...          DEN                      1.0\n",
       " \n",
       " [108 rows x 22 columns],\n",
       " '_i216': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        if self.model == 'catboost':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=X.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i217': \"NFL = FranchiseQB(model='catboost')\",\n",
       " '_i218': 'NFL.run()',\n",
       " '_i219': 'NFL.model_results',\n",
       " '_219':    accuracy        f1   roc_auc\n",
       " 0  0.694444  0.108108  0.428634,\n",
       " '_i220': \"X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(map_response))\",\n",
       " '_i221': \"model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\nmodel.fit(X_train, y_train)\\ny_preds = model.predict(X_test)\\ny_probs = model.predict_proba(X_test)[:,1]\\naccuracy_score(y_test, y_preds)\\nroc_auc_score(y_test, y_probs)\",\n",
       " '_221': 0.7928571428571428,\n",
       " '_i222': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        if self.model == 'catboost':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=X.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i223': \"NFL = FranchiseQB(model='catboost')\",\n",
       " '_i224': 'NFL.run()',\n",
       " '_i225': 'NFL.model_results',\n",
       " '_225':    accuracy        f1   roc_auc\n",
       " 0  0.694444  0.108108  0.428634,\n",
       " '_i226': 'X_train',\n",
       " '_226':      pfr_player_name  round  pick  ...  seasons             name  recent_team\n",
       " 102       Joe Burrow      1     1  ...      7.0       Joe Burrow          CIN\n",
       " 97        Kyle Trask      2    64  ...      5.0       Kyle Trask           TB\n",
       " 105      Jordan Love      1    26  ...      3.0      Jordan Love           GB\n",
       " 146  Nathan Peterman      5   171  ...      7.0  Nathan Peterman          BUF\n",
       " 119       Will Grier      3   100  ...      7.0       Will Grier          CAR\n",
       " ..               ...    ...   ...  ...      ...              ...          ...\n",
       " 61    Jameis Winston      1     1  ...      2.0   Jameis Winston           TB\n",
       " 31        Nick Foles      3    88  ...      8.0       Nick Foles          PHI\n",
       " 13        Cam Newton      1     1  ...      7.0       Cam Newton          CAR\n",
       " 62    Marcus Mariota      1     2  ...      3.0   Marcus Mariota          TEN\n",
       " 112      Ben DiNucci      7   231  ...      2.0      Ben DiNucci          DAL\n",
       " \n",
       " [81 rows x 21 columns],\n",
       " '_i227': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i228': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i229': 'NFL.run()',\n",
       " '_i230': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat'\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i231': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i232': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i233': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i234': 'NFL.run()',\n",
       " '_i235': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=5):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i236': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i237': 'NFL.run()',\n",
       " '_i238': 'NFL.model_results',\n",
       " '_238':    accuracy        f1  roc_auc\n",
       " 0  0.703704  0.428571   0.7189,\n",
       " '_i239': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=3):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i240': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i241': 'NFL.run()',\n",
       " '_i242': 'NFL.model_results',\n",
       " '_242':    accuracy        f1  roc_auc\n",
       " 0  0.657407  0.350877  0.68529,\n",
       " '_i243': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=1):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i244': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i245': 'NFL.run()',\n",
       " '_i246': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        for train, test in folds:\\n            \\n            X_train, X_test = X.iloc[train], X.iloc[test]\\n            y_train, y_test = y.values[train], y.values[test]    \\n\\n            model.fit(X_train, y_train)\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i247': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i248': 'NFL.run()',\n",
       " '_i249': 'NFL.model_results',\n",
       " '_249':    accuracy        f1   roc_auc\n",
       " 0   0.62037  0.349206  0.666085,\n",
       " '_i250': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(X.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       X['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i251': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i252': 'NFL.run()',\n",
       " '_i253': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs(model.predict_proba(X_test)[:,1])\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i254': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i255': 'NFL.run()',\n",
       " '_i256': \"class FranchiseQB:\\n    def __init__(self, \\n                 feature_set='numeric', \\n                 model='catboost', \\n                 dataset='../data/for_modeling.csv', \\n                 **kwargs):\\n        \\n        self.feature_set = feature_set\\n        self.model = model\\n        self.dataset = dataset\\n        self.kwargs = kwargs\\n        self.model_map = {\\n            'catboost' : CatBoostClassifier(),\\n            'xgb' : XGBClassifier(), \\n            'rf' : RandomForestClassifier(),\\n            'lr' : LogisticRegression(),\\n            'hgb' : HistGradientBoostingClassifier(),\\n            'svm' : SVC(),\\n            'nn_basic': ...\\n        }\\n        self.available_models = list(self.model_map.keys())\\n        self.model_func = self.model_map[model]\\n        self.full_dataset = pd.read_csv(dataset)\\n\\n    def create_training_data(self, n_splits=4, stratify=True):\\n        pass\\n\\n    def map_response(self, x):\\n        if x >= 4:\\n            return 1\\n        else: \\n            return 0\\n        \\n    def score(self, y_test, y_probs, y_preds):\\n        accuracy = accuracy_score(y_test, y_preds)\\n        f1 = f1_score(y_test, y_preds)\\n        roc_auc = roc_auc_score(y_test, y_probs)\\n\\n        metric_dict = {\\n            'accuracy' : accuracy,\\n            'f1' : f1,\\n            'roc_auc': roc_auc\\n        }\\n\\n        return metric_dict\\n\\n    def run(self, kfold=False, folds=2):\\n        df = self.full_dataset.dropna()\\n        model = self.model_func\\n\\n        if self.feature_set == 'numeric':\\n            X = df[df.select_dtypes(include='float').columns].drop('seasons_with_draft_team',axis=1)\\n        elif self.feature_set == 'cat':\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            X = df.drop('seasons_with_draft_team', axis=1)\\n        \\n        y = df['seasons_with_draft_team'].apply(self.map_response)\\n\\n        self.X = X\\n        self.y = y\\n\\n        k_strat = StratifiedKFold(n_splits=folds, shuffle=False)\\n        folds = k_strat.split(X, y)\\n        y_preds = []\\n        y_probs = []\\n        y_tests = []\\n        \\n        if kfold:\\n            for train, test in folds:\\n                \\n                X_train, X_test = X.iloc[train], X.iloc[test]\\n                y_train, y_test = y.values[train], y.values[test]    \\n\\n                model.fit(X_train, y_train)\\n                y_tests.extend(y_test)\\n                y_preds.extend(model.predict(X_test))\\n                y_probs.extend(model.predict_proba(X_test)[:,1])\\n        \\n        else:\\n            X_train, X_test, y_train, y_test = train_test_split(df.drop('seasons_with_draft_team',\\n                                                                       axis=1),\\n                                                                       df['seasons_with_draft_team'].apply(self.map_response),\\n                                                                       test_size=.25,\\n                                                                       shuffle=True,\\n                                                                       stratify=df['seasons_with_draft_team'].apply(self.map_response))\\n            model = CatBoostClassifier(one_hot_max_size=5, iterations=300, cat_features=df.select_dtypes(include='object').columns.tolist())\\n            model.fit(X_train, y_train)\\n\\n            y_tests.extend(y_test)\\n            y_preds.extend(model.predict(X_test))\\n            y_probs.extend((model.predict_proba(X_test)[:,1]))\\n        \\n        self.y_preds = y_preds\\n        self.y_probs = y_probs\\n\\n        metrics = self.score(y_tests, y_probs, y_preds)\\n        self.model_results = pd.DataFrame(metrics, index=[0])\",\n",
       " '_i257': \"NFL = FranchiseQB(model='catboost', feature_set='cat')\",\n",
       " '_i258': 'NFL.run()',\n",
       " '_i259': 'NFL.model_results',\n",
       " '_259':    accuracy        f1   roc_auc\n",
       " 0  0.851852  0.666667  0.921429,\n",
       " '_i260': 'import pydantic',\n",
       " 'pydantic': <module 'pydantic' from '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/__init__.py'>,\n",
       " '_i261': 'from pydantic import BaseModel',\n",
       " 'BaseModel': pydantic.main.BaseModel,\n",
       " '_i262': 'from pydantic import BaseModel\\n\\nclass MyFirstModel(BaseModel):\\n    first_name: str\\n    last_name: str\\n\\nvalidating = MyFirstModel(first_name=\"marc\", last_name=\"nealer\")',\n",
       " 'MyFirstModel': __main__.MyFirstModel,\n",
       " 'validating': MyFirstModel(first_name='marc', last_name='nealer'),\n",
       " '_i263': \"MyFirstModel(first_name='Ben', last_name=0)\",\n",
       " '_i264': 'from Exception import RuntimeError',\n",
       " '_i265': 'Exception',\n",
       " '_265': Exception,\n",
       " '_i266': 'ValueError',\n",
       " '_266': ValueError,\n",
       " '_i267': 'compile',\n",
       " '_267': <function compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1, *, _feature_version=-1)>,\n",
       " '_i268': 'globals()',\n",
       " '_268': {...},\n",
       " '_i269': 'globals().__len__()',\n",
       " '_269': 427,\n",
       " '_i270': 'globals()[9]',\n",
       " '_i271': 'locals()',\n",
       " '_271': {...},\n",
       " '_i272': 'vars()'}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for MyFirstModel\nlast_name\n  Input should be a valid string [type=string_type, input_value=0, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[263], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mMyFirstModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for MyFirstModel\nlast_name\n  Input should be a valid string [type=string_type, input_value=0, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type"
     ]
    }
   ],
   "source": [
    "MyFirstModel(first_name='Ben', last_name=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
